<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ly1998117.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:true,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="Chapter 6 Using a neural network to fit the data">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning with PyTorch -- Part 1 Core PyTorch (C6-C8)">
<meta property="og:url" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/index.html">
<meta property="og:site_name" content="LiuYang&#39;s Blog">
<meta property="og:description" content="Chapter 6 Using a neural network to fit the data">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/1.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/2.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/3.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/4.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/5.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/6.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/7.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/8.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/9.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/10.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/11.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/12.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/13.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/14.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/15.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/16.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/17.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/18.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/19.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/20.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/21.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/22.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/23.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/24.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/25.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/26.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/27.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/28.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/29.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/30.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/32.png">
<meta property="article:published_time" content="2021-08-05T07:50:09.000Z">
<meta property="article:modified_time" content="2021-08-10T10:01:01.575Z">
<meta property="article:author" content="LiuYang">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ly1998117.github.io/2021/08/05/core-pytorch3/1.png">


<link rel="canonical" href="https://ly1998117.github.io/2021/08/05/core-pytorch3/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;08&#x2F;05&#x2F;core-pytorch3&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;08&#x2F;05&#x2F;core-pytorch3&#x2F;&quot;,&quot;title&quot;:&quot;Deep Learning with PyTorch -- Part 1 Core PyTorch (C6-C8)&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Deep Learning with PyTorch -- Part 1 Core PyTorch (C6-C8) | LiuYang's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiuYang's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">26</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-6-using-a-neural-network-to-fit-the-data"><span class="nav-number">1.</span> <span class="nav-text">Chapter 6 Using a neural network to fit the data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#artificial-neurons"><span class="nav-number">1.1.</span> <span class="nav-text">6.1 Artificial neurons</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#composing-a-multilayer-network"><span class="nav-number">1.1.1.</span> <span class="nav-text">6.1.1 Composing a multilayer network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#understanding-the-error-function"><span class="nav-number">1.1.2.</span> <span class="nav-text">6.1.2 Understanding the error function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#all-we-need-is-activation"><span class="nav-number">1.1.3.</span> <span class="nav-text">6.1.3 All we need is activation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#capping-the-output-range"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">CAPPING THE OUTPUT RANGE</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#compressing-the-output-range"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">COMPRESSING THE OUTPUT RANGE</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#more-activation-functions"><span class="nav-number">1.1.4.</span> <span class="nav-text">6.1.4 More activation functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#choosing-the-best-activation-function"><span class="nav-number">1.1.5.</span> <span class="nav-text">6.1.5 Choosing the best activation function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#what-learning-means-for-a-neural-network"><span class="nav-number">1.1.6.</span> <span class="nav-text">6.1.6 What learning means for a neural network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-pytorch-nn-module"><span class="nav-number">1.2.</span> <span class="nav-text">6.2 The PyTorch nn module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#using-call-rather-than-forward"><span class="nav-number">1.2.1.</span> <span class="nav-text">6.2.1 Using call rather than forward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#returning-to-the-linear-model"><span class="nav-number">1.2.2.</span> <span class="nav-text">6.2.2 Returning to the linear model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#batching-inputs"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">BATCHING INPUTS</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#optimizing-batches"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">OPTIMIZING BATCHES</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#finally-a-neural-network"><span class="nav-number">1.3.</span> <span class="nav-text">6.3 Finally a neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#replacing-the-linear-model"><span class="nav-number">1.3.1.</span> <span class="nav-text">6.3.1 Replacing the linear model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#inspecting-the-parameters"><span class="nav-number">1.3.2.</span> <span class="nav-text">6.3.2 Inspecting the parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#comparing-to-the-linear-model"><span class="nav-number">1.3.3.</span> <span class="nav-text">6.3.3 Comparing to the linear model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conclusion"><span class="nav-number">1.4.</span> <span class="nav-text">6.4 Conclusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-7-telling-birds-from-airplanes-learning-from-images"><span class="nav-number">2.</span> <span class="nav-text">Chapter 7 Telling birds from airplanes: Learning from images</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a-dataset-of-tiny-images"><span class="nav-number">2.0.1.</span> <span class="nav-text">7.1 A dataset of tiny images</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#downloading-cifar-10"><span class="nav-number">2.0.2.</span> <span class="nav-text">7.1.1 Downloading CIFAR-10</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-dataset-class"><span class="nav-number">2.0.3.</span> <span class="nav-text">7.1.2 The Dataset class</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dataset-transforms"><span class="nav-number">2.0.4.</span> <span class="nav-text">7.1.3 Dataset transforms</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#normalizing-data"><span class="nav-number">2.0.5.</span> <span class="nav-text">7.1.4 Normalizing data</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#distinguishing-birds-from-airplanes"><span class="nav-number">2.1.</span> <span class="nav-text">7.2 Distinguishing birds from airplanes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#building-the-dataset"><span class="nav-number">2.1.1.</span> <span class="nav-text">7.2.1 Building the dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-fully-connected-model"><span class="nav-number">2.1.2.</span> <span class="nav-text">7.2.2 A fully connected model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#output-of-a-classifier"><span class="nav-number">2.1.3.</span> <span class="nav-text">7.2.3 Output of a classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#representing-the-output-as-probabilities"><span class="nav-number">2.1.4.</span> <span class="nav-text">7.2.4 Representing the output as probabilities</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-loss-for-classifying"><span class="nav-number">2.1.5.</span> <span class="nav-text">7.2.5 A loss for classifying</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#training-the-classifier"><span class="nav-number">2.1.6.</span> <span class="nav-text">7.2.6 Training the classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-limits-of-going-fully-connected"><span class="nav-number">2.1.7.</span> <span class="nav-text">7.2.7 The limits of going fully connected</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-8-using-convolutions-to-generalize"><span class="nav-number">3.</span> <span class="nav-text">Chapter 8 Using convolutions to generalize</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-case-for-convolutions"><span class="nav-number">3.1.</span> <span class="nav-text">8.1 The case for convolutions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-convolutions-do"><span class="nav-number">3.1.1.</span> <span class="nav-text">8.1.1 What convolutions do</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convolutions-in-action"><span class="nav-number">3.2.</span> <span class="nav-text">8.2 Convolutions in action</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#padding-the-boundary"><span class="nav-number">3.2.1.</span> <span class="nav-text">8.2.1 Padding the boundary</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#detecting-features-with-convolutions"><span class="nav-number">3.2.2.</span> <span class="nav-text">8.2.2 Detecting features with convolutions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#looking-further-with-depth-and-pooling"><span class="nav-number">3.2.3.</span> <span class="nav-text">8.2.4 Looking further with depth and pooling</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#from-large-to-small-downsampling"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">FROM LARGE TO SMALL: DOWNSAMPLING</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#combining-convolutions-and-downsampling-for-great-good"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#putting-it-all-together-for-our-network"><span class="nav-number">3.2.4.</span> <span class="nav-text">8.2.4 Putting it all together for our network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#subclassing-nn.module"><span class="nav-number">3.3.</span> <span class="nav-text">8.3 Subclassing nn.Module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#our-network-as-an-nn.module"><span class="nav-number">3.3.1.</span> <span class="nav-text">8.3.1 Our network as an nn.Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#how-pytorch-keeps-track-of-parameters-and-submodules"><span class="nav-number">3.3.2.</span> <span class="nav-text">8.3.2 How PyTorch keeps track of parameters and submodules</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-functional-api"><span class="nav-number">3.3.3.</span> <span class="nav-text">8.3.3 The functional API</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-our-convnet"><span class="nav-number">3.4.</span> <span class="nav-text">8.4 Training our convnet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#measuring-accuracy"><span class="nav-number">3.4.1.</span> <span class="nav-text">8.4.1 Measuring accuracy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#saving-and-loading-our-model"><span class="nav-number">3.4.2.</span> <span class="nav-text">8.4.2 Saving and loading our model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#training-on-the-gpu"><span class="nav-number">3.4.3.</span> <span class="nav-text">8.4.3 Training on the GPU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-design"><span class="nav-number">3.5.</span> <span class="nav-text">8.5 Model design</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#adding-memory-capacity-width"><span class="nav-number">3.5.1.</span> <span class="nav-text">8.5.1 Adding memory capacity: Width</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#helping-our-model-to-converge-and-generalize-regularization"><span class="nav-number">3.5.2.</span> <span class="nav-text">8.5.2 Helping our model to converge and generalize: Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#keeping-the-parameters-in-check-weight-penalties"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">KEEPING THE PARAMETERS IN CHECK: WEIGHT PENALTIES</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#not-relying-too-much-on-a-single-input-dropout"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">NOT RELYING TOO MUCH ON A SINGLE INPUT: DROPOUT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#keeping-activations-in-check-batch-normalization"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">KEEPING ACTIVATIONS IN CHECK: BATCH NORMALIZATION</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#going-deeper-to-learn-more-complex-structures-depth"><span class="nav-number">3.5.3.</span> <span class="nav-text">8.5.3 Going deeper to learn more complex structures: Depth</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#skip-connections"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">SKIP CONNECTIONS</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#initialization"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">INITIALIZATION</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#comparing-the-designs-from-this-section"><span class="nav-number">3.5.4.</span> <span class="nav-text">8.5.4 Comparing the designs from this section</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#its-already-outdated"><span class="nav-number">3.6.</span> <span class="nav-text">8.5.5 It’s already outdated</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conclusion-1"><span class="nav-number">3.7.</span> <span class="nav-text">8.6 Conclusion</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiuYang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LiuYang</p>
  <div class="site-description" itemprop="description">人与人的悲欢并不相通</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.macwk.com/" title="https:www.macwk.com&#x2F;" rel="noopener" target="_blank">Macwk</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ly1998117.github.io/2021/08/05/core-pytorch3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="LiuYang">
      <meta itemprop="description" content="人与人的悲欢并不相通">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuYang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning with PyTorch -- Part 1 Core PyTorch (C6-C8)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-08-05 15:50:09" itemprop="dateCreated datePublished" datetime="2021-08-05T15:50:09+08:00">2021-08-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-10 18:01:01" itemprop="dateModified" datetime="2021-08-10T18:01:01+08:00">2021-08-10</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>68k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:02</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="chapter-6-using-a-neural-network-to-fit-the-data">Chapter 6 <em>Using a neural network to fit the data</em></h2>
<span id="more"></span>
<ul>
<li>Nonlinear activation functions as the key difference compared with linear models</li>
<li>Working with PyTorch’s nn module</li>
<li>Solving a linear-fit problem with a neural network</li>
</ul>
<p>In this chapter, we will make some changes to our model architecture: we’re going to implement a full artificial neural network to solve our temperature-conversion problem. We’ll continue using our training loop from the last chapter, along with our Fahrenheit-to-Celsius samples split into training and validation sets.</p>
<h3 id="artificial-neurons">6.1 <em>Artificial neurons</em></h3>
<p>As a matter of fact, although the initial models were inspired by neuroscience, modern artificial neural networks bear only a slight resemblance to the mechanisms of neurons in the brain. It seems likely that both artificial and physiological neural networks use vaguely similar mathematical strategies for approximating complicated functions because that family of strategies works very effectively. Mathematically, we can write this out as <em>o</em> = <em>f</em>(<em>w</em> * <em>x</em> + <em>b</em>), with <em>x</em> as our input, <em>w</em> our weight or scaling factor, and <em>b</em> as our bias or offset. <em>f</em> is our activation function, set to the hyperbolic tangent, or tanh function here.</p>
<p><img src="1.png" alt="1" style="zoom:50%;" /></p>
<center>
Figure 6.2 An artificial neuron: a linear transformation enclosed in a nonlinear function
</center>
<h4 id="composing-a-multilayer-network">6.1.1 <em>Composing a multilayer network</em></h4>
<p>A multilayer neural network, as represented in figure 6.3, is made up of a composition of functions like those we just discussed. where the output of a layer of neurons is used as an input for the following layer. Remember that w_0 here is a matrix, and x is a vector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_1 = f(w_0 * x + b_0)</span><br><span class="line">x_2 = f(w_1 * x_1 + b_1)</span><br><span class="line">...</span><br><span class="line">y = f(w_n * x_n + b_n)</span><br></pre></td></tr></table></figure>
<p><img src="2.png" alt="2" style="zoom:50%;" /></p>
<center>
Figure 6.3 A neural network with three layers
</center>
<h4 id="understanding-the-error-function">6.1.2 <em>Understanding the error function</em></h4>
<p>An important difference between our earlier linear model and what we’ll actually be using for deep learning is the shape of the error function. Our linear model and error-squared loss function had a convex error curve with a singular, clearly defined minimum. If we were to use other methods, we could solve for the parameters mini- mizing the error function automatically and definitively.</p>
<p>Neural networks do not have that same property of a convex error surface, even when using the same error-squared loss function. There’s no single right answer for each parameter we’re attempting to approximate. Instead, we are trying to get all of the parameters, when acting <em>in concert</em>, to produce a useful output. Since that useful output is only going to <em>approximate</em> the truth, there will be some level of imperfection.</p>
<p>A big part of the reason neural networks have non-convex error surfaces is due to the activation function. The ability of an ensemble of neurons to approximate a very wide range of useful functions depends on the combination of the linear and nonlinear behavior inherent to each neuron.</p>
<h4 id="all-we-need-is-activation">6.1.3 <em>All we need is activation</em></h4>
<p>As we have seen, the simplest unit in (deep) neural networks is a linear operation (scaling + offset) followed by an activation function. The activation function plays two important roles:</p>
<ul>
<li>In the inner parts of the model, it allows the output function to have different slopes at different values—something a linear function by definition cannot do. By trickily composing these differently sloped parts for many outputs, neural networks can approximate arbitrary functions, as we will see in section 6.1.6.2</li>
<li>At the last layer of the network, it has the role of concentrating the outputs of the preceding linear operation into a given range.</li>
</ul>
<h5 id="capping-the-output-range">CAPPING THE OUTPUT RANGE</h5>
<p>We want to firmly constrain the output of our linear operation to a specific range so that the consumer of this output doesn’t have to handle numerical inputs of puppies at 12/10, bears at –10, and garbage trucks at –1,000.</p>
<p>One possibility is to just cap the output values: anything below 0 is set to 0, and any- thing above 10 is set to 10. That’s a simple activation function called torch.nn.<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#hardtanh">Hardtanh</a>, but note that the default range is –1 to +1.</p>
<h5 id="compressing-the-output-range">COMPRESSING THE OUTPUT RANGE</h5>
<p>Another family of functions that work well is torch.nn.Sigmoid, which includes 1 / (1 + e ** -x), torch.tanh, and others. These functions have a curve that asymptotically approaches 0 or –1 as <em>x</em> goes to negative infinity, approaches 1 as <em>x</em> increases, and have a mostly constant slope at <em>x</em> == <em>0</em>. Conceptually, functions shaped this way work well because there’s an area in the middle of our linear function’s output that our neuron will be sensitive to, while everything else gets lumped next to the boundary values.</p>
<p>As we can see in figure 6.4, our garbage truck gets a score of –0.97, while bears and foxes and wolves end up somewhere in the –0.3 to 0.3 range. This results in garbage trucks being flagged as “not dogs,” our good dog mapping to “clearly a dog,” and our bear ending up somewhere in the middle.</p>
<p><img src="3.png" alt="3" style="zoom:50%;" /></p>
<center>
Figure 6.4 Dogs, bears, and garbage trucks being mapped to how dog-like they are via the tanh activation function
</center>
<p>With the bear in the sensitive range, small changes to the bear will result in a noticeable change to the result.</p>
<h4 id="more-activation-functions">6.1.4 <em>More activation functions</em></h4>
<p>There are quite a few activation functions, some of which are shown in figure 6.5. In the first column, we see the smooth functions Tanh and Softplus, while the second column has “hard” versions of the activation functions to their left: Hardtanh and ReLU(<em>rectified linear unit</em>) .</p>
<p><img src="4.png" alt="4" style="zoom:50%;" /></p>
<center>
Figure 6.5 A collection of common and not-so-common activation functions
</center>
<p>ReLU (for <em>rectified linear unit</em>) is currently considered one of the best-performing general activation functions; The Sigmoid activation function, also known as the <em>logistic function</em>, was widely used in early deep learning work but has since fallen out of common use except where we explicitly want to move to the 0...1 range: for example, when the output should be a probability. Finally, the LeakyReLU function modifies the standard ReLU to have a small positive slope, rather than being strictly zero for negative inputs (typically this slope is 0.01, but it’s shown here with slope 0.1 for clarity).</p>
<h4 id="choosing-the-best-activation-function">6.1.5 <em>Choosing the best activation function</em></h4>
<p>We’re going to discuss some generalities about activation functions that can probably be trivially disproved in the specific. That said, by definition, activation functions</p>
<ul>
<li>Are nonlinear. Repeated applications of (w*x+b) without an activation function results in a function of the same (affine linear) form. The nonlinearity allows the overall network to approximate more complex functions.</li>
<li>Are differentiable, so that gradients can be computed through them. Point discontinuities, as we can see in Hardtanh or ReLU, are fine.</li>
</ul>
<p>Often (but far from universally so), the activation function will have at least one of these:</p>
<ul>
<li>A lower bound that is approached (or met) as the input goes to negative infinity<br />
</li>
<li>A similar-but-inverse upper bound for positive infinity</li>
</ul>
<p>Put together, all this results in a pretty powerful mechanism: we’re saying that in a network built out of linear + activation units, when different inputs are presented to the network</p>
<ol type="a">
<li><p>different units will respond in different ranges for the same inputs</p></li>
<li><p>the errors associated with those inputs will primarily affect the neurons operating in the sensitive range, leaving other units more or less unaffected by the learning process.</p></li>
</ol>
<p>In addition, thanks to the fact that derivatives of the activation with respect to its inputs are often close to 1 in the sensitive range, estimating the parameters of the linear transformation through gradient descent for the units that operate in that range will look a lot like the linear fit we have seen previously.</p>
<p>We are starting to get a deeper intuition for how joining many linear + activation units in parallel and stacking them one after the other leads us to a mathematical object that is capable of approximating complicated functions.</p>
<h4 id="what-learning-means-for-a-neural-network">6.1.6 <em>What learning means for a neural network</em></h4>
<p>What makes using deep neural networks so attractive is that it saves us from worrying too much about the exact function that represents our data.With a deep neural network model, we have a universal approximator and a method to estimate its parameters. This approximator can be customized to our needs, in terms of model capacity and its ability to model complicated input/output relationships, just by composing simple building blocks.</p>
<p><img src="5.png" alt="5" style="zoom:50%;" /></p>
<center>
Figure 6.6 Composing multiple linear units and tanh activation functions to produce nonlinear outputs
</center>
<p>The four upper-left graphs show four neurons—A, B, C, and D—each with its own (arbitrarily chosen) weight and bias. Each neuron uses the Tanh activation function with a min of –1 and a max of 1. The varied weights and biases move the center point and change how drastically the transition from min to max happens, but they clearly all have the same general shape. The columns to the right of those show both pairs of neurons added together (A + B and then C + D). A + B shows a slight <em>S</em> curve, with the extremes approaching 0, but both a positive bump and a negative bump in the middle. Conversely, C + D has only a large positive bump, which peaks at a higher value than our single-neuron max of 1.</p>
<h3 id="the-pytorch-nn-module">6.2 <em>The PyTorch nn module</em></h3>
<p>PyTorch has a whole submodule dedicated to neural networks, called torch.nn. It contains the building blocks needed to create all sorts of neural network architectures. Those building blocks are called <em>modules</em> in PyTorch parlance (such building blocks are often referred to as <em>layers</em> in other frameworks). A PyTorch module is a Python class deriving from the nn.Module base class.</p>
<blockquote>
<p>NOTE The submodules must be top-level <em>attributes</em>, not buried inside list or dict instances! Otherwise, the optimizer will not be able to locate the submodules (and, hence, their parameters). <strong>For situations where your model requires a list or dict of submodules, PyTorch provides nn.ModuleList and nn.ModuleDict.</strong></p>
</blockquote>
<h4 id="using-call-rather-than-forward">6.2.1 <em>Using <strong>call</strong> rather than forward</em></h4>
<p>All PyTorch-provided subclasses of nn.Module have their <code>__call__</code> method defined. This allows us to instantiate an nn.Linear and call it as if it was a function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">linear_model(t_un_val)</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">tensor([[<span class="number">0.6018</span>],</span><br><span class="line">        [<span class="number">0.2877</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>Calling an instance of nn.Module with a set of arguments ends up calling a method named forward with the same arguments. The forward method is what executes the forward computation, while <code>__call__</code> does other rather important chores before and after calling forward. So, it is technically possible to call forward directly, and it will produce the same output as <code>__call__</code>, but this should not be done from user code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = model(x) <span class="comment">#Correct! </span></span><br><span class="line">y = model.forward(x) <span class="comment">#Silent error. Don’t do it!</span></span><br></pre></td></tr></table></figure>
<p>Here’s the implementation of <code>Module._call_</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *<span class="built_in">input</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_pre_hooks.values():</span><br><span class="line">        hook(self, <span class="built_in">input</span>)</span><br><span class="line">    result = self.forward(*<span class="built_in">input</span>, **kwargs)</span><br><span class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</span><br><span class="line">        hook_result = hook(self, <span class="built_in">input</span>, result)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">for</span> hook <span class="keyword">in</span> self._backward_hooks.values():</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>As we can see, there are a lot of hooks that won’t get called properly if we just use <code>.forward(...)</code>directly.</p>
<h4 id="returning-to-the-linear-model">6.2.2 <em>Returning to the linear model</em></h4>
<p>The constructor to nn.Linear accepts three arguments: the number of input features, the number of output features, and whether the linear model includes a bias or not (defaulting to True, here):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">linear_model(t_un_val)</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">tensor([[<span class="number">0.6018</span>],</span><br><span class="line">        [<span class="number">0.2877</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line">linear_model.weight</span><br><span class="line"><span class="comment"># Out[6]:</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-<span class="number">0.0674</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># In[7]:</span></span><br><span class="line">linear_model.bias</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.7488</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>PyTorch <code>nn.Module</code> and its subclasses are designed to do so on multiple samples at the same time. To accommodate multiple samples, modules expect the zeroth dimen- sion of the input to be the number of samples in the <em>batch</em>.</p>
<h5 id="batching-inputs">BATCHING INPUTS</h5>
<p>Any module in nn is written to produce outputs for a <em>batch</em> of multiple inputs at the same time. Thus, assuming we need to run nn.Linear on 10 samples, we can create an input tensor of size <em>B</em> × <em>Nin</em>, where <em>B</em> is the size of the batch and <em>Nin</em> is the number of input features, and run it once through the model. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[9]:</span></span><br><span class="line">x = torch.ones(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">linear_model(x)</span><br><span class="line"><span class="comment"># Out[9]:</span></span><br><span class="line">tensor([[<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>],</span><br><span class="line">        [<span class="number">0.6814</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure>
<p><img src="6.png" alt="6" style="zoom:50%;" /></p>
<center>
Figure 6.7 Three RGB images batched together and fed into a neural network. The output is a batch of three vectors of size 4.
</center>
<h5 id="optimizing-batches">OPTIMIZING BATCHES</h5>
<p>The reason we want to do this batching is multifaceted.</p>
<ul>
<li>One big motivation is to make sure the computation we’re asking for is big enough to saturate the computing resources we’re using to perform the computation. GPUs in particular are highly parallelized, so a single input on a small model will leave most of the computing units idle. By providing batches of inputs, the calculation can be spread across the otherwise-idle units, which means the batched results come back just as quickly as a single result would.</li>
<li>Another benefit is that some advanced models use statistical information from the entire batch, and those statistics get better with larger batch sizes.</li>
</ul>
<p>We reshape our <em>B</em> inputs to <em>B</em> × <em>Nin</em>, where <em>Nin</em> is 1. That is easily done with unsqueeze:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_c = [<span class="number">0.5</span>,  <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>,  <span class="number">8.0</span>,  <span class="number">3.0</span>, -<span class="number">4.0</span>,  <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c).unsqueeze(<span class="number">1</span>)</span><br><span class="line">t_u = torch.tensor(t_u).unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>When training_loss.backward() is called, grad is accumulated on the leaf nodes of the graph, which are precisely the parameters that were passed to the optimizer.</p>
<p>At this point, the SGD optimizer has everything it needs. When optimizer.step() is called, it will iterate through each Parameter and change it by an amount proportional to what is stored in its grad attribute. Pretty clean design.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val</span>):</span></span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">    	t_p_train = model(t_u_train)</span><br><span class="line">    	loss_train = loss_fn(t_p_train, t_c_train)</span><br><span class="line">    	t_p_val = model(t_u_val)</span><br><span class="line">        loss_val = loss_fn(t_p_val, t_c_val)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">		loss_train.backward()	</span><br><span class="line">		optimizer.step()</span><br><span class="line">        </span><br><span class="line">         <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Training loss <span class="subst">&#123;loss_train.item():<span class="number">.4</span>f&#125;</span>,&quot;</span></span><br><span class="line">                  <span class="string">f&quot; Validation loss <span class="subst">&#123;loss_val.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>There’s one last bit that we can leverage from torch.nn: the loss. Indeed, nn comes with several common loss functions, among them <code>nn.MSELoss</code> (MSE stands for Mean Square Error), which is exactly what we defined earlier as our loss_fn. Loss functions in nn are still subclasses of nn.Module, so we will create an instance and call it as a function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer = optim.SGD(linear_model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">3000</span>,</span><br><span class="line">    optimizer = optimizer,</span><br><span class="line">    model = linear_model,</span><br><span class="line">    loss_fn = nn.MSELoss(),</span><br><span class="line">    t_u_train = t_un_train,</span><br><span class="line">    t_u_val = t_un_val,</span><br><span class="line">    t_c_train = t_c_train,</span><br><span class="line">    t_c_val = t_c_val)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(linear_model.weight)</span><br><span class="line"><span class="built_in">print</span>(linear_model.bias)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[15]:</span></span><br><span class="line">Epoch <span class="number">1</span>, Training loss <span class="number">134.9599</span>, Validation loss <span class="number">183.1707</span></span><br><span class="line">Epoch <span class="number">1000</span>, Training loss <span class="number">4.8053</span>, Validation loss <span class="number">4.7307</span></span><br><span class="line">Epoch <span class="number">2000</span>, Training loss <span class="number">3.0285</span>, Validation loss <span class="number">3.0889</span></span><br><span class="line">Epoch <span class="number">3000</span>, Training loss <span class="number">2.8569</span>, Validation loss <span class="number">3.9105</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">5.4319</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-<span class="number">17.9693</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="finally-a-neural-network">6.3 <em>Finally a neural network</em></h3>
<p>There’s one last step left to take: replacing our linear model with a neural network as our approximating function.</p>
<h4 id="replacing-the-linear-model">6.3.1 <em>Replacing the linear model</em></h4>
<p>Let’s build the simplest possible neural network: a linear module, followed by an activation function, feeding into another linear module. The first linear + activation layer is commonly referred to as a <em>hidden</em> layer for historical reasons, since its outputs are not observed directly but fed into the output layer.</p>
<p>There is no standard way to depict neural networks. Figure 6.8 shows two ways that seem to be somewhat prototypical</p>
<p><img src="7.png" alt="7" style="zoom:50%;" /></p>
<center>
Figure 6.8 Our simplest neural network in two views. Left: beginner’s version. Right: higher-level version.
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">seq_model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">1</span>, <span class="number">13</span>),</span><br><span class="line">	nn.Tanh(),</span><br><span class="line">	nn.Linear(<span class="number">13</span>, <span class="number">1</span>))</span><br><span class="line">seq_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[16]:</span></span><br><span class="line">Sequential(</span><br><span class="line">          (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">13</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (<span class="number">1</span>): Tanh()</span><br><span class="line">          (<span class="number">2</span>): Linear(in_features=<span class="number">13</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>The end result is a model that takes the inputs expected by the first module specified as an argument of nn.Sequential, passes intermediate outputs to subsequent modules, and produces the output returned by the last module.</p>
<h4 id="inspecting-the-parameters">6.3.2 <em>Inspecting the parameters</em></h4>
<p>Calling <code>model.parameters()</code> will collect weight and bias from both the first and second linear modules. It’s instructive to inspect the parameters in this case by printing their shapes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line">[param.shape <span class="keyword">for</span> param <span class="keyword">in</span> seq_model.parameters()]</span><br><span class="line"><span class="comment"># Out[17]:</span></span><br><span class="line">[torch.Size([<span class="number">13</span>, <span class="number">1</span>]), torch.Size([<span class="number">13</span>]), torch.Size([<span class="number">1</span>, <span class="number">13</span>]), torch.Size([<span class="number">1</span>])]</span><br></pre></td></tr></table></figure>
<p>A few notes on parameters of nn.Modules. When inspecting parameters of a model made up of several submodules, it is handy to be able to identify parameters by name. There’s a method for that, called named_parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[18]:</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> seq_model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.shape)</span><br><span class="line"><span class="comment"># Out[18]:</span></span><br><span class="line"><span class="number">0.</span>weight torch.Size([<span class="number">13</span>, <span class="number">1</span>])</span><br><span class="line"><span class="number">0.</span>bias torch.Size([<span class="number">13</span>])</span><br><span class="line"><span class="number">2.</span>weight torch.Size([<span class="number">1</span>, <span class="number">13</span>])</span><br><span class="line"><span class="number">2.</span>bias torch.Size([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>The name of each module in Sequential is just the ordinal with which the module appears in the arguments. Interestingly, Sequential also accepts an OrderedDict, in which we can name each module passed to Sequential:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">seq_model = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;hidden_linear&#x27;</span>, nn.Linear(<span class="number">1</span>, <span class="number">8</span>)),</span><br><span class="line">    (<span class="string">&#x27;hidden_activation&#x27;</span>, nn.Tanh()),</span><br><span class="line">    (<span class="string">&#x27;output_linear&#x27;</span>, nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">]))</span><br><span class="line">seq_model</span><br><span class="line"><span class="comment"># Out[19]:</span></span><br><span class="line">Sequential(</span><br><span class="line">    (hidden_linear): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (hidden_activation): Tanh()</span><br><span class="line">    (output_linear): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[20]:</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> seq_model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.shape)</span><br><span class="line"><span class="comment"># Out[20]:</span></span><br><span class="line">hidden_linear.weight torch.Size([<span class="number">8</span>, <span class="number">1</span>])</span><br><span class="line">hidden_linear.bias torch.Size([<span class="number">8</span>])</span><br><span class="line">output_linear.weight torch.Size([<span class="number">1</span>, <span class="number">8</span>])</span><br><span class="line">output_linear.bias torch.Size([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>We can also access a particular Parameter by using submodules as attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seq_model.output_linear.bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[21]:</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.0173</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>We can run the training loop for the new neural network model and then look at the resulting gradients after the last epoch:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(seq_model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">5000</span>,</span><br><span class="line">    optimizer = optimizer,</span><br><span class="line">    model = seq_model,</span><br><span class="line">    loss_fn = nn.MSELoss(),</span><br><span class="line">    t_u_train = t_un_train,</span><br><span class="line">    t_u_val = t_un_val,</span><br><span class="line">    t_c_train = t_c_train,</span><br><span class="line">    t_c_val = t_c_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output&#x27;</span>, seq_model(t_un_val))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;answer&#x27;</span>, t_c_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;hidden&#x27;</span>, seq_model.hidden_linear.weight.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[22]:</span></span><br><span class="line">Epoch <span class="number">1</span>, Training loss <span class="number">182.9724</span>, Validation loss <span class="number">231.8708</span></span><br><span class="line">Epoch <span class="number">1000</span>, Training loss <span class="number">6.6642</span>, Validation loss <span class="number">3.7330</span></span><br><span class="line">Epoch <span class="number">2000</span>, Training loss <span class="number">5.1502</span>, Validation loss <span class="number">0.1406</span></span><br><span class="line">Epoch <span class="number">3000</span>, Training loss <span class="number">2.9653</span>, Validation loss <span class="number">1.0005</span></span><br><span class="line">Epoch <span class="number">4000</span>, Training loss <span class="number">2.2839</span>, Validation loss <span class="number">1.6580</span></span><br><span class="line">Epoch <span class="number">5000</span>, Training loss <span class="number">2.1141</span>, Validation loss <span class="number">2.0215</span></span><br><span class="line">output tensor([[-<span class="number">1.9930</span>],</span><br><span class="line">               [<span class="number">20.8729</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">answer tensor([[-<span class="number">4.</span>],</span><br><span class="line">               [<span class="number">21.</span>]])</span><br><span class="line">hidden tensor([[ <span class="number">0.0272</span>],</span><br><span class="line">               [ <span class="number">0.0139</span>],</span><br><span class="line">               [ <span class="number">0.1692</span>],</span><br><span class="line">               [ <span class="number">0.1735</span>],</span><br><span class="line">               [-<span class="number">0.1697</span>],</span><br><span class="line">               [ <span class="number">0.1455</span>],</span><br><span class="line">               [-<span class="number">0.0136</span>],</span><br><span class="line">               [-<span class="number">0.0554</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="comparing-to-the-linear-model">6.3.3 <em>Comparing to the linear model</em></h4>
<p>We can also evaluate the model on all of the data and see how it differs from a line:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_t_u = normalize(t_u[train_index]).unsqueeze(<span class="number">1</span>)</span><br><span class="line">train_t_c = t_c[train_index].unsqueeze(<span class="number">1</span>)</span><br><span class="line">val_t_u = normalize(t_u[val_index]).unsqueeze(<span class="number">1</span>)</span><br><span class="line">val_t_c = t_c[val_index].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">SinglePlot.init(x_name=<span class="string">&#x27;Fahrenheit&#x27;</span>, y_name=<span class="string">&#x27;Celsius&#x27;</span>, title=<span class="string">&#x27;Model Plot&#x27;</span>, tick_size=<span class="number">12</span>, font_size=<span class="number">15</span>)</span><br><span class="line">SinglePlot.plot_scatter(t_u.numpy(), t_c.numpy())</span><br><span class="line">t_range = torch.arange(<span class="number">20.0</span>, <span class="number">90.0</span>)</span><br><span class="line">SinglePlot.plot_curve(t_range, model(normalize(t_range, t_u.mean(<span class="number">0</span>), t_u.var(<span class="number">0</span>)).unsqueeze(<span class="number">1</span>)).detach().numpy())</span><br><span class="line">SinglePlot.plot_x(t_u.numpy(), model(normalize(t_u).unsqueeze(<span class="number">1</span>)).detach().numpy())</span><br><span class="line">SinglePlot.show()</span><br></pre></td></tr></table></figure>
<p><img src="8.png" alt="8" style="zoom:50%;" /></p>
<center>
Figure 6.9 The plot of our neural network model, with input data (circles) and model output (Xs). The continuous line shows behavior between samples.
</center>
<h3 id="conclusion">6.4 Conclusion</h3>
<p>We dissected building differentiable models and training them using gradient descent, first using raw autograd and then relying on nn. By now you should have confidence in your understanding of what’s going on behind the scenes.</p>
<h2 id="chapter-7-telling-birds-from-airplanes-learning-from-images">Chapter 7 <em>Telling birds from airplanes: Learning from images</em></h2>
<ul>
<li>Building a feed-forward neural network</li>
<li>Loading data using Datasets and DataLoaders</li>
<li>Understanding classification loss</li>
</ul>
<p>In this chapter, we’ll keep moving ahead with building our neural network foun- dations. This time, we’ll turn our attention to images. Image recognition is arguably the task that made the world realize the potential of deep learning.</p>
<p>We will approach a simple image recognition problem step by step, building from a simple neural network like the one we defined in the last chapter. This time, instead of a tiny dataset of numbers, we’ll use a more extensive dataset of tiny images.</p>
<h4 id="a-dataset-of-tiny-images">7.1 <em>A dataset of tiny images</em></h4>
<p>One of the most basic datasets for image recognition is the handwritten digit-recognition dataset known as MNIST. Here we will use another dataset that is similarly simple and a bit more fun. It’s called CIFAR-10, and, like its sibling CIFAR-100, it has been a computer vision classic for a decade.</p>
<p>CIFAR-10 consists of 60,000 tiny 32 × 32 color (RGB) images, labeled with an inte- ger corresponding to 1 of 10 classes: airplane (0), automobile (1), bird (2), cat (3), deer (4), dog (5), frog (6), horse (7), ship (8), and truck (9).</p>
<p><img src="9.png" alt="9" style="zoom:50%;" /></p>
<center>
Figure 7.1 Image samples from all CIFAR-10 classes
</center>
<h4 id="downloading-cifar-10">7.1.1 <em>Downloading CIFAR-10</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line">data_path = <span class="string">&#x27;../data-unversioned/p1ch7/&#x27;</span></span><br><span class="line">cifar10 = datasets.CIFAR10(data_path, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">cifar10_val = datasets.CIFAR10(data_path, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>the dataset is returned as a subclass of <code>torch.utils.data.Dataset</code>. We can see that the method-resolution order of our cifar10 instance includes it as a base class:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line"><span class="built_in">type</span>(cifar10).__mro__</span><br><span class="line"><span class="comment"># Out[4]:</span></span><br><span class="line">(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">torchvision</span>.<span class="title">datasets</span>.<span class="title">cifar</span>.<span class="title">CIFAR10</span>&#x27;&gt;, </span></span><br><span class="line"><span class="class"> &lt;<span class="title">class</span> &#x27;<span class="title">torchvision</span>.<span class="title">datasets</span>.<span class="title">vision</span>.<span class="title">VisionDataset</span>&#x27;&gt;, </span></span><br><span class="line"><span class="class"> &lt;<span class="title">class</span> &#x27;<span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">dataset</span>.<span class="title">Dataset</span>&#x27;&gt;, </span></span><br><span class="line"><span class="class"> &lt;<span class="title">class</span> &#x27;<span class="title">typing</span>.<span class="title">Generic</span>&#x27;&gt;, </span></span><br><span class="line"><span class="class"> &lt;<span class="title">class</span> &#x27;<span class="title">object</span>&#x27;&gt;)</span></span><br></pre></td></tr></table></figure>
<h4 id="the-dataset-class">7.1.2 The Dataset class</h4>
<p>PyTorch Dataset is all about. It is an object that is required to implement two methods: <code>__len__ and __getitem</code>__. The former should return the number of items in the dataset; the latter should return the item, consisting of a sample and its corresponding label (an integer index)</p>
<p>In practice, when a Python object is equipped with the <code>__len__</code>method, we can pass it as an argument to the len Python built-in function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"><span class="built_in">len</span>(cifar10)</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line"><span class="number">50000</span></span><br></pre></td></tr></table></figure>
<p><img src="10.png" alt="10" style="zoom:50%;" /></p>
<center>
Figure 7.2 Concept of a PyTorch Dataset object: it doesn’t necessarily hold the data, but it provides uniform access to it through <strong>len</strong> and <strong>getitem</strong>.
</center>
<h4 id="dataset-transforms">7.1.3 <em>Dataset transforms</em></h4>
<p>That’s all very nice, but we’ll likely need a way to convert the PIL image to a PyTorch tensor before we can do anything with it. That’s where torchvision.transforms comes in. This module defines a set of composable, function-like objects that can be passed as an argument to a torchvision dataset such as datasets.CIFAR10(...), and that perform transformations on the data after it is loaded but before it is returned by <code>__getitem__</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="built_in">dir</span>(transforms)</span><br><span class="line"><span class="comment"># Out[8]:</span></span><br><span class="line">[<span class="string">&#x27;CenterCrop&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ColorJitter&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line"> <span class="string">&#x27;Normalize&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Pad&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;RandomAffine&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line"> <span class="string">&#x27;RandomResizedCrop&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;RandomRotation&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;RandomSizedCrop&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line"> <span class="string">&#x27;TenCrop&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ToPILImage&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ToTensor&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><code>ToTensor</code>, which turns NumPy arrays and PIL images to tensors. It also takes care to lay out the dimensions of the output tensor as <em>C</em> × <em>H</em> × <em>W</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.tramforms <span class="keyword">import</span> ToTensor</span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">img_t = to_tensor(img)</span><br><span class="line">img_t.shape</span><br><span class="line"><span class="comment"># Out[9]:</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br></pre></td></tr></table></figure>
<p>The image has been turned into a 3 × 32 × 32 tensor and therefore a 3-channel (RGB) 32 × 32 image. Note that nothing has happened to label; it is still an integer. we can pass the transform directly as an argument to dataset .CIFAR10:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_cifar10 = datasets.CIFAR10(data_path, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=ToTensor())</span><br></pre></td></tr></table></figure>
<p>Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per channel), the ToTensor transform turns the data into a 32-bit floating-point per channel, scaling the values down from 0.0 to 1.0.</p>
<h4 id="normalizing-data">7.1.4 <em>Normalizing data</em></h4>
<p>Transforms are really handy because we can chain them using transforms.Compose, and they can handle normalization and data augmentation transparently, directly in the data loader. For instance, it’s good practice to normalize the dataset so that each channel has zero mean and unitary standard deviation.</p>
<p><strong>we also have an intuition for why: by choosing activation functions that are linear around 0 plus or minus 1 (or 2), keeping the data in the same range means it’s more likely that neurons have nonzero gradients and hence will learn sooner. Also, normalizing each channel so that it has the same distribution will ensure that channel information can be mixed and updated through gradient descent using the same learning rate.</strong></p>
<p>Since the CIFAR-10 dataset is small, we’ll be able to manipulate it entirely in mem- ory. Let’s stack all the tensors returned by the dataset along an extra dimension:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[15]:</span></span><br><span class="line">imgs = torch.stack([img_t <span class="keyword">for</span> img_t, _ <span class="keyword">in</span> tensor_cifar10], dim=<span class="number">3</span>)</span><br><span class="line">imgs.shape</span><br><span class="line"><span class="comment"># Out[15]:</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">50000</span>])</span><br><span class="line"><span class="comment"># In[16]:</span></span><br><span class="line">imgs.view(<span class="number">3</span>, -<span class="number">1</span>).mean(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Out[16]:</span></span><br><span class="line">tensor([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>])</span><br><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line">imgs.view(<span class="number">3</span>, -<span class="number">1</span>).std(dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Out[17]:</span></span><br><span class="line">tensor([<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>])</span><br></pre></td></tr></table></figure>
<p>With these numbers in our hands, we can initialize the Normalize transform</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transformer = torchvision.transforms.Compose([</span><br><span class="line">        ToTensor(),</span><br><span class="line">        Normalize(mean=[<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>], </span><br><span class="line">                  std=[<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>])</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>
<p>plotting an image drawn from the dataset won’t provide us with a faithful representation of the actual image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = training_data[<span class="number">99</span>][<span class="number">0</span>]</span><br><span class="line">plt.imshow(img.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="11.png" alt="11" style="zoom:40%;" /></p>
<center>
Figure 7.5 Our random CIFAR-10 image after normalization
</center>
<h3 id="distinguishing-birds-from-airplanes">7.2 <em>Distinguishing birds from airplanes</em></h3>
<p>Jane, our friend at the bird-watching club, has set up a fleet of cameras in the woods south of the airport. The cameras are supposed to save a shot when something enters the frame and upload it to the club’s real-time bird-watching blog. The problem is that a lot of planes coming and going from the airport end up triggering the camera, so Jane spends a lot of time deleting pictures of airplanes from the blog. What she needs is an automated system.</p>
<p><img src="12.png" alt="11" style="zoom:50%;" /></p>
<h4 id="building-the-dataset">7.2.1 <em>Building the dataset</em></h4>
<p>The first step is to get the data in the right shape. We could create a Dataset subclass that only includes birds and airplanes. However, the dataset is small, and we only need indexing and len to work on our dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">label_map = &#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">2</span>: <span class="number">1</span>&#125;</span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>]</span><br><span class="line">cifar2 = [(img, label_map[label]) <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10 <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">cifar2_val = [(img, label_map[label]) <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar10_val <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
<h4 id="a-fully-connected-model">7.2.2 <em>A fully connected model</em></h4>
<p>We know that it’s a tensor of features in, a tensor of features out. After all, an image is just a set of numbers laid out in a spatial configuration. in theory if we just take the image pixels and straighten them into a long 1D vector, we could consider those numbers as input features</p>
<p><img src="13.png" alt="11" style="zoom:50%;" /></p>
<center>
Figure 7.7 Treating our image as a 1D vector of values and training a fully connected classifier on it
</center>
<p>followed by an activation, and then another nn.Linear that tapers the network down to an appropriate output number of features (2, for this use case):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">n_out = <span class="number">2</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3072</span>, <span class="number">512</span>),</span><br><span class="line">    		nn.Tanh(),</span><br><span class="line">    		nn.Linear(<span class="number">512</span>, n_out)</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<p>We somewhat arbitrarily pick 512 hidden features. A neural network needs at least one hidden layer (of activations, so two modules) with a nonlinearity in between in order to be able to learn arbitrary functions</p>
<h4 id="output-of-a-classifier">7.2.3 <em>Output of a classifier</em></h4>
<ul>
<li>make our net- work output a single scalar value (so n_out = 1), cast the labels to floats (0.0 for air- plane and 1.0 for bird), and use those as a target for MSELoss (the average of squared differences in the batch). Doing so, we would cast the problem into a <strong>regression problem</strong>.</li>
<li>However, We need to recognize that the output is categorical: it’s either a bird or an airplane (or something else if we had all 10 of the original classes). As we learned in chapter 4, when we have to represent a categorical variable, we should switch to a one-hot-encoding representation of that variable, such as [1, 0] for airplane or [0, 1] for bird (the order is arbitrary).</li>
</ul>
<p>In the ideal case, the network would output torch.tensor([1.0, 0.0]) for an airplane and torch.tensor([0.0, 1.0]) for a bird. The key realization in this case is that we can interpret our output as probabilities: the first entry is the probability of “airplane,” and the second is the probability of “bird.”</p>
<p>The elements of the output must add up to 1.0 (we’re certain that one of the two outcomes will occur). there’s a very smart trick that does exactly that, and it’s differentiable: it’s called <em>softmax</em>.</p>
<h4 id="representing-the-output-as-probabilities">7.2.4 <em>Representing the output as probabilities</em></h4>
<p>Softmax is a function that takes a vector of values and produces another vector of the same dimension, where the values satisfy the constraints we just listed to represent probabilities.</p>
<p><img src="14.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 7.8 Handwritten softmax
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.exp(x) / torch.exp(x).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">softmax(x)</span><br><span class="line"><span class="comment"># Out[8]:</span></span><br><span class="line">tensor([<span class="number">0.0900</span>, <span class="number">0.2447</span>, <span class="number">0.6652</span>])</span><br><span class="line"><span class="comment"># In[9]:</span></span><br><span class="line">softmax(x).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># Out[9]:</span></span><br><span class="line">tensor(<span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<p>Softmax is a monotone function, in that lower values in the input will correspond to lower values in the output. However, it’s not <em>scale invariant</em>, in that the ratio between values is not preserved. In fact, the ratio between the first and second elements of the input is 0.5, while the ratio between the same elements in the output is 0.3678.</p>
<p>The nn module makes softmax available as a module.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[10]:</span></span><br><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">x = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],</span><br><span class="line">                  [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">softmax(x)</span><br><span class="line"><span class="comment"># Out[10]:</span></span><br><span class="line">tensor([[<span class="number">0.0900</span>, <span class="number">0.2447</span>, <span class="number">0.6652</span>],</span><br><span class="line">        [<span class="number">0.0900</span>, <span class="number">0.2447</span>, <span class="number">0.6652</span>]])</span><br></pre></td></tr></table></figure>
<p>We can now add a softmax at the end of our model, and our network will be equipped to produce probabilities:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3072</span>, <span class="number">512</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[14]:</span></span><br><span class="line">out = model(img_batch)</span><br><span class="line">out</span><br><span class="line"><span class="comment"># Out[14]:</span></span><br><span class="line">tensor([[<span class="number">0.4784</span>, <span class="number">0.5216</span>]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[15]:</span></span><br><span class="line">_, index = torch.<span class="built_in">max</span>(out, dim=<span class="number">1</span>)</span><br><span class="line">index</span><br><span class="line"><span class="comment"># Out[15]:</span></span><br><span class="line">tensor([<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>torch.max</code> returns the maximum element along that dimension as well as the index at which that value occurs.</p>
<h4 id="a-loss-for-classifying">7.2.5 <em>A loss for classifying</em></h4>
<p>In chapters 5 and 6, we used mean square error (MSE) as our loss. We could still use MSE and make our output probabilities converge to [0.0, 1.0] and [1.0, 0.0]. However, thinking about it, we’re not really interested in reproducing these values exactly. Looking back at the argmax operation we used to extract the index of the predicted class, what we’re really interested in is that the first probability is higher than the second for airplanes and vice versa for birds. In other words, <strong>we want to penalize misclassifications rather than painstakingly penalize everything that doesn’t look exactly like a 0.0 or 1.0.</strong></p>
<p>What we need to maximize in this case is the probability associated with the correct class, <code>out[class_index]</code>. the loss should be low when the likelihood is higher than the alternatives, and we’re not really fixated on driving the probability up to 1.0</p>
<p>There’s a loss function that behaves that way, and it’s called <strong><em>negative log likelihood</em> (NLL)</strong>. It has the expression <code>NLL = - sum(log(out_i[c_i]))</code>, where the sum is taken over <em>N</em> samples and c_i is the correct class for sample <em>i</em>. Let’s take a look at figure 7.10, which shows the NLL as a function of predicted probability.</p>
<p><img src="15.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 7.10 The NLL loss as a function of the predicted probabilities
</center>
<p>Summing up, our loss for classification can be computed as follows. For each sample in the batch:</p>
<ol type="1">
<li>Run the forward pass, and obtain the output values from the last (linear) layer.</li>
<li>Compute their softmax, and obtain probabilities.</li>
<li>Take the predicted probability corresponding to the correct class (the likelihood of the parameters). Note that we know what the correct class is because it’s a supervised problem—it’s our ground truth.</li>
<li>Compute its logarithm, slap a minus sign in front of it, and add it to the loss.</li>
</ol>
<p>PyTorch has an <code>nn.NLLLoss</code> class. However (gotcha ahead), it does not take probabilities but rather takes a tensor of log probabilities as input. <strong>There’s a good reason behind the input convention: taking the logarithm of a probability is tricky when the probability gets close to zero.</strong> The workaround is to use nn.LogSoftmax instead of nn.Softmax, which takes care to make the calculation numerically stable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">             nn.Linear(<span class="number">3072</span>, <span class="number">512</span>),</span><br><span class="line">             nn.Tanh(),</span><br><span class="line">             nn.Linear(<span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">             nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">loss = nn.NLLLoss()</span><br></pre></td></tr></table></figure>
<p>The loss takes the output of nn.LogSoftmax for a batch as the first argument and a tensor of class indices (zeros and ones, in our case) as the second argument.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img, label = cifar2[<span class="number">0</span>]</span><br><span class="line">out = model(img.view(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>))</span><br><span class="line">loss(out, torch.tensor([label]))</span><br><span class="line">tensor(<span class="number">0.6509</span>, grad_fn=&lt;NllLossBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>Ending our investigation of losses, we can look at how using cross-entropy loss improves over MSE. In figure 7.11, we see that the cross-entropy loss has some slope when the prediction is off target(in the low-loss corner, the correct class is assigned a predicted probability of 99.97%), while the MSE we dismissed at the beginning saturates much earlier and—crucially—also for very wrong predictions. <strong>The underlying reason is that the slope of the MSE is too low to compensate for the flatness of the softmax function for wrong predictions. This is why the MSE for probabilities is not a good fit for classification work.</strong></p>
<p><img src="16.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 7.11 The cross entropy (left) and MSE between predicted probabilities and the target probability vector (right) as functions of the predicted scores—that is, before the (log-) softmax
</center>
<h4 id="training-the-classifier">7.2.6 <em>Training the classifier</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> cifar2:</span><br><span class="line">        out = model(img.view(-<span class="number">1</span>).unsqueeze(<span class="number">0</span>))</span><br><span class="line">        loss = loss_fn(out, torch.tensor([label]))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: %d, Loss: %f&quot;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br></pre></td></tr></table></figure>
<p>SGD stands for <strong><em>stochastic gradient descent</em></strong>, and this is what the <em>S</em> is about: working on small batches (aka minibatches) of shuffled data. It turns out that following gradients estimated over minibatches, which are poorer approximations of gradients estimated across the whole dataset, helps convergence and prevents the optimization process from getting stuck in local minima it encounters along the way.</p>
<p>As depicted in figure 7.13, gradients from minibatches are randomly off the ideal trajectory, which is part of the reason why we want to use a reasonably small learning rate. Shuffling the dataset at each epoch helps ensure that the sequence of gradients estimated over mini- batches is representative of the gradients computed across the full dataset.</p>
<p><img src="17.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 7.13 Gradient descent averaged over the whole dataset (light path) versus stochastic gradient descent, where the gradient is estimated on randomly picked minibatches
</center>
<p>In our training code, we chose minibatches of size 1 by picking one item at a time from the dataset. The torch.utils.data module has a class that helps with shuffling and organizing the data in minibatches: DataLoader. The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose from different sampling strategies. A very common strategy is uniform sampling after shuffling the data at each epoch. Figure 7.14 shows the data loader shuffling the indices it gets from the Dataset.</p>
<p><img src="18.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 7.14 A data loader dispensing minibatches by using a dataset to sample individual data items
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(cifar2, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3072</span>, <span class="number">512</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">            nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line">loss_fn = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch_size = imgs.shape[<span class="number">0</span>]</span><br><span class="line">        outputs = model(imgs.view(batch_size, -<span class="number">1</span>))</span><br><span class="line">		loss = loss_fn(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">		loss.backward()</span><br><span class="line">		optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: %d, Loss: %f&quot;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br></pre></td></tr></table></figure>
<p>Since our goal here is to correctly assign classes to images, and preferably do that on an independent dataset, we can compute the accuracy of our model on the validation set in terms of the number of correct classifications over the total:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> val_loader:</span><br><span class="line">        batch_size = imgs.shape[<span class="number">0</span>]</span><br><span class="line">        outputs = model(imgs.view(batch_size, -<span class="number">1</span>))</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        total += labels.shape[<span class="number">0</span>]</span><br><span class="line">        correct += <span class="built_in">int</span>((predicted == labels).<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %f&quot;</span>, correct / total)</span><br><span class="line"></span><br><span class="line">val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> val_loader:</span><br><span class="line">        batch_size = imgs.shape[<span class="number">0</span>]</span><br><span class="line">        outputs = model(imgs.view(batch_size, -<span class="number">1</span>))</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        total += labels.shape[<span class="number">0</span>]</span><br><span class="line">        correct += <span class="built_in">int</span>((predicted == labels).<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: %f&quot;</span>, correct / total)</span><br><span class="line">Accuracy: <span class="number">0.794000</span></span><br></pre></td></tr></table></figure>
<p>We can certainly add some bling to our model by including more layers, which will increase the model’s depth and capacity. One rather arbitrary possibility is</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3072</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">            nn.LogSoftmax(dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>Here we are trying to taper the number of features more gently toward the output, in the hope that intermediate layers will do a better job of squeezing information in increasingly shorter intermediate outputs.</p>
<p><strong>The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss</strong>.Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs.</p>
<p>To add to the confusion, in information theory, up to normalization by sample size, this cross entropy can be interpreted as a negative log likelihood of the predicted dis- tribution under the target distribution as an outcome. So both losses are the negative log likelihood of the model parameters given the data when our model predicts the (softmax-applied) probabilities.</p>
<p>It is quite common to drop the last nn.LogSoftmax layer from the network and use nn.CrossEntropyLoss as a loss. Let us try that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3072</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">512</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>Training this model and evaluating the accuracy on the validation set (0.802000) lets us appreciate that a larger model bought us an increase in accuracy, but not that much. The accuracy on the training set is practically perfect (0.998100).</p>
<p>To find out how many elements are in each tensor instance, we can call the numel method. Summing those gives us our total count. Depending on our use case, counting parameters might require us to check whether a parameter has requires_grad set to True, as well.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numel_list = [p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> connected_model.parameters() <span class="keyword">if</span> p.requires_grad == <span class="literal">True</span>]</span><br><span class="line"><span class="built_in">sum</span>(numel_list), numel_list</span><br><span class="line">(<span class="number">3737474</span>, [<span class="number">3145728</span>, <span class="number">1024</span>, <span class="number">524288</span>, <span class="number">512</span>, <span class="number">65536</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h4 id="the-limits-of-going-fully-connected">7.2.7 <em>The limits of going fully connected</em></h4>
<p>Let’s reason about what using a linear module on a 1D view of our image entails—figure 7.15 shows what is going on. It’s like taking every single input value—that is, every single component in our RGB image—and computing a linear combination of it with all the other values for every output feature.</p>
<p>On one hand, we are allowing for the combination of any pixel with every other pixel in the image being potentially relevant for our task. On the other hand, we aren’t utilizing the relative position of neighboring or faraway pixels, since we are treating the image as one big vector of numbers.</p>
<p><img src="19.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 7.15 Using a fully connected module with an input image: every input pixel is combined with every other to produce each element in the output.
</center>
<p>An airplane flying in the sky captured in a 32 × 32 image will be very roughly similar to a dark, cross-like shape on a blue background. A fully connected network as in figure 7.15 would need to learn that when pixel 0,1 is dark, pixel 1,1 is also dark, and so on.This is illustrated in the top half of figure 7.16. In more technical terms, a fully connected network is not <strong><em>translation invariant</em>.</strong> We would then have to <em>augment</em> the dataset—that is, apply random translations to images during training— so the network would have a chance to see Spitfires all over the image, and we would need to do this for every image in the dataset</p>
<p><img src="20.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 7.16 Translation invariance, or the lack thereof, with fully connected layers
</center>
<p>However, this <em>data augmentation</em> strategy comes at a cost: the number of hidden features—that is, of parameters—must be large enough to store the information about all of these translated replicas.</p>
<h2 id="chapter-8-using-convolutions-to-generalize">Chapter 8 <em>Using convolutions to generalize</em></h2>
<ul>
<li>Understanding convolution</li>
<li>Building a convolutional neural network</li>
<li>Creating custom nn.Module subclasses</li>
<li>The difference between the module and functional APIs</li>
<li>Design choices for neural networks</li>
</ul>
<p>in the last chapter, we could augment our training data by using a wide variety of recropped images to try to force generalization, but that won’t address the issue of having too many parameters.</p>
<p>There is a better way! It consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: convolution.</p>
<h3 id="the-case-for-convolutions">8.1 <em>The case for convolutions</em></h3>
<p>if we want to recognize patterns corresponding to objects, like an airplane in the sky, we will likely need to look at how nearby pixels are arranged, and we will be less interested in how pixels that are far from each other appear in combination. In order to translate this intuition into mathematical form, we could compute the weighted sum of a pixel with its immediate neighbors, rather than with all other pixels in the image. This would be equivalent to building weight matrices, one per output feature and output pixel location, in which all weights beyond a certain distance from a center pixel are zero.</p>
<h4 id="what-convolutions-do">8.1.1 <em>What convolutions do</em></h4>
<p>We identified one more desired property earlier: we would like these localized patterns to have an effect on the output regardless of their location in the image: that is, to be <strong><em>translation invariant</em>.</strong> To achieve this goal in a matrix applied to the image-as-a-vector would require implementing a rather complicated pattern of weights：most of the weight matrix would be zero (for entries corresponding to input pixels too far away from the output pixel to have an influence). For other weights, we would have to find a way to keep entries in sync that correspond to the same relative position of input and output pixels. This means we would need to initialize them to the same values and ensure that all these <em>tied</em> weights stayed the same while the network is updated during training. This way, we would ensure that weights operate in neighborhoods to respond to local patterns, and local patterns are identified no matter where they occur in the image.</p>
<p>Convolution, or more precisely, <strong><em>discrete convolution</em></strong> (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the <em>kernel</em>, with every neighborhood in the input. Figure 8.1 shows this computation in action.</p>
<p>Note that the same kernel, and thus each weight in the kernel, is reused across the whole image. Thinking back to autograd, this means the use of each weight has a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.</p>
<p><img src="21.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.1 Convolution: locality and translation invariance
</center>
<p>Summarizing, by switching to convolutions, we get</p>
<ul>
<li>Local operations on neighborhoods</li>
<li>Translation invariance</li>
<li>Models with a lot fewer parameters</li>
</ul>
<h3 id="convolutions-in-action">8.2 <em>Convolutions in action</em></h3>
<p>The torch.nn module provides convolutions for 1, 2, and 3 dimensions: nn.Conv1d for time series, nn.Conv2d for images, and nn.Conv3d for volumes or videos. For our CIFAR-10 data, we’ll resort to nn.Conv2d. At a minimum, the arguments we provide to nn.Conv2d are <strong>the number of input features , the number of output features, and the size of the kernel</strong>. It is very common to have kernel sizes that are the same in all directions, so PyTorch has a shortcut for this: whenever kernel_size=3 is specified for a 2D convolution, it means 3 × 3 (provided as a tuple (3, 3) in Python).</p>
<p>The kernel is of size 3 × 3, so we want the weight to consist of 3 × 3 parts. For a single output pixel value, our kernel would consider, say, in_ch = 3 input channels, so the weight component for a single output pixel value (and by translation the invariance for the entire output channel) is of shape in_ch × 3 × 3. Finally, we have as many of those as we have output channels, here out_ch = 16, so the complete weight tensor is out_ch × in_ch × 3 × 3, in our case 16 × 3 × 3 × 3. The bias will have size 16</p>
<h4 id="padding-the-boundary">8.2.1 <em>Padding the boundary</em></h4>
<p>The fact that our output image is smaller than the input is a side effect of deciding what to do at the boundary of the image. Applying a convolution kernel as a weighted sum of pixels in a 3 × 3 neighborhood requires that there are neighbors in all directions. By default, PyTorch will slide the convolution kernel within the input picture, getting <code>width - kernel_width + 1</code> horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, 3//2 = 1) smaller on each side.</p>
<p>PyTorch gives us the possibility of <em>padding</em> the image by creating <em>ghost</em> pixels around the border that have value zero as far as the convolution is concerned. Figure 8.3 shows padding in action.</p>
<p><img src="22.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.3 Zero padding to preserve the image size in the output
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv = nn.Conv2d(<span class="number">3</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">output = conv(img.unsqueeze(<span class="number">0</span>))</span><br><span class="line">img.unsqueeze(<span class="number">0</span>).shape, output.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[16]:</span></span><br><span class="line">(torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]), torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>]))</span><br></pre></td></tr></table></figure>
<p>There are two main reasons to pad convolutions</p>
<ul>
<li>doing so helps us separate the matters of convolution and changing image sizes, so we have one less thing to remember.</li>
<li>when we have more elaborate structures such as skip connections (discussed in section 8.5.3) or the U-Nets we’ll cover in part 2, we want the tensors before and after a few convolutions to be of compatible size so that we can add them or take differences.</li>
</ul>
<h4 id="detecting-features-with-convolutions">8.2.2 <em>Detecting features with convolutions</em></h4>
<p>We said earlier that weight and bias are parameters that are learned through back- propagation, exactly as it happens for weight and bias in nn.Linear. However, <strong>we can play with convolution by setting weights by hand and see what happens.</strong></p>
<p>Let’s first zero out bias, just to remove any confounding factors, and then set weights to a constant value so that each pixel in the output gets the mean of its neighbors.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    conv.bias.zero_()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    conv.weight.fill_(<span class="number">1.0</span> / <span class="number">9.0</span>)</span><br><span class="line"></span><br><span class="line">output = conv(img.unsqueeze(<span class="number">0</span>))</span><br><span class="line">plt.imshow(output[<span class="number">0</span>, <span class="number">0</span>].detach(), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="23.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.4 Our bird, this time blurred thanks to a constant convolution kernel
</center>
<p>Next, let’s try something different. The following kernel may look a bit mysterious at first:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv = nn.Conv2d(<span class="number">3</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    conv.weight[:] = torch.tensor([[-<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>],</span><br><span class="line">                                   [-<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>],</span><br><span class="line">								   [-<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]]),</span><br><span class="line">conv.bias.zero_()</span><br></pre></td></tr></table></figure>
<p>It’s an <strong><em>edge-detection</em></strong> kernel: the kernel highlights the vertical edge between two horizontally adjacent regions.</p>
<p>Applying the convolution kernel to our image, we see the result shown in figure 8.5. As expected, the convolution kernel enhances the vertical edges.</p>
<p><img src="24.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.5 Vertical edges throughout our bird, courtesy of a handcrafted convolution kernel
</center>
<p>the job of a convolutional neural network is to estimate the ker- nel of a set of filter banks in successive layers that will transform a multichannel image into another multichannel image, where different channels correspond to different features Figure 8.6 shows how the training automatically learns the kernels.</p>
<p><img src="25.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.6 The process of learning with convolutions by estimating the gradient at the kernel weights and updating them individually in order to optimize for the loss
</center>
<h4 id="looking-further-with-depth-and-pooling">8.2.4 <em>Looking further with depth and pooling</em></h4>
<p>stacking one convolution after the other and at the same time downsampling the image between successive convolutions.</p>
<h5 id="from-large-to-small-downsampling">FROM LARGE TO SMALL: DOWNSAMPLING</h5>
<p>Downsampling could in principle occur in different ways. Scaling an image by half is the equivalent of taking four neighboring pixels as input and producing one pixel as output. How we compute the value of the output based on the values of the input is up to us. We could</p>
<ul>
<li><p><em>Average the four pixels.</em> This <em>average pooling</em> was a common approach early on but has fallen out of favor somewhat.</p></li>
<li><p><em>Take the maximum of the four pixels.</em> This approach, called <em>max pooling,</em> is currently the most commonly used approach, but it has a downside of discarding the other three-quarters of the data.</p></li>
<li><p><em>Perform a strided convolution, where only every Nth pixel is calculated.</em> A 3 × 4 convolu- tion with stride 2 still incorporates input from all pixels from the previous layer. The literature shows promise for this approach, but it has not yet supplanted max pooling.</p></li>
</ul>
<p><img src="26.png" alt="14" style="zoom:40%;" /></p>
<center>
Figure 8.7 Max pooling in detail
</center>
<p>Intuitively, the output images from a convolution layer, especially since they are fol- lowed by an activation just like any other linear layer, tend to have a high magnitude where certain features corresponding to the estimated kernel are detected (such as vertical lines).</p>
<h5 id="combining-convolutions-and-downsampling-for-great-good">COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD</h5>
<p>Let’s now see how combining convolutions and downsampling can help us recognize larger structures.</p>
<blockquote>
<p>In figure 8.8, we start by applying a set of 3 × 3 kernels on our 8 × 8 image, obtaining a multichannel output image of the same size. Then we scale down the output image by half, obtaining a 4 × 4 image, and apply another set of 3 × 3 ker- nels to it. This second set of kernels operates on a 3 × 3 neighborhood of something that has been scaled down by half, so it effectively maps back to 8 × 8 neighborhoods of the input.</p>
</blockquote>
<p><img src="27.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 8.8 More convolutions by hand, showing the effect of stacking convolutions and downsampling: a large cross is highlighted using two small, cross-shaped kernels and max pooling.
</center>
<blockquote>
<p><strong>The receptive field of output pixels</strong></p>
<p>When the second 3 × 3 convolution kernel produces 21 in its conv output in figure 8.8, this is based on the top-left 3 × 3 pixels of the first max pool output. They, in turn, correspond to the 6 × 6 pixels in the top-left corner in the first conv output, which in turn are computed by the first convolution from the top-left 7 × 7 pixels. So the pixel in the second convolution output is influenced by a 7 × 7 input square. The first convolution also uses an implicitly “padded” column and row to produce the output in the corner; otherwise, we would have an 8 × 8 square of input pixels informing a given pixel (away from the boundary) in the second convolution’s output. In fancy language, we say that a given output neuron of the 3 × 3-conv, 2 × 2-max-pool, 3 × 3-conv construction has a <em>receptive field</em> of 8 × 8.</p>
</blockquote>
<h4 id="putting-it-all-together-for-our-network">8.2.4 <em>Putting it all together for our network</em></h4>
<p>With these building blocks in our hands, we can now proceed to build our convolu- tional neural network for detecting birds and airplanes. Let’s take our previous fully connected model as a starting point and introduce nn.Conv2d and nn.MaxPool2d as described previously:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line">            nn.Linear(<span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">32</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>This code gives us a neural network as shown in figure 8.9.</p>
<p><img src="28.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 8.9 Shape of a typical convolutional network, including the one we’re building. An image is fed to a series of convolutions and max pooling modules and then straightened into a 1D vector and fed into fully connected modules.
</center>
<h3 id="subclassing-nn.module">8.3 <em>Subclassing nn.Module</em></h3>
<p>When we want to build models that do more complex things than just applying one layer after another, we need to leave nn.Sequential for something that gives us added flexibility. PyTorch allows us to use any computation in our model by subclass- ing nn.Module.</p>
<p>In order to subclass nn.Module, at a minimum we need to define a forward function that takes the inputs to the module and returns the output.</p>
<h4 id="our-network-as-an-nn.module">8.3.1 <em>Our network as an nn.Module</em></h4>
<p>Here we use a subclass of nn.Module to contain our entire model. We could also use subclasses to define new building blocks for more complex net- works.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.Tanh()</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.Tanh()</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">32</span>)</span><br><span class="line">        self.act3 = nn.Tanh()</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.pool1(self.act1(self.conv1(x)))</span><br><span class="line">        out = self.pool2(self.act2(self.conv2(out)))</span><br><span class="line">        out = out.view(-<span class="number">1</span>, <span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        out = self.act3(self.fc1(out))</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p><img src="29.png" alt="14" style="zoom:50%;" /></p>
<center>
Figure 8.10 Our baseline convolutional network architecture
</center>
<p>First, our goal is reflected by the size of our intermediate values generally shrinking—this is done by reducing the number of channels in the convolutions, by reducing the number of pixels through pooling, and by having an output dimension lower than the input dimension in the linear layers. This is a common trait of classification networks.</p>
<p>Second, in one layer, there is not a reduction of output size with regard to input size: the initial convolution. If we consider a single output pixel as a vector of 32 ele- ments (the channels), it is a linear transformation of 27 elements (as a convolution of 3 channels × 3 × 3 kernel size)—only a moderate increase.</p>
<h4 id="how-pytorch-keeps-track-of-parameters-and-submodules">8.3.2 <em>How PyTorch keeps track of parameters and submodules</em></h4>
<p>Interestingly, assigning an instance of nn.Module to an attribute in an nn.Module, as we did in the earlier constructor, automatically registers the module as a submodule.</p>
<blockquote>
<p>NOTE The submodules must be top-level <em>attributes</em>, not buried inside list or dict instances! Otherwise the optimizer will not be able to locate the sub- modules (and, hence, their parameters). For situations where your model requires a list or dict of submodules, PyTorch provides <code>nn.ModuleList</code> and <code>nn.ModuleDict.</code></p>
</blockquote>
<p>We can call arbitrary methods of an nn.Module subclass. Be aware that calling such methods will be similar to calling forward instead of the module itself—they will be ignorant of hooks, and the JIT does not see the module structure when using them because we are missing the equivalent of the <code>__call__</code> bits shown in section 6.2.1.</p>
<p>This allows Net to have access to the parameters of its submodules without further action by the user:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = Net()</span><br><span class="line">numel_list = [p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()]</span><br><span class="line"><span class="built_in">sum</span>(numel_list), numel_list</span><br><span class="line"><span class="comment"># Out[27]:</span></span><br><span class="line">(<span class="number">18090</span>, [<span class="number">432</span>, <span class="number">16</span>, <span class="number">1152</span>, <span class="number">8</span>, <span class="number">16384</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h4 id="the-functional-api">8.3.3 <em>The functional API</em></h4>
<p>it appears a bit of a waste that we are also registering submodules that have no parameters, like nn.Tanh and nn.MaxPool2d. And that’s why PyTorch has <em>functional</em> counterparts for every nn module. torch .nn.functional provides many functions that work like the modules we find in nn. But instead of working on the input arguments and stored parameters like the module counterparts, they take inputs and parameters as arguments to the function call. For instance, the functional counterpart of nn.Linear is nn.functional.linear, which is a function that has signature linear(input, weight, bias=None). The weight and bias parameters are arguments to the function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>, <span class="number">32</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">32</span>, <span class="number">2</span>)    </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = F.max_pool2d(torch.tanh(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        out = F.max_pool2d(torch.tanh(self.conv2(out)), <span class="number">2</span>)</span><br><span class="line">        out = out.view(-<span class="number">1</span>, <span class="number">8</span> * <span class="number">8</span> * <span class="number">8</span>)</span><br><span class="line">        out = torch.tanh(self.fc1(out))</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h3 id="training-our-convnet">8.4 <em>Training our convnet</em></h3>
<p>Recall that the core of our convnet is two nested loops: an outer one over the <em>epochs</em> and an inner one of the DataLoader that produces batches from our Dataset. In each loop, we then have to</p>
<ol type="1">
<li>Feed the inputs through the model (the forward pass).</li>
<li>Compute the loss (also part of the forward pass).</li>
<li>Zero any old gradients.</li>
<li>Call loss.backward() to compute the gradients of the loss with respect to all parameters (the backward pass).</li>
<li>Have the optimizer take a step in toward lower loss.</li>
</ol>
<p>Also, we collect and print some information. So here is our training loop, looking almost as it does in the previous chapter—but it is good to remember what each thing is doing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> datetime</span><br><span class="line">    </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">n_epochs, optimizer, model, loss_fn, train_loader</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        loss_train = <span class="number">0.0</span></span><br><span class="line">		<span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            loss = loss_fn(outputs, labels)</span><br><span class="line">            </span><br><span class="line">   	 		optimizer.zero_grad()</span><br><span class="line">    		loss.backward()</span><br><span class="line">    		optimizer.step()</span><br><span class="line">            loss_train += loss.item()</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Epoch &#123;&#125;, Training loss &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                datetime.datetime.now(), epoch,</span><br><span class="line">                loss_train / <span class="built_in">len</span>(train_loader)))</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">train_loader = torch.utils.data.DataLoader(cifar2, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = Net()  <span class="comment">#</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)  <span class="comment">#</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">100</span>,</span><br><span class="line">	optimizer = optimizer,</span><br><span class="line">	model = model,</span><br><span class="line">	loss_fn = loss_fn,</span><br><span class="line">	train_loader = train_loader,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="measuring-accuracy">8.4.1 <em>Measuring accuracy</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">model, train_loader, val_loader</span>):</span></span><br><span class="line">    <span class="keyword">for</span> name, loader <span class="keyword">in</span> [(<span class="string">&quot;train&quot;</span>, train_loader), (<span class="string">&quot;val&quot;</span>, val_loader)]:</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">		total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    		<span class="keyword">for</span> imgs, labels <span class="keyword">in</span> loader:</span><br><span class="line">                outputs = model(imgs)</span><br><span class="line">				_, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">                total += labels.shape[<span class="number">0</span>]</span><br><span class="line">                correct += <span class="built_in">int</span>((predicted == labels).<span class="built_in">sum</span>())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Accuracy &#123;&#125;: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(name , correct / total))</span><br></pre></td></tr></table></figure>
<h4 id="saving-and-loading-our-model">8.4.2 <em>Saving and loading our model</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), data_path + <span class="string">&#x27;birds_vs_airplanes.pt&#x27;</span>)</span><br><span class="line">loaded_model = Net()</span><br><span class="line">loaded_model.load_state_dict(torch.load(data_path + <span class="string">&#x27;birds_vs_airplanes.pt&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="training-on-the-gpu">8.4.3 <em>Training on the GPU</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">device = (torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Training on device <span class="subst">&#123;device&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">n_epochs, optimizer, model, loss_fn, train_loader</span>):</span></span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">                loss_train = <span class="number">0.0</span></span><br><span class="line">                <span class="keyword">for</span> imgs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                    imgs = imgs.to(device=device)</span><br><span class="line">                    labels = labels.to(device=device)</span><br><span class="line">                    outputs = model(imgs)</span><br><span class="line">                    loss = loss_fn(outputs, labels)</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line"></span><br><span class="line">    				loss_train += loss.item()</span><br><span class="line">               <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                	<span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Epoch &#123;&#125;, Training loss &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                		datetime.datetime.now(), epoch,</span><br><span class="line">                		loss_train / <span class="built_in">len</span>(train_loader)))</span><br><span class="line">                    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    train_loader = torch.utils.data.DataLoader(cifar2, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    model = Net().to(device=device)</span><br><span class="line">	optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">	loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    training_loop(</span><br><span class="line">        n_epochs = <span class="number">100</span>,</span><br><span class="line">        optimizer = optimizer,</span><br><span class="line">        model = model,</span><br><span class="line">        loss_fn = loss_fn,</span><br><span class="line">        train_loader = train_loader,</span><br><span class="line">	)</span><br></pre></td></tr></table></figure>
<h3 id="model-design">8.5 <em>Model design</em></h3>
<p>Plus images may not be our sole focus in the real world, where we have tabular data, sequences, and text. The promise of neural networks is sufficient flexibility to solve problems on all these kinds of data given the proper architecture (that is, the interconnection of layers or modules) and the proper loss function.</p>
<p>PyTorch ships with a very comprehensive collection of modules and loss functions to implement state-of-the-art architectures ranging from feed-forward components to long short-term memory (LSTM) modules and transformer networks (two very popular architectures for sequential data). Several models are available through PyTorch Hub or as part of torchvision and other vertical community efforts.</p>
<p>The purpose of this section is precisely to provide conceptual tools that will allow us to read the latest research paper and start implementing it in PyTorch—or, since authors often release PyTorch implementations of their papers, to read the implementations without choking on our coffee.</p>
<h4 id="adding-memory-capacity-width">8.5.1 <em>Adding memory capacity: Width</em></h4>
<p>Given our feed-forward architecture, there are a couple of dimensions we’d likely want to explore before getting into further complications. The first dimension is the <em>width</em> of the network: the number of neurons per layer, or channels per convolution. We can make a model wider very easily in PyTorch.</p>
<p>The numbers specifying channels and features for each layer are directly related to the number of parameters in a model; all other things being equal, they increase the <em>capacity</em> of the model.</p>
<p><strong>The greater the capacity, the more variability in the inputs the model will be able to manage; but at the same time, the more likely overfitting will be, since the model can use a greater number of parameters to memorize unessential aspects of the input.</strong> We already went into ways to combat overfitting, the best being increasing the sample size or, in the absence of new data, augmenting existing data through artificial modifications of the same data.</p>
<p>There are a few more tricks we can play at the model level (without acting on the data) to control overfitting. Let’s review the most common ones.</p>
<h4 id="helping-our-model-to-converge-and-generalize-regularization">8.5.2 <em>Helping our model to converge and generalize: Regularization</em></h4>
<p>Training a model involves two critical steps:</p>
<ol type="1">
<li>optimization, when we need the loss to decrease on the training set</li>
<li>generalization, when the model has to work not only on the training set but also on data it has not seen before, like the validation set.</li>
</ol>
<p>The mathematical tools aimed at easing these two steps are sometimes subsumed under the label <em>regularization</em>.</p>
<h5 id="keeping-the-parameters-in-check-weight-penalties">KEEPING THE PARAMETERS IN CHECK: WEIGHT PENALTIES</h5>
<p>The first way to stabilize generalization is to <strong>add a regularization term to the loss</strong>. This term is crafted so that the weights of the model tend to be small on their own, limiting how much training makes them grow. In other words, it is a penalty on larger weight values. This makes the loss have a smoother topography, and there’s relatively less to gain from fitting individual samples.</p>
<p>The most popular regularization terms of this kind are L2 regularization, which is the sum of squares of all weights in the model, and L1 regularization, which is the sum of the absolute values of all weights in the model. Both of them are scaled by a (small) factor, which is a hyperparameter we set prior to training.</p>
<blockquote>
<p>L2 regularization is also referred to as <em>weight decay</em>. The reason for this name is that, thinking about SGD and backpropagation, the negative gradient of the L2 regularization term with respect to a parameter w_i is <code>- 2 * lambda * w_i</code>, where lambda is the aforementioned hyperparameter, simply named <em>weight decay</em> in PyTorch. So, adding L2 regularization to the loss function is equivalent to decreasing each weight by an amount proportional to its current value during the optimization step (hence, the name <em>weight decay</em>). Note that weight decay applies to all parameters of the network, such as biases.</p>
</blockquote>
<p>In PyTorch, we could implement regularization pretty easily by adding a term to the loss. After computing the loss, whatever the loss function is, we can iterate the parameters of the model, sum their respective square (for L2) or abs (for L1), and backpropagate:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = loss_fn(outputs, labels)</span><br><span class="line">l2_lambda = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># Replaces pow(2.0) with abs() for L1 regularization</span></span><br><span class="line">l2_norm = <span class="built_in">sum</span>(p.<span class="built_in">pow</span>(<span class="number">2.0</span>).<span class="built_in">sum</span>() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters())</span><br><span class="line">loss = loss + l2_lambda * l2_norm</span><br></pre></td></tr></table></figure>
<p>However, <strong>the SGD optimizer in PyTorch already has a weight_decay parameter that corresponds to 2 * lambda</strong>, and it directly performs weight decay during the update as described previously. It is fully equivalent to adding the L2 norm of weights to the loss, without the need for accumulating terms in the loss and involving autograd.</p>
<h5 id="not-relying-too-much-on-a-single-input-dropout">NOT RELYING TOO MUCH ON A SINGLE INPUT: DROPOUT</h5>
<p>An effective strategy for combating overfitting was originally proposed in 2014 by Nit- ish Srivastava and coauthors from Geoff Hinton’s group in Toronto, in a paper aptly entitled “Dropout: a Simple Way to Prevent Neural Networks from Overfitting” (http://mng.bz/nPMa).</p>
<ul>
<li><p>The idea behind dropout is indeed simple: zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration.</p></li>
<li><p>This procedure effectively generates slightly different models with different neuron topologies at each iteration, giving neurons in the model less chance to coordinate in the memorization process that happens during overfitting.</p></li>
<li><p>An alternative point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network.</p></li>
</ul>
<p>In PyTorch, we can implement dropout in a model by adding an <code>nn.Dropout</code> module between the nonlinear activation function and the linear or convolutional module of the subsequent layer. In case of convolutions, we’ll use the specialized <code>nn.Dropout2d</code> or <code>nn.Dropout3d</code>, which zero out entire channels of the input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv_dropout = nn.Dropout2d(p=<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<p>Note that dropout is normally active during training, while during the evaluation of a trained model in production, dropout is bypassed or, equivalently, assigned a probability equal to zero. This is controlled through the train property of the Dropout module. Recall that PyTorch lets us switch between the two modalities by calling</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h5 id="keeping-activations-in-check-batch-normalization">KEEPING ACTIVATIONS IN CHECK: <em>BATCH NORMALIZATION</em></h5>
<p>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” (https://arxiv.org/abs/1502.03167). The paper described a technique that had multiple beneficial effects on training: <strong>allowing us to increase the learning rate and make training less dependent on initialization and act as a regularizer, thus representing an alternative to dropout.</strong></p>
<p><strong>The main idea behind batch normalization is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution.</strong> In practical terms, batch normalization shifts and scales an intermediate input using the mean and standard deviation collected at that intermediate location over the samples of the minibatch.</p>
<p>Batch normalization in PyTorch is provided through the nn.BatchNorm1D, nn.BatchNorm2d, and nn.BatchNorm3d modules, depending on the dimensionality of the input.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_channels)</span><br></pre></td></tr></table></figure>
<p>Just as for dropout, batch normalization needs to behave differently during training and inference. In fact, at inference time, we want to avoid having the output for a specific input depend on the statistics of the other inputs we’re presenting to the model. As such, we need a way to still normalize, but this time fixing the normalization parameters once and for all.</p>
<p><strong>As minibatches are processed, in addition to estimating the mean and standard deviation for the current minibatch, PyTorch also updates the running estimates for mean and standard deviation that are representative of the whole dataset, as an approximation. </strong></p>
<h4 id="going-deeper-to-learn-more-complex-structures-depth">8.5.3 <em>Going deeper to learn more complex structures: Depth</em></h4>
<p>With depth, the complexity of the function the network is able to approximate generally increases. Depth allows a model to deal with hierarchical information when we need to understand the context in order to say something about some input.</p>
<p>There’s another way to think about depth: increasing depth is related to increasing the length of the sequence of operations that the network will be able to perform when processing input.</p>
<h5 id="skip-connections">SKIP CONNECTIONS</h5>
<p>Depth comes with some additional challenges, which prevented deep learning models from reaching 20 or more layers until late 2015.</p>
<ul>
<li><p><strong>gradient <em>vanish</em></strong></p>
<p>Adding depth to a model generally makes training harder to converge. The bottom line is that a long chain of multiplications will tend to make the contribution of the parameter to the <strong>gradient <em>vanish</em></strong>, leading to ineffective training of that layer since that parameter and others like it won’t be properly updated.</p>
<ul>
<li><p>In December 2015, Kaiming He and coauthors presented <em>residual networks</em> (ResNets), an architecture that uses a simple trick to allow very deep networks to be successfully trained. That work opened the door to networks ranging from tens of layers to 100 layers in depth, surpassing the then state of the art in computer vision benchmark problems.</p></li>
<li><p>The trick we mentioned is the following: <strong>using a <em>skip connection</em> to short-circuit blocks of layers</strong>, as shown in figure 8.11.</p></li>
<li><p><img src="30.png" alt="14" style="zoom:50%;" /></p></li>
<li><center>
<p>Figure 8.11 The architecture of our network with three convolutional layers. The skip connection is what differentiates NetRes from NetDepth.</p>
</center></li>
<li><p><strong>A skip connection is nothing but the addition of the input to the output of a block of layers.</strong> This is exactly how it is done in PyTorch. Let’s add one layer to our simple convolutional model, and let’s use ReLU as the activation for a change. The vanilla module with an extra layer looks like this:</p></li>
<li><p>```python out = F.max_pool2d(torch.relu(self.conv2(out)), 2) out1 = out out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    + we can appreciate that a skip connection, or a sequence of skip connections in a deep network, creates a direct path from the deeper parameters to the loss. This makes their contribution to the gradient of the loss more direct, as partial derivatives of the loss with respect to those parameters have a chance not to be multiplied by a long chain of other operations.</span><br><span class="line"></span><br><span class="line">    + It has been observed that skip connections have a beneficial effect on convergence especially in the initial phases of training. Also, the loss landscape of deep residual networks is a lot smoother than feed-forward networks of the same depth and width.</span><br><span class="line"></span><br><span class="line">##### BUILDING VERY DEEP MODELS IN PYTORCH</span><br><span class="line"></span><br><span class="line">The standard strategy is to define a building block, such as a (Conv2d, ReLU, Conv2d) + skip connection block, and then build the network dynamically in a for loop. Let’s see it done in practice. We will create the network depicted in figure 8.12.</span><br><span class="line"></span><br><span class="line">&lt;img src=&quot;31.png&quot; alt=&quot;1&quot; style=&quot;zoom:50%;&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;center&gt;</span><br><span class="line">    Figure 8.12 Our deep architecture with residual connections. On the left, we define a simplistic residual block. We use it as a building block in our network, as shown on the right.</span><br><span class="line">&lt;/center&gt;</span><br><span class="line"></span><br><span class="line">We first create a module subclass whose sole job is to provide the computation for one *block*—that is, one group of convolutions, activation, and skip connection:</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">class ResBlock(nn.Module):</span><br><span class="line">    def __init__(self, n_chans):</span><br><span class="line">        super(ResBlock, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)</span><br><span class="line">        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)</span><br><span class="line"></span><br><span class="line">        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity=&#x27;relu&#x27;)</span><br><span class="line">        torch.nn.init.constant_(self.batch_norm.weight, 0.5)</span><br><span class="line">        torch.nn.init.zeros_(self.batch_norm.bias)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.conv(x)</span><br><span class="line">        out = self.batch_norm(out)</span><br><span class="line">        out = torch.relu(out)</span><br><span class="line">        return out + x</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
</ul>
<p><strong>Uses custom initializations <code>. kaiming_normal_</code> initializes with normal random elements with standard deviation as computed in the ResNet paper. The batch norm is initialized to produce output distributions that initially have 0 mean and 0.5 variance.</strong></p>
<p>Since we’re planning to generate a deep model, we are including batch normalization in the block, since this will help prevent gradients from vanishing during training.</p>
<p>First, in <em>init</em>, we create nn.Sequential containing a list of ResBlock instances. nn.Sequential will ensure that the output of one block is used as input to the next. It will also ensure that all the parameters in the block are visible to Net. Then, in forward, we just call the sequential to traverse the 100 blocks and generate the output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNetDeep</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_channels1=<span class="number">32</span>, n_block=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNetDeep, self).__init__()</span><br><span class="line">        self.conv_block = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=n_channels1,</span><br><span class="line">                      kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            *(n_block * [ResNetBlock(n_channels1)]),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=n_channels1 * <span class="number">8</span> * <span class="number">8</span>, out_features=<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv_block(x)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">==========================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                   Output Shape              Param <span class="comment">#</span></span><br><span class="line">==========================================================================================</span><br><span class="line">ResNetDeep                               --                        --</span><br><span class="line">├─Sequential: <span class="number">1</span>-<span class="number">1</span>                        [<span class="number">1</span>, <span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>]             --</span><br><span class="line">│    └─Conv2d: <span class="number">2</span>-<span class="number">1</span>                       [<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span>]           <span class="number">896</span></span><br><span class="line">│    └─ReLU: <span class="number">2</span>-<span class="number">2</span>                         [<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span>]           --</span><br><span class="line">│    └─MaxPool2d: <span class="number">2</span>-<span class="number">3</span>                    [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           --</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">4</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           --</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">1</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           <span class="number">9</span>,<span class="number">216</span></span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">2</span>             [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           <span class="number">64</span></span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">5</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">3</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">4</span>             [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">6</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">5</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">6</span>             [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">7</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">7</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">8</span>             [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">8</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">9</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">10</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">9</span>                  [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">11</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">12</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">10</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">13</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">14</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">11</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">15</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">16</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">12</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">17</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">18</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─ResNetBlock: <span class="number">2</span>-<span class="number">13</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─Conv2d: <span class="number">3</span>-<span class="number">19</span>                 [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    │    └─BatchNorm2d: <span class="number">3</span>-<span class="number">20</span>            [<span class="number">1</span>, <span class="number">32</span>, <span class="number">16</span>, <span class="number">16</span>]           (recursive)</span><br><span class="line">│    └─MaxPool2d: <span class="number">2</span>-<span class="number">14</span>                   [<span class="number">1</span>, <span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>]             --</span><br><span class="line">├─Sequential: <span class="number">1</span>-<span class="number">2</span>                        [<span class="number">1</span>, <span class="number">10</span>]                   --</span><br><span class="line">│    └─Flatten: <span class="number">2</span>-<span class="number">15</span>                     [<span class="number">1</span>, <span class="number">2048</span>]                 --</span><br><span class="line">│    └─Linear: <span class="number">2</span>-<span class="number">16</span>                      [<span class="number">1</span>, <span class="number">32</span>]                   <span class="number">65</span>,<span class="number">568</span></span><br><span class="line">│    └─ReLU: <span class="number">2</span>-<span class="number">17</span>                        [<span class="number">1</span>, <span class="number">32</span>]                   --</span><br><span class="line">│    └─Linear: <span class="number">2</span>-<span class="number">18</span>                      [<span class="number">1</span>, <span class="number">10</span>]                   <span class="number">330</span></span><br><span class="line">==========================================================================================</span><br><span class="line">Total params: <span class="number">76</span>,074</span><br><span class="line">Trainable params: <span class="number">76</span>,074</span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">Total mult-adds (M): <span class="number">24.58</span></span><br><span class="line">==========================================================================================</span><br><span class="line">Input size (MB): <span class="number">0.01</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">0.39</span></span><br><span class="line">Params size (MB): <span class="number">0.30</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">0.71</span></span><br><span class="line">==========================================================================================</span><br></pre></td></tr></table></figure>
<p>Unsurprisingly, the network is quite a bit slower to converge. It is also more fragile in convergence. This is why we used more-detailed initializations and trained our NetRes with a learning rate of 3e – 3 instead of the 1e – 2 we used for the other networks. We trained none of the networks to convergence, but we would not have gotten anywhere without these tweaks.</p>
<h5 id="initialization">INITIALIZATION</h5>
<p>Initialization is one of the important tricks in training neural networks. Unfortunately, for historical reasons, <strong>PyTorch has default weight initializations that are not ideal.</strong> In the meantime, we need to fix the weight initialization ourselves. We found that our model did not converge and looked at what people commonly choose as initialization (a smaller variance in weights; and zero mean and unit variance outputs for batch norm), and then we halved the output variance in the batch norm when the network would not converge.</p>
<h4 id="comparing-the-designs-from-this-section">8.5.4 <em>Comparing the designs from this section</em></h4>
<p>We summarize the effect of each of our design modifications in isolation in figure 8.13. We should not overinterpret any of the specific numbers—our problem setup and experiments are simplistic, and repeating the experiment with different random seeds will probably generate variation at least as large as the differences in validation accuracy. For this demonstration, we left all other things equal, from learning rate to number of epochs to train; in practice, we would try to get the best results by varying those. Also, we would likely want to combine some of the additional design elements.</p>
<p><img src="32.png" alt="1" style="zoom:50%;" /></p>
<center>
Figure 8.13 The modified networks all perform similarly.
</center>
<h3 id="its-already-outdated">8.5.5 <em>It’s already outdated</em></h3>
<p>The curse and blessing of a deep learning practitioner is that neural network architec- tures evolve at a very rapid pace. This is not to say that what we’ve seen in this chapter is necessarily old school, but a thorough illustration of the latest and greatest architec- tures is a matter for another book.</p>
<h3 id="conclusion-1">8.6 Conclusion</h3>
<p>After quite a lot of work, we now have a model that our fictional friend Jane can use to filter images for her blog. All we have to do is take an incoming image, crop and resize it to 32 × 32, and see what the model has to say about it.</p>
<p>In this chapter, we have built reasonable, working models in PyTorch that can learn from images. We did it in a way that helped us build our intuition around convolutional networks. We also explored ways in which we can make our models wider and deeper, while controlling effects like overfitting. Although we still only scratched the surface, we have taken another significant step ahead from the previous chapter. We now have a solid basis for facing the challenges we’ll encounter when working on deep learning projects.</p>
<p>Part 2 uses automatic detection of lung cancer as an ongoing example; we will go from being familiar with the PyTorch API to being able to implement entire projects using PyTorch. We’ll start in the next chapter by explaining the problem from a high level, and then we’ll get into the details of the data we’ll be using.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/08/01/ML9-2/" rel="prev" title="机器学习 by 李宏毅(9-2)">
                  <i class="fa fa-chevron-left"></i> 机器学习 by 李宏毅(9-2)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/08/07/ML10/" rel="next" title="机器学习 by 李宏毅(10-1)">
                  机器学习 by 李宏毅(10-1) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuYang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">957k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">14:30</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.1.0/dist/quicklink.umd.js"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{&quot;enable&quot;:true,&quot;home&quot;:true,&quot;archive&quot;:true,&quot;delay&quot;:true,&quot;timeout&quot;:3000,&quot;priority&quot;:true,&quot;ignores&quot;:null,&quot;url&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;08&#x2F;05&#x2F;core-pytorch3&#x2F;&quot;}</script>
  <script src="/js/third-party/quicklink.js"></script>



  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script async src="/js/cursor/explosion.js"></script>

</body>
</html>
