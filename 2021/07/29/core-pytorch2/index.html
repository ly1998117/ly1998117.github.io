<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ly1998117.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:true,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="Chapter 4 Real-world data representation using tensors">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning with PyTorch -- Part 1 Core PyTorch (C4-C5)">
<meta property="og:url" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/index.html">
<meta property="og:site_name" content="LiuYang&#39;s Blog">
<meta property="og:description" content="Chapter 4 Real-world data representation using tensors">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/1.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/2.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/3.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/4.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/5.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/6.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/7.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/8.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/9.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/10.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/11.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/1.gif">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/12.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/13.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/14.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/15.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/16.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/17.png">
<meta property="article:published_time" content="2021-07-29T13:48:40.000Z">
<meta property="article:modified_time" content="2021-08-03T08:36:53.819Z">
<meta property="article:author" content="LiuYang">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ly1998117.github.io/2021/07/29/core-pytorch2/1.png">


<link rel="canonical" href="https://ly1998117.github.io/2021/07/29/core-pytorch2/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;29&#x2F;core-pytorch2&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;07&#x2F;29&#x2F;core-pytorch2&#x2F;&quot;,&quot;title&quot;:&quot;Deep Learning with PyTorch -- Part 1 Core PyTorch (C4-C5)&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Deep Learning with PyTorch -- Part 1 Core PyTorch (C4-C5) | LiuYang's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiuYang's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">26</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-4-real-world-data-representation-using-tensors"><span class="nav-number">1.</span> <span class="nav-text">Chapter 4 Real-world data representation using tensors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#working-with-images"><span class="nav-number">1.1.</span> <span class="nav-text">4.1 Working with images</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#adding-color-channels"><span class="nav-number">1.1.1.</span> <span class="nav-text">4.11 Adding color channels</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#loading-an-image-file"><span class="nav-number">1.1.2.</span> <span class="nav-text">4.1.2 Loading an image file</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#changing-the-layout"><span class="nav-number">1.1.3.</span> <span class="nav-text">4.1.3 Changing the layout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#normalizing-the-data"><span class="nav-number">1.1.4.</span> <span class="nav-text">4.1.4 Normalizing the data</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-images-volumetric-data"><span class="nav-number">1.2.</span> <span class="nav-text">4.2 3D images: Volumetric data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#loading-a-specialized-format"><span class="nav-number">1.2.1.</span> <span class="nav-text">4.2.1 Loading a specialized format</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#representing-tabular-data"><span class="nav-number">1.3.</span> <span class="nav-text">4.3 Representing tabular data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#using-a-real-world-dataset"><span class="nav-number">1.3.1.</span> <span class="nav-text">4.3.1 Using a real-world dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#loading-a-wine-data-tensor"><span class="nav-number">1.3.2.</span> <span class="nav-text">4.3.2 Loading a wine data tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#representing-scores"><span class="nav-number">1.3.3.</span> <span class="nav-text">4.3.3 Representing scores</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#one-hot-encoding"><span class="nav-number">1.3.4.</span> <span class="nav-text">4.3.4 One-hot encoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#when-to-categorize"><span class="nav-number">1.3.5.</span> <span class="nav-text">4.3.5 When to categorize</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#finding-thresholds"><span class="nav-number">1.3.6.</span> <span class="nav-text">4.3.6 Finding thresholds</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#working-with-time-series"><span class="nav-number">1.4.</span> <span class="nav-text">4.4 Working with time series</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#adding-a-time-dimension"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.4.1 Adding a time dimension</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#shaping-the-data-by-time-period"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.4.2 Shaping the data by time period</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ready-for-training"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.4.3 Ready for training</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#representing-text"><span class="nav-number">1.5.</span> <span class="nav-text">4.5 Representing text</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#converting-text-to-numbers"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.5.1 Converting text to numbers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#one-hot-encoding-characters"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.5.2 One-hot-encoding characters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#one-hot-encoding-whole-words"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.5.3 One-hot encoding whole words</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#text-embeddings"><span class="nav-number">1.5.4.</span> <span class="nav-text">4.5.4 Text embeddings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#text-embeddings-as-a-blueprint"><span class="nav-number">1.5.5.</span> <span class="nav-text">4.5.5 Text embeddings as a blueprint</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-5-the-mechanics-of-learning"><span class="nav-number">2.</span> <span class="nav-text">Chapter 5 The mechanics of learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a-timeless-lesson-in-modeling"><span class="nav-number">2.1.</span> <span class="nav-text">5.1 A timeless lesson in modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-is-just-parameter-estimation"><span class="nav-number">2.2.</span> <span class="nav-text">5.2 Learning is just parameter estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a-hot-problem"><span class="nav-number">2.2.1.</span> <span class="nav-text">5.2.1 A hot problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gathering-some-data"><span class="nav-number">2.2.2.</span> <span class="nav-text">5.2.2 Gathering some data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#visualizing-the-data"><span class="nav-number">2.2.3.</span> <span class="nav-text">5.2.3 Visualizing the data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#choosing-a-linear-model-as-a-first-try"><span class="nav-number">2.2.4.</span> <span class="nav-text">5.2.4 Choosing a linear model as a first try</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#less-loss-is-what-we-want"><span class="nav-number">2.3.</span> <span class="nav-text">5.3 Less loss is what we want</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#from-problem-back-to-pytorch"><span class="nav-number">2.3.1.</span> <span class="nav-text">5.3.1 From problem back to PyTorch</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#down-along-the-gradient"><span class="nav-number">2.4.</span> <span class="nav-text">5.4 Down along the gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#decreasing-loss"><span class="nav-number">2.4.1.</span> <span class="nav-text">5.4.1 Decreasing loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#getting-analytical"><span class="nav-number">2.4.2.</span> <span class="nav-text">5.4.2 Getting analytical</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#compute-the-derivatives"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">Compute The Derivatives</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#applying-the-derivatives-to-the-model"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">APPLYING THE DERIVATIVES TO THE MODEL</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#defining-the-gradient-function"><span class="nav-number">2.4.2.3.</span> <span class="nav-text">DEFINING THE GRADIENT FUNCTION</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#iterating-to-fit-the-model"><span class="nav-number">2.4.3.</span> <span class="nav-text">5.4.3 Iterating to fit the model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#the-training-loop"><span class="nav-number">2.4.3.1.</span> <span class="nav-text">THE TRAINING LOOP</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#normalizing-inputs"><span class="nav-number">2.4.4.</span> <span class="nav-text">5.4.4 Normalizing inputs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#visualizing-again"><span class="nav-number">2.4.5.</span> <span class="nav-text">5.4.5 Visualizing (again)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pytorchs-autograd-backpropagating-all-things"><span class="nav-number">2.5.</span> <span class="nav-text">5.5 PyTorch’s autograd: Backpropagating all things</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#computing-the-gradient-automatically"><span class="nav-number">2.5.1.</span> <span class="nav-text">5.5.1 Computing the gradient automatically</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#applying-autograd"><span class="nav-number">2.5.1.1.</span> <span class="nav-text">APPLYING AUTOGRAD</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#using-the-grad-attribute"><span class="nav-number">2.5.1.2.</span> <span class="nav-text">USING THE GRAD ATTRIBUTE</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#accumulating-grad-functions"><span class="nav-number">2.5.1.3.</span> <span class="nav-text">ACCUMULATING GRAD FUNCTIONS</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#optimizers-a-la-carte"><span class="nav-number">2.5.2.</span> <span class="nav-text">5.5.2 Optimizers a la carte</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#using-a-gradient-descent-optimizer"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">USING A GRADIENT DESCENT OPTIMIZER</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#testing-other-optimizers"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">TESTING OTHER OPTIMIZERS</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#training-validation-and-overfitting"><span class="nav-number">2.5.3.</span> <span class="nav-text">5.5.3 Training, validation, and overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#evaluating-the-training-loss"><span class="nav-number">2.5.3.1.</span> <span class="nav-text">EVALUATING THE TRAINING LOSS</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#generalizing-to-the-validation-set"><span class="nav-number">2.5.3.2.</span> <span class="nav-text">GENERALIZING TO THE VALIDATION SET</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#splitting-a-dataset"><span class="nav-number">2.5.3.3.</span> <span class="nav-text">SPLITTING A DATASET</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#autograd-nits-and-switching-it-off"><span class="nav-number">2.5.4.</span> <span class="nav-text">5.5.4 Autograd nits and switching it off</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conclusion"><span class="nav-number">2.6.</span> <span class="nav-text">5.6 Conclusion</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiuYang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LiuYang</p>
  <div class="site-description" itemprop="description">人与人的悲欢并不相通</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.macwk.com/" title="https:www.macwk.com&#x2F;" rel="noopener" target="_blank">Macwk</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ly1998117.github.io/2021/07/29/core-pytorch2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="LiuYang">
      <meta itemprop="description" content="人与人的悲欢并不相通">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuYang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning with PyTorch -- Part 1 Core PyTorch (C4-C5)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-29 21:48:40" itemprop="dateCreated datePublished" datetime="2021-07-29T21:48:40+08:00">2021-07-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-03 16:36:53" itemprop="dateModified" datetime="2021-08-03T16:36:53+08:00">2021-08-03</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>57k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>52 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="chapter-4-real-world-data-representation-using-tensors">Chapter 4 <em>Real-world data representation using tensors</em></h2>
<span id="more"></span>
<ul>
<li>Representing real-world data as PyTorch tensors</li>
<li>Working with a range of data types</li>
<li>Loading data from a file</li>
<li>Converting data to tensors</li>
<li>Shaping tensors so they can be used as inputs for neural network models</li>
</ul>
<p>Neural networks take tensors as input and produce tensors as outputs. Here’s a question that we can already address: how do we take a piece of data, a video, or a line of text, and represent it with a tensor in a way that is appropriate for training a deep learning model?</p>
<p>We’ll be using a lot of image and volumetric data through the rest of the book, since those are common data types and they reproduce well in book format. We’ll also cover tabular data, time series, and text, as those will also be of interest to a number of our readers. Next, we’ll work with tabular data about wines, just like what we’d find in a spreadsheet. After that, we’ll move to <em>ordered</em> tabular data, with a time-series dataset from a bike-sharing program. Finally, we’ll dip our toes into text data from Jane Austen.</p>
<h3 id="working-with-images">4.1 <em>Working with images</em></h3>
<p>Loading an image from common image formats and then transform the data into a tensor representation that has the various parts of the image arranged in the way PyTorch expects.</p>
<p>Scalars representing values at individual pixels are often encoded using 8-bit integers, as in consumer cameras. In medical, scientific, and industrial applications, it is not unusual to find higher numerical precision, such as 12-bit or 16-bit. This allows a wider range or increased sensitivity in cases where the pixel encodes information about a physical property, like bone density, temperature, or depth.</p>
<h4 id="adding-color-channels">4.11 <em>Adding color channels</em></h4>
<p>There are several ways to encode colors into numbers. The most common is RGB, where a color is defined by three numbers representing the intensity of red, green, and blue. Figure 4.1 shows a rainbow, where each of the RGB channels captures a certain portion of the spectrum</p>
<p><img src="1.png" alt="1" style="zoom:45%;" /></p>
<center>
Figure 4.1 A rainbow, broken into red, green, and blue channels
</center>
<h4 id="loading-an-image-file">4.1.2 <em>Loading an image file</em></h4>
<p>Images come in several different file formats, but luckily there are plenty of ways to load images in Python. Let’s start by loading a PNG image using the <code>imageio</code> module</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">img_arr = imageio.imread(<span class="string">&#x27;../data/p1ch4/image-dog/bobby.jpg&#x27;</span>)</span><br><span class="line">img_arr.shape</span><br><span class="line"><span class="comment"># Out[2]:</span></span><br><span class="line">(<span class="number">720</span>, <span class="number">1280</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>At this point, img is a NumPy array-like object with three dimensions: two spatial dimensions, width and height; and a third dimension corresponding to the red, green, and blue channels. PyTorch modules dealing with image data require tensors to be laid out as <em>C</em> × <em>H</em> × <em>W</em> : channels, height, and width, respectively.</p>
<h4 id="changing-the-layout">4.1.3 <em>Changing the layout</em></h4>
<p>We can use the tensor’s permute method with the old dimensions for each new dimension to get to an appropriate layout. Given an input tensor <em>H</em> × <em>W</em> × <em>C</em> as obtained previously, we get a proper layout by having channel 2 first and then channels 0 and 1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line">img = torch.from_numpy(img_arr)</span><br><span class="line">out = img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Following the same strategy we’ve used for earlier data types, to create a dataset of multiple images to use as an input for our neural networks, we store the images in a batch along the first dimension to obtain an <em>N</em> × <em>C</em> × <em>H</em> × <em>W</em> tensor.</p>
<p>Notice the type of the tensor: we’re expecting each color to be rep- resented as an 8-bit integer, as in most photographic formats from standard consumer cameras. We can now load all PNG images from an input directory and store them in the tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">batch = torch.zeros(batch_size, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>, dtype=torch.uint8)</span><br><span class="line">data_dir = <span class="string">&#x27;../data/p1ch4/image-cats/&#x27;</span></span><br><span class="line">filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(data_dir) <span class="keyword">if</span> os.path.splitext(name)[-<span class="number">1</span>] == <span class="string">&#x27;.png&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, filename <span class="keyword">in</span> <span class="built_in">enumerate</span>(filenames):</span><br><span class="line">    img_arr = imageio.imread(os.path.join(data_dir, filename))</span><br><span class="line">    img_t = torch.from_numpy(img_arr)</span><br><span class="line">    img_t = img_t.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Here we keep only the first three channels. Sometimes images also have an alpha channel </span></span><br><span class="line">    <span class="comment"># indicating transparency, but our network only wants RGB input.</span></span><br><span class="line">    img_t = img_t[:<span class="number">3</span>]</span><br><span class="line">    batch[i] = img_t</span><br></pre></td></tr></table></figure>
<h4 id="normalizing-the-data">4.1.4 <em>Normalizing the data</em></h4>
<p>Neural networks exhibit the best training performance when the input data ranges roughly from 0 to 1, or from -1 to 1 (this is an effect of how their building blocks are defined). So a typical thing we’ll want to do is cast a tensor to floating-point and normalize the values of the pixels.</p>
<p>Casting to floating-point is easy, but normalization is trickier</p>
<ul>
<li><p>One possibility is to just divide the values of the pixels by 255 (the maximum representable number in 8-bit unsigned):</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = batch.<span class="built_in">float</span>()</span><br><span class="line">batch /= <span class="number">255.0</span></span><br></pre></td></tr></table></figure></p></li>
<li><p>Another possibility is to compute the mean and standard deviation of the input data and scale it so that the output has zero mean and unit standard deviation across each channel:</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_channels = batch.shape[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_channels):</span><br><span class="line">    mean = torch.mean(batch[:, c])</span><br><span class="line">    std = torch.std(batch[:, c])</span><br><span class="line">    batch[:, c] = (batch[:, c] - mean) / std</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Note: Here, we normalize just a single batch of images. We can perform several other operations on inputs, such as geometric transforma- tions like rotations, scaling, and cropping. These may help with training or may be required to make an arbitrary input conform to the input requirements of a network, like the size of the image.</p>
</blockquote></li>
</ul>
<h3 id="d-images-volumetric-data">4.2 <em>3D images: Volumetric data</em></h3>
<p>In some contexts, such as medical imaging applications involving, say, CT (computed tomography) scans, <strong>we typically deal with sequences of images stacked along the head-to-foot axis</strong>, each corresponding to a slice across the human body. In CT scans, the inten- sity represents the density of the different parts of the body—lungs, fat, water, muscle, and bone, in order of increasing density—mapped from dark to bright when the CT scan is displayed on a clinical workstation. The density at each point is computed from the amount of X-rays reaching a detector after crossing through the body, with some complex math to deconvolve the raw sensor data into the full volume.</p>
<p>CTs have only a single intensity channel, similar to a grayscale image. This means that often, the channel dimension is left out in native data formats; By stacking individual 2D slices into a 3D tensor, we can build volumetric data representing the 3D anatomy of a subject. Unlike what we saw in figure 4.1, the extra dimension in figure 4.2 represents an offset in physical space, rather than a particular band of the visible spectrum.</p>
<p><img src="2.png" alt="2" style="zoom:45%;" /></p>
<center>
Figure 4.2 Slices of a CT scan, from the top of the head to the jawline
</center>
<p>Part 2 of this book will be devoted to tackling a medical imaging problem in the real world, so we won’t go into the details of medical-imaging data formats.</p>
<h4 id="loading-a-specialized-format">4.2.1 <em>Loading a specialized format</em></h4>
<p>Let’s load a sample CT scan using the <code>volread</code> function in the imageio module, which takes a directory as an argument and assembles all Digital Imaging and Communications in Medicine (DICOM) files2 in a series in a NumPy 3D array</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dir_path = <span class="string">&quot;../data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083&quot;</span></span><br><span class="line">vol_arr = imageio.volread(dir_path, <span class="string">&#x27;DICOM&#x27;</span>)</span><br><span class="line">vol_arr.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[2]:</span></span><br><span class="line">Reading DICOM (examining files): <span class="number">1</span>/<span class="number">99</span> files (<span class="number">1.0</span>%<span class="number">99</span>/<span class="number">99</span> files (<span class="number">100.0</span>%)</span><br><span class="line">  Found <span class="number">1</span> correct series.</span><br><span class="line">Reading DICOM (loading data): <span class="number">31</span>/<span class="number">99</span>  (<span class="number">31.392</span>/<span class="number">99</span>  (<span class="number">92.999</span>/<span class="number">99</span>  (<span class="number">100.0</span>%)</span><br><span class="line">(<span class="number">99</span>, <span class="number">512</span>, <span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<p>the layout is different from what PyTorch expects, due to having no channel information. So we’ll have to make room for the channel dimen- sion using unsqueeze:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line">vol = torch.from_numpy(vol_arr).<span class="built_in">float</span>()</span><br><span class="line">vol = torch.unsqueeze(vol, <span class="number">0</span>)</span><br><span class="line">vol.shape</span><br><span class="line"><span class="comment"># Out[3]:</span></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">99</span>, <span class="number">512</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure>
<p>At this point we could assemble a 5D dataset by stacking multiple volumes along the batch direction, just as we did in the previous section. We’ll see a lot more CT data in part 2.</p>
<h3 id="representing-tabular-data">4.3 <em>Representing tabular data</em></h3>
<p>The simplest form of data we’ll encounter on a machine learning job is sitting in a spreadsheet, CSV file, or database. Whatever the medium, it’s a table containing one row per sample (or record), where columns contain one piece of information about our sample.</p>
<p>At first we are going to assume there’s no meaning to the order in which samples appear in the table: such a table is a collection of independent samples. Columns may contain numerical values, like temperatures at specific locations; or labels, like a string expressing an attribute of the sample, like “blue.” Therefore, tabu- lar data is typically not homogeneous: different columns don’t have the same type.</p>
<p>PyTorch tensors, on the other hand, are homogeneous. Information in PyTorch is typically encoded as a number, typically floating-point. This numeric encoding is deliberate, since neural networks are mathematical entities that take real numbers as inputs and produce real numbers as output through successive application of matrix multiplications and nonlinear functions.</p>
<h4 id="using-a-real-world-dataset">4.3.1 <em>Using a real-world dataset</em></h4>
<p>A large number of tabular datasets are freely available on the internet; see, for instance, <a target="_blank" rel="noopener" href="https://github.com/caesar0301/awesome-public-datasets">https://github.com/caesar0301/awesome-public-datasets</a>. The Wine Quality dataset is a freely available table containing chemical characterizations of samples of <em>vinho verde</em>, a wine from north Portugal, together with a sensory quality score. The dataset for white wines can be downloaded here: http://mng.bz/90Ol.</p>
<p>The file contains a comma-separated collection of values organized in 12 columns preceded by a header line containing the column names. The first 11 columns con- tain values of chemical variables, and the last column contains the sensory quality score from 0 (very bad) to 10 (excellent).</p>
<blockquote>
<p>fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality</p>
</blockquote>
<p>A possible machine learning task on this dataset is predicting the quality score from chemical characterization alone. As we can see in figure 4.3, we’re hoping to find a relationship between one of the chem- ical columns in our data and the quality column.</p>
<p><img src="3.png" alt="3" style="zoom:45%;" /></p>
<center>
Figure 4.3 The (we hope) relationship between sulfur and quality in wine
</center>
<h4 id="loading-a-wine-data-tensor">4.3.2 <em>Loading a wine data tensor</em></h4>
<p>Python offers several options for quickly loading a CSV file. Three popular options are</p>
<ul>
<li>he csv module that ships with Python</li>
<li>NumPy</li>
<li>Pandas</li>
</ul>
<p>The third option is the most time- and memory-efficient. PyTorch has excellent NumPy interoperability. Let’s load our file and turn the resulting NumPy array into a PyTorch tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wine_path = <span class="string">&quot;../data/p1ch4/tabular-wine/winequality-white.csv&quot;</span></span><br><span class="line">wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=<span class="string">&quot;;&quot;</span>, skiprows=<span class="number">1</span>)</span><br><span class="line">wineq = torch.from_numpy(wineq_numpy)</span><br><span class="line">wineq_numpy</span><br><span class="line"><span class="comment"># Out[2]:</span></span><br><span class="line">array([[ <span class="number">7.</span>  ,  <span class="number">0.27</span>,  <span class="number">0.36</span>, ...,  <span class="number">0.45</span>,  <span class="number">8.8</span> ,  <span class="number">6.</span>  ],</span><br><span class="line">[<span class="number">6.3</span>, <span class="number">0.3</span>, <span class="number">0.34</span>,..., <span class="number">0.49</span>, <span class="number">9.5</span>, <span class="number">6.</span> ],</span><br><span class="line">[<span class="number">8.1</span>, <span class="number">0.28</span>, <span class="number">0.4</span>,..., <span class="number">0.44</span>,<span class="number">10.1</span>, <span class="number">6.</span> ],</span><br><span class="line">...,</span><br><span class="line">[ <span class="number">6.5</span> , <span class="number">0.24</span>, <span class="number">0.19</span>, ..., <span class="number">0.46</span>, <span class="number">9.4</span> , <span class="number">6.</span> ],</span><br><span class="line">[<span class="number">5.5</span>, <span class="number">0.29</span>, <span class="number">0.3</span>,..., <span class="number">0.38</span>,<span class="number">12.8</span>, <span class="number">7.</span> ],</span><br><span class="line">[ <span class="number">6.</span> , <span class="number">0.21</span>, <span class="number">0.38</span>, ..., <span class="number">0.32</span>, <span class="number">11.8</span> , <span class="number">6.</span> ]], dtype=float32)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>the delimiter used to separate values in each row, and the fact that the first line should not be read since it contains the column names.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">col_list = <span class="built_in">next</span>(csv.reader(<span class="built_in">open</span>(wine_path), delimiter=<span class="string">&#x27;;&#x27;</span>))</span><br><span class="line">wineq_numpy.shape, col_list</span><br><span class="line"><span class="comment"># Out[3]:</span></span><br><span class="line">((<span class="number">4898</span>, <span class="number">12</span>),</span><br><span class="line"> [<span class="string">&#x27;fixed acidity&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;volatile acidity&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;citric acid&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;residual sugar&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;chlorides&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;free sulfur dioxide&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;total sulfur dioxide&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;density&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;pH&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sulphates&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;alcohol&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;quality&#x27;</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Continuous, ordinal, and categorical values</p>
<ul>
<li><em>continuous</em> values. These are the most intu- itive when represented as numbers. They are strictly ordered, and a difference between various values has a strict meaning.</li>
<li>ordinal values. The strict ordering we have with continuous values remains, but the fixed relationship between values no longer applies.</li>
<li><em>categorical</em> values have neither ordering nor numerical meaning to their values. These are often just enumerations of possibilities assigned arbitrary numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example.</li>
</ul>
</blockquote>
<h4 id="representing-scores">4.3.3 <em>Representing scores</em></h4>
<p>We could treat the score as a continuous variable, keep it as a real number, and perform a regression task, or treat it as a label and try to guess the label from the chemical analysis in a classification task.</p>
<p>In both approaches, we will typically remove the score from the tensor of input data and keep it in a separate tensor, so that we can use the score as the ground truth without it being input to our model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">data = wineq[:, :-<span class="number">1</span>]</span><br><span class="line">data, data.shape</span><br><span class="line">Selects <span class="built_in">all</span> rows <span class="keyword">and</span> <span class="built_in">all</span> columns <span class="keyword">except</span> the last</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">(tensor([[ <span class="number">7.00</span>,  <span class="number">0.27</span>,  ...,  <span class="number">0.45</span>,  <span class="number">8.80</span>],</span><br><span class="line">         [ <span class="number">6.30</span>,  <span class="number">0.30</span>,  ...,  <span class="number">0.49</span>,  <span class="number">9.50</span>],</span><br><span class="line">         ...,</span><br><span class="line">         [ <span class="number">5.50</span>,  <span class="number">0.29</span>,  ...,  <span class="number">0.38</span>, <span class="number">12.80</span>],</span><br><span class="line">         [ <span class="number">6.00</span>,  <span class="number">0.21</span>,  ...,  <span class="number">0.32</span>, <span class="number">11.80</span>]]), torch.Size([<span class="number">4898</span>, <span class="number">11</span>]))</span><br><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line">target = wineq[:, -<span class="number">1</span>]</span><br><span class="line">target, target.shape</span><br><span class="line"><span class="comment"># Out[6]:</span></span><br><span class="line">Selects <span class="built_in">all</span> rows <span class="keyword">and</span> the last column</span><br><span class="line">        (tensor([<span class="number">6.</span>, <span class="number">6.</span>,  ..., <span class="number">7.</span>, <span class="number">6.</span>]), torch.Size([<span class="number">4898</span>]))</span><br></pre></td></tr></table></figure>
<p>If we want to transform the target tensor in a tensor of labels, we have two options, depending on the strategy or what we use the categorical data for. One is simply to treat labels as an integer vector of scores:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[7]:</span></span><br><span class="line">target = wineq[:, -<span class="number">1</span>].long()</span><br><span class="line">target</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">tensor([<span class="number">6</span>, <span class="number">6</span>,  ..., <span class="number">7</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>If targets were string labels, like <em>wine color</em>, assigning an integer number to each string would let us follow the same approach.</p>
<h4 id="one-hot-encoding">4.3.4 One-hot encoding</h4>
<p>The other approach is to build a <em>one-hot encoding</em> of the scores: that is, encode each of the 10 scores in a vector of 10 elements, with all elements set to 0 but one, at a different index for each score. scores are purely discrete, like grape variety, one-hot encoding will be a much better fit, as there’s no implied ordering or distance.</p>
<p>We can achieve one-hot encoding using the <code>scatter_</code> method</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line">target_onehot = torch.zeros(target.shape[<span class="number">0</span>], <span class="number">10</span>)</span><br><span class="line">target_onehot.scatter_(<span class="number">1</span>, target.unsqueeze(<span class="number">1</span>), <span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># Out[8]:</span></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,  ..., <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>The arguments for <code>scatter_</code> are as follows:</p>
<ul>
<li>The dimension along which the following two arguments are specified</li>
<li>A column tensor indicating the indices of the elements to scatter</li>
<li>A tensor containing the elements to scatter or a single scalar to scatter (1, in this case)</li>
</ul>
<p>The second argument of <code>scatter_</code>, the index tensor, is required to have the same number of dimensions as the tensor we scatter into. Since <code>target_onehot</code> has two dimensions (4,898 × 10), we need to add an extra dummy dimension to target using <code>unsqueeze</code>. The call to unsqueeze adds a <em>singleton</em> dimension, from a 1D tensor of 4,898 elements to a 2D tensor of size (4,898 × 1), without changing its contents—no extra elements are added</p>
<h4 id="when-to-categorize">4.3.5 <em>When to categorize</em></h4>
<p>We summarize our data mapping in a small flow chart in figure 4.4.</p>
<p><img src="4.png" alt="4" style="zoom:50%;" /></p>
<center>
Figure 4.4 How to treat columns with continuous, ordinal, and categorical data
</center>
<p>We can use the functions in the PyTorch Tensor API to manipulate our data in tensor form. Let’s first obtain the mean and standard deviations for each column:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data.mean(<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">6.8548e+00</span>, <span class="number">2.7824e-01</span>, <span class="number">3.3419e-01</span>, <span class="number">6.3914e+00</span>, <span class="number">4.5772e-02</span>, <span class="number">3.5308e+01</span>,</span><br><span class="line">        <span class="number">1.3836e+02</span>, <span class="number">9.9403e-01</span>, <span class="number">3.1883e+00</span>, <span class="number">4.8985e-01</span>, <span class="number">1.0514e+01</span>])</span><br><span class="line"></span><br><span class="line">data.var(<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">7.1211e-01</span>, <span class="number">1.0160e-02</span>, <span class="number">1.4646e-02</span>, <span class="number">2.5726e+01</span>, <span class="number">4.7733e-04</span>, <span class="number">2.8924e+02</span>,</span><br><span class="line">        <span class="number">1.8061e+03</span>, <span class="number">8.9455e-06</span>, <span class="number">2.2801e-02</span>, <span class="number">1.3025e-02</span>, <span class="number">1.5144e+00</span>])</span><br></pre></td></tr></table></figure>
<p>At this point, we can normalize the data by subtracting the mean and dividing by the standard deviation, which helps with the learning process</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data_normalized = (data-data.mean(<span class="number">0</span>))/data.var(<span class="number">0</span>).sqrt()</span><br><span class="line">tensor([[ <span class="number">1.7208e-01</span>, -<span class="number">8.1761e-02</span>,  <span class="number">2.1326e-01</span>,  ..., -<span class="number">1.2468e+00</span>,</span><br><span class="line">         -<span class="number">3.4915e-01</span>, -<span class="number">1.3930e+00</span>],</span><br><span class="line">        [-<span class="number">6.5743e-01</span>,  <span class="number">2.1587e-01</span>,  <span class="number">4.7996e-02</span>,  ...,  <span class="number">7.3995e-01</span>,</span><br><span class="line">          <span class="number">1.3422e-03</span>, -<span class="number">8.2419e-01</span>],</span><br><span class="line">        [ <span class="number">1.4756e+00</span>,  <span class="number">1.7450e-02</span>,  <span class="number">5.4378e-01</span>,  ...,  <span class="number">4.7505e-01</span>,</span><br><span class="line">         -<span class="number">4.3677e-01</span>, -<span class="number">3.3663e-01</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">4.2043e-01</span>, -<span class="number">3.7940e-01</span>, -<span class="number">1.1915e+00</span>,  ..., -<span class="number">1.3130e+00</span>,</span><br><span class="line">         -<span class="number">2.6153e-01</span>, -<span class="number">9.0545e-01</span>],</span><br><span class="line">        [-<span class="number">1.6054e+00</span>,  <span class="number">1.1666e-01</span>, -<span class="number">2.8253e-01</span>,  ...,  <span class="number">1.0049e+00</span>,</span><br><span class="line">         -<span class="number">9.6251e-01</span>,  <span class="number">1.8574e+00</span>],</span><br><span class="line">        [-<span class="number">1.0129e+00</span>, -<span class="number">6.7703e-01</span>,  <span class="number">3.7852e-01</span>,  ...,  <span class="number">4.7505e-01</span>,</span><br><span class="line">         -<span class="number">1.4882e+00</span>,  <span class="number">1.0448e+00</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="finding-thresholds">4.3.6 <em>Finding thresholds</em></h4>
<p>Next, let’s start to look at the data with an eye to seeing if there is an easy way to tell good and bad wines apart at a glance. First, we’re going to determine which rows in target correspond to a score less than or equal to 3:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch also provides comparison functions, here torch.le(target, 3), </span></span><br><span class="line"><span class="comment"># but using operators seems to be a good standard.</span></span><br><span class="line">bad_indexes = target &lt;= <span class="number">3</span></span><br><span class="line">bad_indexes.shape, bad_indexes.dtype, bad_indexes.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[13]:</span></span><br><span class="line">(torch.Size([<span class="number">4898</span>]), torch.<span class="built_in">bool</span>, tensor(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<p>Note that only 20 of the bad_indexes entries are set to True! By using a feature in PyTorch called <em>advanced indexing</em>, we can use a tensor with data type torch.bool to index the data tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[14]:</span></span><br><span class="line">bad_data = data[bad_indexes]</span><br><span class="line">bad_data.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[14]:</span></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">11</span>])</span><br></pre></td></tr></table></figure>
<p>Now we can start to get information about wines grouped into good, middling, and bad categories. Let’s take the .mean() of each column:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[15]:</span></span><br><span class="line">bad_data = data[target &lt;= <span class="number">3</span>]</span><br><span class="line">mid_data = data[(target&gt;<span class="number">3</span>) &amp; (target&lt;<span class="number">7</span>)]</span><br><span class="line">good_data = data[target &gt;= <span class="number">7</span>]</span><br><span class="line"></span><br><span class="line">bad_mean = bad_data.mean(<span class="number">0</span>)</span><br><span class="line">mid_mean = mid_data.mean(<span class="number">0</span>)</span><br><span class="line">good_mean = good_data.mean(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, args <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(col_list, bad_mean, mid_mean, good_mean)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;:2&#125; &#123;:20&#125; &#123;:6.2f&#125; &#123;:6.2f&#125; &#123;:6.2f&#125;&#x27;</span>.<span class="built_in">format</span>(i, *args))</span><br><span class="line">  </span><br><span class="line"> <span class="number">0</span> fixed acidity          <span class="number">7.60</span>   <span class="number">6.89</span>   <span class="number">6.73</span></span><br><span class="line"> <span class="number">1</span> volatile acidity       <span class="number">0.33</span>   <span class="number">0.28</span>   <span class="number">0.27</span></span><br><span class="line"> <span class="number">2</span> citric acid            <span class="number">0.34</span>   <span class="number">0.34</span>   <span class="number">0.33</span></span><br><span class="line"> <span class="number">3</span> residual sugar         <span class="number">6.39</span>   <span class="number">6.71</span>   <span class="number">5.26</span></span><br><span class="line"> <span class="number">4</span> chlorides              <span class="number">0.05</span>   <span class="number">0.05</span>   <span class="number">0.04</span></span><br><span class="line"> <span class="number">5</span> free sulfur dioxide   <span class="number">53.33</span>  <span class="number">35.42</span>  <span class="number">34.55</span></span><br><span class="line"> <span class="number">6</span> total sulfur dioxide <span class="number">170.60</span> <span class="number">141.83</span> <span class="number">125.25</span></span><br><span class="line"> <span class="number">7</span> density                <span class="number">0.99</span>   <span class="number">0.99</span>   <span class="number">0.99</span></span><br><span class="line"> <span class="number">8</span> pH                     <span class="number">3.19</span>   <span class="number">3.18</span>   <span class="number">3.22</span></span><br><span class="line"> <span class="number">9</span> sulphates              <span class="number">0.47</span>   <span class="number">0.49</span>   <span class="number">0.50</span></span><br><span class="line"><span class="number">10</span> alcohol               <span class="number">10.34</span>  <span class="number">10.26</span>  <span class="number">11.42</span></span><br></pre></td></tr></table></figure>
<p>Let’s get the indexes where the total sulfur dioxide column is below the midpoint we calculated earlier, like so:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">total_sulfur_threshold = <span class="number">141.83</span></span><br><span class="line">total_sulfur_data = data[:,<span class="number">6</span>]</span><br><span class="line">predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)</span><br><span class="line">predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.<span class="built_in">sum</span>() </span><br><span class="line"><span class="comment"># Out[16]:</span></span><br><span class="line">(torch.Size([<span class="number">4898</span>]), torch.<span class="built_in">bool</span>, tensor(<span class="number">2727</span>))</span><br></pre></td></tr></table></figure>
<p>This means our threshold implies that just over half of all the wines are going to be high quality.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line">actual_indexes = target &gt; <span class="number">5</span></span><br><span class="line">actual_indexes.shape, actual_indexes.dtype, actual_indexes.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># Out[17]:</span></span><br><span class="line">(torch.Size([<span class="number">4898</span>]), torch.<span class="built_in">bool</span>, tensor(<span class="number">3258</span>))</span><br><span class="line">n_predicted = predicted_index.<span class="built_in">sum</span>().item()</span><br><span class="line">n_actual = actual_index.<span class="built_in">sum</span>().item()</span><br><span class="line">n_matches = (predicted_index &amp; actual_index).<span class="built_in">sum</span>()</span><br><span class="line">predicted_acc = n_matches / n_predicted</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;acc: <span class="subst">&#123;predicted_acc*<span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%    match/actual: <span class="subst">&#123;n_matches/n_actual*<span class="number">100</span>:<span class="number">.3</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line">acc: <span class="number">74.001</span>%    match/actual: <span class="number">61.940</span>%</span><br></pre></td></tr></table></figure>
<p>there are 3,200 good wines, and we only identified 61% of them. Of course, this is all very naive: we know for sure that multiple variables contribute to wine quality, and the relationships between the values of these variables and the outcome (which could be the actual score, rather than a binarized version of it) is likely more complicated than a simple threshold on a single value.</p>
<h3 id="working-with-time-series">4.4 <em>Working with time series</em></h3>
<p>every row in the table was independent from the others; their order did not matter. Or, equivalently, there was no column that encoded information about what rows came earlier and what came later.</p>
<p>In the meantime, we’ll switch to another interesting dataset: data from a Washington, D.C., bike-sharing system reporting the hourly count of rental bikes in 2011–2012 in the Capital Bikeshare system, along with weather and seasonal information (available here: http://mng.bz/jgOx). Our goal will be to take a flat, 2D dataset and transform it into a 3D one, as shown in figure 4.5.</p>
<p><img src="5.png" alt="5" style="zoom:50%;" /></p>
<center>
Figure 4.5 Transforming a 1D, multichannel dataset into a 2D, multichannel dataset by separating the date and hour of each sample into separate axes
</center>
<h4 id="adding-a-time-dimension">4.4.1 <em>Adding a time dimension</em></h4>
<p>In the source data, each row is a separate hour of data (figure 4.5 shows a transposed version of this to better fit on the printed page). We want to change the row-per-hour organization so that we have one axis that increases at a rate of one day per index increment, and another axis that represents the hour of the day (independent of the date). The third axis will be our different columns of data (weather, temperature, and so on).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[2]:</span></span><br><span class="line">bikes_numpy = np.loadtxt(</span><br><span class="line">    <span class="string">&quot;../data/p1ch4/bike-sharing-dataset/hour-fixed.csv&quot;</span>,</span><br><span class="line">    dtype=np.float32,</span><br><span class="line">    delimiter=<span class="string">&quot;,&quot;</span>,</span><br><span class="line">    skiprows=<span class="number">1</span>,</span><br><span class="line">    <span class="comment"># Converts date strings to numbers corresponding to the day of the month</span></span><br><span class="line">	converters=&#123;<span class="number">1</span>: <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x[<span class="number">8</span>:<span class="number">10</span>])&#125;)</span><br><span class="line">bikes = torch.from_numpy(bikes_numpy)</span><br><span class="line">bikes</span><br><span class="line"><span class="comment"># Out[2]:</span></span><br><span class="line">tensor([[<span class="number">1.0000e+00</span>, <span class="number">1.0000e+00</span>,  ..., <span class="number">1.3000e+01</span>, <span class="number">1.6000e+01</span>],</span><br><span class="line">        [<span class="number">2.0000e+00</span>, <span class="number">1.0000e+00</span>,  ..., <span class="number">3.2000e+01</span>, <span class="number">4.0000e+01</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">1.7378e+04</span>, <span class="number">3.1000e+01</span>,  ..., <span class="number">4.8000e+01</span>, <span class="number">6.1000e+01</span>],</span><br><span class="line">        [<span class="number">1.7379e+04</span>, <span class="number">3.1000e+01</span>,  ..., <span class="number">3.7000e+01</span>, <span class="number">4.9000e+01</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>For every hour, the dataset reports the following variables:</p>
<ul>
<li>Index of record: instant</li>
<li>Day of month: day</li>
<li>Season: season (1: spring, 2: summer, 3: fall, 4: winter)</li>
<li>Year: yr (0: 2011, 1: 2012)</li>
<li>Month:mnth(1to12)</li>
<li>Hour:hr(0to23)</li>
<li>Holiday status: holiday</li>
<li>Day of the week: weekday</li>
<li>Working day status: workingday</li>
<li>Weather situation: weathersit (1: clear, 2:mist, 3: light rain/snow, 4: heavy rain/snow)</li>
<li>Temperature in °C: temp</li>
<li>Perceived temperature in °C: atemp</li>
<li>Humidity:hum</li>
<li>Wind speed: windspeed</li>
<li>Number of casual users: casual</li>
<li>Number of registered users: registered</li>
<li>Count of rental bikes: cnt</li>
</ul>
<p>In a time series dataset such as this one, rows represent successive time-points: there is a dimension along which they are ordered. the existence of an ordering gives us the opportunity to exploit causal relationships across time. For instance, it allows us to predict bike rides at one time based on the fact that it was raining at an earlier time.</p>
<p>This neural network model will need to see a number of sequences of values for each different quantity, such as ride count, time of day, temperature, and weather conditions: <em>N</em> parallel sequences of size <em>C</em>.</p>
<h4 id="shaping-the-data-by-time-period">4.4.2 <em>Shaping the data by time period</em></h4>
<p>We might want to break up the two-year dataset into wider observation periods, like days. This way we’ll have <em>N</em> (for <em>number of samples</em>) collections of <em>C</em> sequences of length <em>L</em>.In other words, our time series dataset would be a tensor of dimension 3 and shape <em>N</em> × <em>C</em> × <em>L</em>. The C would remain our 17 channels, while L would be 24.</p>
<p>Let’s go back to our bike-sharing dataset. The first column is the index (the global ordering of the data), the second is the date, and the sixth is the time of day. We have everything we need to create a dataset of daily sequences of ride counts and other exogenous variables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line">bikes.shape, bikes.stride()</span><br><span class="line"><span class="comment"># Out[3]:</span></span><br><span class="line">(torch.Size([<span class="number">17520</span>, <span class="number">17</span>]), (<span class="number">17</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>That’s 17,520 hours, 17 columns. Now let’s reshape the data to have 3 axes—day, hour, and then our 17 columns:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line">daily_bikes = bikes.view(-<span class="number">1</span>, <span class="number">24</span>, bikes.shape[<span class="number">1</span>])</span><br><span class="line">daily_bikes.shape, daily_bikes.stride()</span><br><span class="line"><span class="comment"># Out[4]:</span></span><br><span class="line">(torch.Size([<span class="number">730</span>, <span class="number">24</span>, <span class="number">17</span>]), (<span class="number">408</span>, <span class="number">17</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>As you learned in the previous chapter, calling <code>view</code> on a tensor <strong>returns a new tensor that changes the number of dimensions and the striding information, without changing the storage.</strong>This means we can rearrange our tensor at basically zero cost, because no data will be copied.</p>
<p>We see that the rightmost dimension is the number of columns in the original dataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential hours. In other words, we now have <em>N</em> sequences of <em>L</em> hours in a day, for <em>C</em> channels. To get to our desired <em>N</em> × <em>C</em> × <em>L</em> ordering, we need to transpose the tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line">daily_bikes = daily_bikes.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">daily_bikes.shape, daily_bikes.stride()</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">(torch.Size([<span class="number">730</span>, <span class="number">17</span>, <span class="number">24</span>]), (<span class="number">408</span>, <span class="number">1</span>, <span class="number">17</span>))</span><br></pre></td></tr></table></figure>
<h4 id="ready-for-training">4.4.3 <em>Ready for training</em></h4>
<p>The “weather situation” variable is ordinal. It has four levels: 1 for good weather, and 4 for, er, really bad. We could treat this variable as categorical, with levels interpreted as labels, or as a continuous variable. If we decided to go with categorical, we would turn the variable into a one-hot-encoded vector and concatenate the columns with the dataset.</p>
<p>In order to make it easier to render our data, we’re going to limit ourselves to the first day for a moment. We initialize a zero-filled matrix with a number of rows equal to the number of hours in the day and number of columns equal to the number of weather levels:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line">first_day = bikes[:<span class="number">24</span>].long()</span><br><span class="line">weather_onehot = torch.zeros(first_day.shape[<span class="number">0</span>], <span class="number">4</span>)</span><br><span class="line">first_day[:,<span class="number">9</span>]</span><br><span class="line"><span class="comment"># Out[6]:</span></span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line"><span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>Then we scatter ones into our matrix according to the corresponding level at each row.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># In[7]:</span></span><br><span class="line">weather_onehot.scatter_(</span><br><span class="line">    dim=<span class="number">1</span>,</span><br><span class="line">    index=first_day[:,<span class="number">9</span>].unsqueeze(<span class="number">1</span>).long() - <span class="number">1</span>,</span><br><span class="line">    value=<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>Last, we concatenate our matrix to our original dataset using the cat function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line">torch.cat((bikes[:<span class="number">24</span>], weather_onehot), <span class="number">1</span>)[:<span class="number">1</span>]</span><br><span class="line"><span class="comment"># Out[8]:</span></span><br><span class="line">tensor([[ <span class="number">1.0000</span>,  <span class="number">1.0000</span>,  <span class="number">1.0000</span>,  <span class="number">0.0000</span>,  <span class="number">1.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">         <span class="number">6.0000</span>,  <span class="number">0.0000</span>,  <span class="number">1.0000</span>,  <span class="number">0.2400</span>,  <span class="number">0.2879</span>,  <span class="number">0.8100</span>,  <span class="number">0.0000</span>,</span><br><span class="line">         <span class="number">3.0000</span>, <span class="number">13.0000</span>, <span class="number">16.0000</span>,  <span class="number">1.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>
<p>We could have done the same with the reshaped daily_bikes tensor. Remember that it is shaped (<em>B</em>, <em>C</em>, <em>L</em>), where <em>L</em> = 24. We first create the zero tensor, with the same <em>B</em> and <em>L</em>, but with the number of additional columns as <em>C</em>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">daily_weather_onehot = torch.zeros(daily_bikes.shape[<span class="number">0</span>], <span class="number">4</span>, daily_bikes.shape[<span class="number">2</span>]).scatter(<span class="number">1</span>, daily_bikes[:,<span class="number">9</span>,:].unsqueeze(<span class="number">1</span>).long()-<span class="number">1</span>, <span class="number">1.0</span>).shape</span><br><span class="line">daily_weather_onehot.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[10]:</span></span><br><span class="line">torch.Size([<span class="number">730</span>, <span class="number">4</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure>
<p>And we concatenate along the <em>C</em> dimension:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[11]:</span></span><br><span class="line">daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>We mentioned earlier that this is not the only way to treat our “weather situation” variable. Indeed, its labels have an ordinal relationship, so we could pretend they are spe- cial values of a continuous variable. We could just transform the variable so that it runs from 0.0 to 1.0:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[12]:</span></span><br><span class="line">daily_bikes[:, <span class="number">9</span>, :] = (daily_bikes[:, <span class="number">9</span>, :] - <span class="number">1.0</span>) / <span class="number">3.0</span></span><br></pre></td></tr></table></figure>
<p>As we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval or the [-1.0, 1.0] interval is something we’ll want to do for all quantitative variables, like temperature (column 10 in our dataset).There are multiple possibilities for rescaling variables. We can either map their range to [0.0, 1.0]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[13]:</span></span><br><span class="line">temp = daily_bikes[:, <span class="number">10</span>, :]</span><br><span class="line">temp_min = torch.<span class="built_in">min</span>(temp)</span><br><span class="line">temp_max = torch.<span class="built_in">max</span>(temp)</span><br><span class="line">daily_bikes[:, <span class="number">10</span>, :] = ((daily_bikes[:, <span class="number">10</span>, :] - temp_min)</span><br><span class="line">                         / (temp_max - temp_min))</span><br></pre></td></tr></table></figure>
<p>or subtract the mean and divide by the standard deviation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[14]:</span></span><br><span class="line">temp = daily_bikes[:, <span class="number">10</span>, :]</span><br><span class="line">daily_bikes[:, <span class="number">10</span>, :] = ((daily_bikes[:, <span class="number">10</span>, :] - torch.mean(temp))</span><br><span class="line">                         / torch.std(temp))</span><br></pre></td></tr></table></figure>
<p>In the latter case, our variable will have 0 mean and unitary standard deviation. If our variable were drawn from a Gaussian distribution, 68% of the samples would sit in the [-1.0, 1.0] interval.</p>
<h3 id="representing-text">4.5 <em>Representing text</em></h3>
<p>Deep learning has taken the field of natural language processing (NLP) by storm, par- ticularly using models that repeatedly consume a combination of new input and previ- ous model output. These models are called <em>recurrent neural networks</em> (RNNs), and they have been applied with great success to text categorization, text generation, and automated translation systems. More recently, a class of networks called <em>transformers</em> with a more flexible way to incorporate past information has made a big splash. Previous NLP workloads were characterized by sophisticated multistage pipelines that included rules encoding the grammar of a language.</p>
<p>Our goal in this section is to turn text into something a neural network can pro- cess: a tensor of numbers, just like our previous cases.</p>
<h4 id="converting-text-to-numbers">4.5.1 <em>Converting text to numbers</em></h4>
<p>There are two particularly intuitive levels at which networks operate on text:</p>
<ul>
<li>character level, by processing one character at a time</li>
<li>word level, where individual words are the finest-grained entities to be seen by the network.</li>
</ul>
<p>Let’s start with a character-level example. An amazing resource here is <a href="www.gutenberg.org"><strong>Project Gutenberg</strong></a>, a volunteer effort to digitize and archive cultural work and make it available for free in open formats, including plain text files. If we’re aiming at larger-scale corpora, the Wikipedia corpus stands out: it’s the complete collection of Wikipedia articles, containing 1.9 billion words and more than 4.4 million articles. Several other corpora can be found at the <a href="www.english-corpora.org">English Corpora website</a>.</p>
<p>Let’s load <a href="www.gutenberg.org/files/1342/1342-0.txt.">Jane Austen’s <em>Pride and Prejudice</em> from the Project Gutenberg website</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_path, mode=<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br></pre></td></tr></table></figure>
<h4 id="one-hot-encoding-characters">4.5.2 <em>One-hot-encoding characters</em></h4>
<p>Every written character is represented by a code: a sequence of bits of appropriate length so that each character can be uniquely identified. The simplest such encoding is ASCII. ASCII encodes 128 characters using 128 integers. For instance, the letter <em>a</em> corresponds to binary 1100001 or decimal 97, the letter <em>b</em> to binary 1100010 or decimal 98, and so on. The encoding fits 8 bits, which was a big bonus in 1965.</p>
<p>In our case, since we loaded text in English, it is safe to use ASCII and deal with a small encoding. We could also make all of the characters lowercase, to reduce the number of different characters in our encoding. Similarly, we could screen out punctuation, numbers, or other characters that aren’t relevant to our expected kinds of text. This may or may not make a practical difference to a neural network, depending on the task at hand.</p>
<p>We first split our text into a list of lines and pick an arbitrary line to focus on:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line">lines = text.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">line = lines[<span class="number">200</span>]</span><br><span class="line">line</span><br><span class="line"><span class="comment"># Out[3]:</span></span><br><span class="line"><span class="string">&#x27;“Impossible, Mr. Bennet, impossible, when I am not acquainted with him&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line">letter_t = torch.zeros(<span class="built_in">len</span>(line), <span class="number">128</span>)</span><br><span class="line">letter_t.shape</span><br><span class="line"><span class="comment"># Out[4]:</span></span><br><span class="line">torch.Size([<span class="number">70</span>, <span class="number">128</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, letter <span class="keyword">in</span> <span class="built_in">enumerate</span>(line.lower().strip()):</span><br><span class="line">    letter_index = <span class="built_in">ord</span>(letter) <span class="keyword">if</span> <span class="built_in">ord</span>(letter) &lt; <span class="number">128</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"> letter_t[i][letter_index] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="one-hot-encoding-whole-words">4.5.3 <em>One-hot encoding whole words</em></h4>
<p>Word-level encoding can be done the same way by establishing a vocabulary and one-hot encoding sentences—sequences of words—along the rows of our tensor.</p>
<p>Define clean_words, which takes text and returns it in lowercase and stripped of punctuation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_words</span>(<span class="params">txt</span>):</span></span><br><span class="line">    punctuation = <span class="string">&quot;.,;:\&quot;!?”“_-+=`~|/&gt;&lt;$%^&amp;A*()[]&#123;&#125;#&#x27;&quot;</span></span><br><span class="line">    word_list = txt.lower().replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>).split()</span><br><span class="line">    word_list = [word.strip(punctuation) <span class="keyword">for</span> word <span class="keyword">in</span> word_list]</span><br><span class="line">    (<span class="keyword">lambda</span> x: [x.pop(x.index(<span class="string">&#x27;&#x27;</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.count(<span class="string">&#x27;&#x27;</span>))])(word_list)</span><br><span class="line">    <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_list = <span class="built_in">sorted</span>(<span class="built_in">set</span>(clean_words(text)))</span><br><span class="line">word2index = &#123;word: i <span class="keyword">for</span> (i, word) <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line"><span class="built_in">len</span>(word2index_dict), word2index_dict[<span class="string">&#x27;impossible&#x27;</span>]</span><br><span class="line">(<span class="number">7164</span>, <span class="number">3325</span>)</span><br></pre></td></tr></table></figure>
<p>Note that word2index_dict is now a dictionary with words as keys and an integer as a value. We create an empty vector and assign the one-hot-encoded values of the word in the sentence:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">word_in_line = clean_words(line)</span><br><span class="line">word_t = torch.zeros(<span class="built_in">len</span>(word_in_line), <span class="built_in">len</span>(word_list))</span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_in_line):</span><br><span class="line">    index = word2index_dict[word]</span><br><span class="line">    word_t[i][index] = <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span> <span class="subst">&#123;index&#125;</span> <span class="subst">&#123;word&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(word_t.shape)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="number">0</span> <span class="number">3325</span> impossible</span><br><span class="line"><span class="number">1</span> <span class="number">4224</span> mr</span><br><span class="line"><span class="number">2</span> <span class="number">757</span> bennet</span><br><span class="line"><span class="number">3</span> <span class="number">3325</span> impossible</span><br><span class="line"><span class="number">4</span> <span class="number">6985</span> when</span><br><span class="line"><span class="number">5</span> <span class="number">3246</span> i</span><br><span class="line"><span class="number">6</span> <span class="number">363</span> am</span><br><span class="line"><span class="number">7</span> <span class="number">4352</span> <span class="keyword">not</span></span><br><span class="line"><span class="number">8</span> <span class="number">187</span> acquainted</span><br><span class="line"><span class="number">9</span> <span class="number">7053</span> <span class="keyword">with</span></span><br><span class="line"><span class="number">10</span> <span class="number">3147</span> him</span><br><span class="line">torch.Size([<span class="number">11</span>, <span class="number">7164</span>])</span><br></pre></td></tr></table></figure>
<p>Figure 4.6 compares the gist of our two options for splitting text. The choice between character-level and word-level encoding leaves us to make a trade-off. In many languages, there are significantly fewer characters than words. On the other hand, words convey much more meaning than individual characters, so a representation of words is con- siderably more informative by itself.</p>
<p><img src="6.png" alt="6" style="zoom:50%;" /></p>
<center>
Figure 4.6 Three ways to encode a word
</center>
<h4 id="text-embeddings">4.5.4 <em>Text embeddings</em></h4>
<p>Instead of vectors of many zeros and a single one, we can use vectors of floating-point numbers. <strong>A vector of, say, 100 floating-point numbers can indeed represent a large number of words. The trick is to find an effective way to map individual words into this 100-dimensional space in a way that facilitates downstream learning.</strong> This is called an <strong><em>embedding</em></strong>.</p>
<p>In principle, we could simply iterate over our vocabulary and generate a set of 100 random floating-point numbers for each word. This would work, in that we could cram a very large vocabulary into just 100 numbers, <strong>but it would forgo any concept of distance between words based on meaning or context. An ideal solution would be to generate the embedding in such a way that words used in similar contexts mapped to nearby regions of the embedding.</strong></p>
<p>if we were to design a solution to this problem by hand, we might decide to build our embedding space by choosing to map basic nouns and adjectives along the axes. We can generate a 2D space where axes map to nouns—<em>fruit</em> (0.0-0.33), <em>flower</em> (0.33-0.66), and <em>dog</em> (0.66-1.0)—and adjectives—<em>red</em> (0.0-0.2), <em>orange</em> (0.2-0.4), <em>yellow</em> (0.4-0.6), <em>white</em> (0.6-0.8), and <em>brown</em> (0.8-1.0).</p>
<blockquote>
<p>Our goal is to take actual fruit, flowers, and dogs and lay them out in the embedding. we can map <em>apple</em> to a number in the <em>fruit</em> and <em>red</em> quadrant. Likewise, we can easily map <em>tangerine</em>, <em>lemon</em>, <em>lychee</em>, and <em>kiwi</em> (to round out our list of colorful fruits).Then we can start on flowers, and assign <em>rose</em>, <em>poppy</em>, <em>daffodil</em>, <em>lily</em>, and ... Well, <em>sunflower</em> can get <em>flower</em>, <em>yellow</em>, and <em>brown</em>, and then <em>daisy</em> can get <em>flower</em>, <em>white</em>, and <em>yellow</em>. For dogs and color, we can embed <em>redbone</em> near <em>red</em>; uh, <em>fox</em> perhaps for <em>orange</em>; <em>golden retriever</em> for <em>yellow</em>, <em>poodle</em> for <em>white</em>, and ... most kinds of dogs are <em>brown</em>. Now our embeddings look like figure 4.7.</p>
</blockquote>
<p><img src="7.png" alt="7" style="zoom:50%;" /></p>
<center>
Figure 4.7 Our manual word embeddings
</center>
<p>This kind of work can be automated. By processing a large corpus of organic text, embeddings similar to the one we just discussed can be generated. The main differences are that there are 100 to 1,000 elements in the embedding vector and that <strong>axes do not map directly to concepts: rather, conceptually similar words map in neighboring regions of an embedding space whose axes are arbitrary floating-point dimensions.</strong> embeddings are often generated using neural networks, trying to predict a word from nearby words (the context) in a sentence.</p>
<p>In this case, we could start from one-hot-encoded words and use a (usually rather shallow) neural network to generate the embedding. One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also having consistent spatial relationships with other words. For example, we could begin to perform analogies like <code>lemon ≈ apple - red - sweet + yellow + sour</code></p>
<p>More contemporary embedding models—with BERT and GPT-2 making headlines even in mainstream media—are much more elaborate and are context sensitive: that is, the mapping of a word in the vocabulary to a vector is not fixed but depends on the surrounding sentence.</p>
<h4 id="text-embeddings-as-a-blueprint">4.5.5 <em>Text embeddings as a blueprint</em></h4>
<p>We believe that how text is represented and processed can also be seen as an example for dealing with categorical data in general. Embeddings are useful wherever one-hot encoding becomes cumbersome. processing text is perhaps the most common, well-explored task dealing with sequences; so, for example, when working on tasks with time series, we might look for inspiration in what is done in natural language processing.</p>
<h2 id="chapter-5-the-mechanics-of-learning">Chapter 5 <em>The mechanics of learning</em></h2>
<ul>
<li>Understanding how algorithms can learn from data</li>
<li>Reframing learning as parameter estimation, using differentiation and gradient descent</li>
<li>Walking through a simple learning algorithm</li>
<li>How PyTorch supports learning with autograd</li>
</ul>
<p>how is it exactly that a machine learns? What are the mechanics of this process—or, in words, what is the <em>algorithm</em> behind it? From the point of view of an observer, a learning algorithm is presented with input data that is paired with desired outputs. Once learning has occurred, that algorithm will be capable of producing correct outputs when it is fed new data that is <em>similar enough</em> to the input data it was trained on.</p>
<h3 id="a-timeless-lesson-in-modeling">5.1 <em>A timeless lesson in modeling</em></h3>
<p>In fact, in this book there is virtually no difference between saying that we’ll <em>fit</em> the data or that we’ll make an algorithm <em>learn</em> from data. The process always involves a function with a number of unknown parameters whose values are estimated from data: in short, a <em>model</em>.</p>
<p>In particular, PyTorch is designed to make it easy to create models for which the derivatives of the fitting error, with respect to the parameters, can be expressed analytically.</p>
<p>This chapter is about how to automate generic function-fitting. After all, this is what we do with deep learning—deep neural networks being the generic functions we’re talking about—and PyTorch makes this process as simple and transparent as possible.</p>
<h3 id="learning-is-just-parameter-estimation">5.2 <em>Learning is just parameter estimation</em></h3>
<p>In this section, we’ll learn how we can take data, choose a model, and estimate the parameters of the model so that it will give good predictions on new data. To do so, we’ll divert our attention to the second- hardest problem in physics: <strong>calibrating instruments.</strong></p>
<p>Figure 5.2 shows the high-level overview of what we’ll implement by the end of the chapter.</p>
<ul>
<li>Given input data and the corresponding desired outputs (<strong>ground truth</strong>), as well as initial values for the weights, the model is fed input data (forward pass), and a measure of the error is evaluated by comparing the resulting outputs to the ground truth.</li>
<li>In order to optimize the parameter of the model—its <em>weights</em>—the change in the error following a unit change in weights (that is, the <em>gradient</em> of the error with respect to the parameters) is computed using the chain rule for the derivative of a composite function (backward pass).</li>
<li>The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level.</li>
</ul>
<p>When we finish the chapter, we will have covered many of the essential concepts that underlie training deep neural networks, even if our motivating example is very simple and our model isn’t actually a neural network</p>
<p><img src="8.png" alt="8" style="zoom:45%;" /></p>
<center>
Figure 5.2 Our mental model of the learning process
</center>
<h4 id="a-hot-problem">5.2.1 <em>A hot problem</em></h4>
<p>we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.</p>
<h4 id="gathering-some-data">5.2.2 <em>Gathering some data</em></h4>
<p>We’ll start by making a note of temperature data in good old Celsius5 and measurements from our new thermometer, and figure things out.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[2]:</span></span><br><span class="line">t_c = [<span class="number">0.5</span>,  <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">28.0</span>, <span class="number">11.0</span>,  <span class="number">8.0</span>,  <span class="number">3.0</span>, -<span class="number">4.0</span>,  <span class="number">6.0</span>, <span class="number">13.0</span>, <span class="number">21.0</span>]</span><br><span class="line">t_u = [<span class="number">35.7</span>, <span class="number">55.9</span>, <span class="number">58.2</span>, <span class="number">81.9</span>, <span class="number">56.3</span>, <span class="number">48.9</span>, <span class="number">33.9</span>, <span class="number">21.8</span>, <span class="number">48.4</span>, <span class="number">60.4</span>, <span class="number">68.4</span>]</span><br><span class="line">t_c = torch.tensor(t_c)</span><br><span class="line">t_u = torch.tensor(t_u)</span><br></pre></td></tr></table></figure>
<p>the t_c values are temperatures in Celsius, and the t_u values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings.</p>
<h4 id="visualizing-the-data">5.2.3 <em>Visualizing the data</em></h4>
<p>A quick plot of our data in figure 5.3 tells us that it’s noisy, but we think there’s a pattern here.</p>
<p><img src="9.png" alt="9" style="zoom:50%;" /></p>
<center>
Figure 5.3 Our unknown data just might follow a linear model.
</center>
<h4 id="choosing-a-linear-model-as-a-first-try">5.2.4 <em>Choosing a linear model as a first try</em></h4>
<p>The two may be linearly related—that is, multiplying t_u by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit): <span class="math display">\[
t\_c = w*t\_u+b
\]</span> We chose to name w and b after <em>weight</em> and <em>bias</em>, two very common terms for linear scaling and the additive constant—we’ll bump into those all the time.</p>
<p>now we need to estimate w and b, the parameters in our model, based on the data we have. We’ll go through this simple example using PyTorch and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.</p>
<p>We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the <strong><em>loss function</em></strong>, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding w and b so that the loss function is at a minimum.</p>
<h3 id="less-loss-is-what-we-want">5.3 <em>Less loss is what we want</em></h3>
<p><strong>A <em>loss function</em> (or <em>cost function</em>)</strong> is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples.</p>
<p>In our case, that would be the difference between the predicted temperatures <code>t_p</code> output by our model and the actual measurements: <code>t_p – t_c</code>. We need to make sure the loss function makes the loss positive both when <code>t_p</code> is greater than and when it is less than the true <code>t_c</code>, since the goal is for <code>t_p</code> to match <code>t_c</code>. the most straightforward being <code>|t_p – t_c|</code> and<code>(t_p – t_c)^2</code>. Both of the example loss functions have a clear minimum at zero and grow mono-tonically as the predicted value moves further from the true value in either direction. Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be <strong><em>convex</em></strong>. Since our model is linear, the loss as a function of w and b is also <strong>convex</strong>.</p>
<p><img src="10.png" alt="10" style="zoom:40%;" /></p>
<center>
Figure 5.4 Absolute difference versus difference squared
</center>
<p>we notice that the square of the differences behaves more nicely around the mini- mum: the derivative of the error-squared loss with respect to <code>t_p</code> is zero when <code>t_p</code> equals <code>t_c</code>. It’s worth noting that the square difference also penalizes wildly wrong results more than the absolute difference does. Often, having more slightly wrong results is better than hav- ing a few wildly wrong ones</p>
<h4 id="from-problem-back-to-pytorch">5.3.1 <em>From problem back to PyTorch</em></h4>
<p>We’ve already created our data tensors, so now let’s write out the model as a Python function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">t_u, w, b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w * t_u + b</span><br></pre></td></tr></table></figure>
<p>We’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias parameter, respectively. In our model, the parameters will be PyTorch scalars, and the product operation will use broadcasting to yield the returned tensors.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">t_p, t_c</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ((t_p-t_c)**<span class="number">2</span>).mean()</span><br></pre></td></tr></table></figure>
<p>we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a <strong><em>mean square loss</em>.</strong></p>
<p>We can now initialize the parameters, invoke the model,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = torch.ones()</span><br><span class="line">b = torch.zeros(())</span><br><span class="line">t_p = model(t_u, w, b)</span><br><span class="line">t_p</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">tensor([<span class="number">35.7000</span>, <span class="number">55.9000</span>, <span class="number">58.2000</span>, <span class="number">81.9000</span>, <span class="number">56.3000</span>, <span class="number">48.9000</span>, <span class="number">33.9000</span>,</span><br><span class="line">        <span class="number">21.8000</span>, <span class="number">48.4000</span>, <span class="number">60.4000</span>, <span class="number">68.4000</span>])</span><br><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line">loss = loss_fn(t_p, t_c)</span><br><span class="line">loss</span><br><span class="line"><span class="comment"># Out[6]:</span></span><br><span class="line">tensor(<span class="number">1763.8846</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Broadcasting</strong></p>
<p>In our example, we have two scalars (zero-dimensional tensors) w and b, and we multiply them with and add them to vectors (one-dimensional tensors) of length <em>b</em>.</p>
<p>Usually we can only use element-wise binary operations such as addition, subtraction, multiplication, and division for arguments of the same shape. The entries in matching positions in each of the tensors will be used to calculate the corresponding entry in the result tensor. Broadcasting relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:</p>
<ul>
<li>For each index dimension, counted from the back, if one of the operands is size 1 in that dimension, PyTorch will use the single entry along this dimen- sion with each of the entries in the other tensor along this dimension.</li>
<li>If both sizes are greater than 1, they must be the same, and natural matching is used.</li>
<li>If one of the tensors has more index dimensions than the other, the entirety of the other tensor will be used for each entry along these dimensions.</li>
</ul>
<p>some code examples:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># In[7]:</span></span><br><span class="line">x = torch.ones(())</span><br><span class="line">y = torch.ones(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">z = torch.ones(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">a = torch.ones(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;shapes: x: <span class="subst">&#123;x.shape&#125;</span>, y: <span class="subst">&#123;y.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;        z: <span class="subst">&#123;z.shape&#125;</span>, a: <span class="subst">&#123;a.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x * y:&quot;</span>, (x * y).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y * z:&quot;</span>, (y * z).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y * z * a:&quot;</span>, (y * z * a).shape)</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">shapes: x: torch.Size([]), y: torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">        z: torch.Size([<span class="number">1</span>, <span class="number">3</span>]), a: torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">x * y: torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y * z: torch.Size([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">y * z * a: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</blockquote>
<p>how do we estimate w and b such that the loss reaches a minimum?</p>
<ul>
<li><p>work things out by hand</p></li>
<li><p>use PyTorch’s superpowers to solve the same problem in a more general, off-the-shelf way.</p></li>
</ul>
<h3 id="down-along-the-gradient">5.4 <em>Down along the gradient</em></h3>
<p>We’ll optimize the loss function with respect to the parameters using the <strong><em>gradient descent</em> algorithm.</strong></p>
<h4 id="decreasing-loss">5.4.1 <em>Decreasing loss</em></h4>
<p>we can estimate the rate of change by adding a small number to w and b and seeing how much the loss changes in that neighborhood:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line">delta = <span class="number">0.1</span></span><br><span class="line">loss_rate_of_change_w = \</span><br><span class="line">	(loss_fn(model(t_u, w + delta, b), t_c) -</span><br><span class="line"> 	loss_fn(model(t_u, w - delta, b), t_c)) / (<span class="number">2.0</span> * delta)</span><br></pre></td></tr></table></figure>
<p>This is saying that in the neighborhood of the current values of w and b, a unit increase in w leads to some change in the loss. <strong>If the change is negative, then we need to increase w to minimize the loss, whereas if the change is positive, we need to decrease w.</strong> By how much? Applying a change to w that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramat- ically different at a distance from the neighborhood of the current w value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is <strong>learning_rate</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[9]:</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>():</span></span><br><span class="line">    delta = <span class="number">0.01</span></span><br><span class="line">    loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) -</span><br><span class="line">                             loss_fn(model(t_u, w - delta, b), t_c)) / (<span class="number">2.0</span> * delta)</span><br><span class="line">    loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) -</span><br><span class="line">                             loss_fn(model(t_u, w, b - delta), t_c)) / (<span class="number">2.0</span> * delta)</span><br><span class="line"></span><br><span class="line">    w.storage()[<span class="number">0</span>] = <span class="built_in">float</span>(w - learning_rate * loss_rate_of_change_w)</span><br><span class="line">    b.storage()[<span class="number">0</span>] = <span class="built_in">float</span>(b - learning_rate * loss_rate_of_change_b)</span><br></pre></td></tr></table></figure>
<h4 id="getting-analytical">5.4.2 <em>Getting analytical</em></h4>
<p>Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of w and b doesn’t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose delta equal to 0.1 in the previous section, but it all depends on the shape of the loss as a function of w and b.</p>
<p>In a model with two or more parameters like the one we’re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: <strong>the <em>gradient</em>.</strong></p>
<h5 id="compute-the-derivatives">Compute The Derivatives</h5>
<p>In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter: <span class="math display">\[
d\space loss\_fn / d\ w = (d\ loss\_fn / d\ t\_p) * (d\ t\_p / d\ w)
\]</span> Recall that our model is a linear function, and our loss is a sum of squares. Let’s figure out the expressions for the derivatives. Remembering thatd x^2 / d x = 2 x, we get</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dloss_fn</span>(<span class="params">t_p, t_c</span>):</span></span><br><span class="line">    <span class="comment"># The division is from the derivative of mean.</span></span><br><span class="line">    dsq_diffs = <span class="number">2</span> * (t_p - t_c) / t_p.size(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dsq_diffs</span><br></pre></td></tr></table></figure>
<h5 id="applying-the-derivatives-to-the-model">APPLYING THE DERIVATIVES TO THE MODEL</h5>
<p>For the model, we get these derivatives:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dmodel_dw</span>(<span class="params">t_u, w, b</span>):</span></span><br><span class="line">	<span class="keyword">return</span> t_u	</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dmodel_db</span>(<span class="params">t_u, w, b</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1.0</span>	</span><br></pre></td></tr></table></figure>
<h5 id="defining-the-gradient-function">DEFINING THE GRADIENT FUNCTION</h5>
<p>Putting all of this together, the function returning the gradient of the loss with respect to w and b is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_fn</span>(<span class="params">t_u, t_c, t_p, w, b</span>):</span></span><br><span class="line">    dloss_dtp = dloss_fn(t_p, t_c)</span><br><span class="line">    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)</span><br><span class="line">    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)</span><br><span class="line">    <span class="keyword">return</span> torch.stack([dloss_dw.<span class="built_in">sum</span>(), dloss_db.<span class="built_in">sum</span>()])</span><br></pre></td></tr></table></figure>
<p><img src="11.png" alt="11" style="zoom:50%;" /></p>
<center>
Figure 5.7 The derivative of the loss function with respect to the weights
</center>
<h4 id="iterating-to-fit-the-model">5.4.3 <em>Iterating to fit the model</em></h4>
<p>Starting from a tentative value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing.</p>
<h5 id="the-training-loop">THE TRAINING LOOP</h5>
<p>We call a training iteration during which we update the parameters for all of our training samples an <em>epoch</em>. The complete training loop looks like this :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">n_epochs, learning_rate, params, t_u, t_c</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        w, b = params</span><br><span class="line">        t_p = model(t_u, w, b) </span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">        grad = grad_fn(t_u, t_c, t_p, w, b)</span><br><span class="line">        params = params - learning_rate * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss %f&#x27;</span> % (epoch, <span class="built_in">float</span>(loss)))</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<p>Now, let’s invoke our training loop:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">100</span>,</span><br><span class="line">    learning_rate = <span class="number">1e-4</span>,</span><br><span class="line">    params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]),</span><br><span class="line">    t_u = t_u,</span><br><span class="line">    t_c = t_c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[17]:</span></span><br><span class="line">Epoch <span class="number">1</span>, Loss <span class="number">1763.884644</span></span><br><span class="line">	Params: tensor([ <span class="number">0.5483</span>, -<span class="number">0.0083</span>])</span><br><span class="line">	Grad:   tensor([<span class="number">4517.2969</span>,   <span class="number">82.6000</span>])</span><br><span class="line">Epoch <span class="number">2</span>, Loss <span class="number">323.090546</span></span><br><span class="line">	Params: tensor([ <span class="number">0.3623</span>, -<span class="number">0.0118</span>])</span><br><span class="line">    Grad:   tensor([<span class="number">1859.5493</span>,   <span class="number">35.7843</span>])</span><br><span class="line">Epoch <span class="number">3</span>, Loss <span class="number">78.929634</span></span><br><span class="line">	Params: tensor([ <span class="number">0.2858</span>, -<span class="number">0.0135</span>])</span><br><span class="line">    Grad:   tensor([<span class="number">765.4667</span>,  <span class="number">16.5122</span>])</span><br><span class="line">    ...</span><br><span class="line">Epoch <span class="number">99</span>, Loss <span class="number">29.023582</span></span><br><span class="line">	Params: tensor([ <span class="number">0.2327</span>, -<span class="number">0.0435</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">0.0533</span>,  <span class="number">3.0226</span>])</span><br><span class="line">Epoch <span class="number">100</span>, Loss <span class="number">29.022669</span></span><br><span class="line">	Params: tensor([ <span class="number">0.2327</span>, -<span class="number">0.0438</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">0.0532</span>,  <span class="number">3.0226</span>])</span><br></pre></td></tr></table></figure>
<h4 id="normalizing-inputs">5.4.4 <em>Normalizing inputs</em></h4>
<p>We can see that the first-epoch gradient for the weight is about 50 times larger than the gradient for the bias. This means the weight and bias live in differently scaled spaces. If this is the case, a learning rate that’s large enough to meaningfully update one will be so large as to be unstable for the other; and a rate that’s appropriate for the other won’t be large enough to meaningfully change the first.</p>
<p>There’s a simpler way to keep things in check: changing the inputs so that the gra- dients aren’t quite so different. We can make sure the range of the input doesn’t get too far from the range of –1.0 to 1.0, roughly speaking.</p>
<p>In our case, we can achieve something close enough to that by simply multiplying t_u by 0.1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">t_un = <span class="number">0.1</span> * t_u</span><br><span class="line"></span><br><span class="line">training_loop(</span><br><span class="line">    n_epochs = <span class="number">100</span>,</span><br><span class="line">    learning_rate = <span class="number">1e-2</span>,</span><br><span class="line">    params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]),</span><br><span class="line">    t_u = t_un,</span><br><span class="line">    t_c = t_c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[20]:</span></span><br><span class="line">Epoch <span class="number">1</span>, Loss <span class="number">80.364342</span></span><br><span class="line">    Params: tensor([<span class="number">1.7761</span>, <span class="number">0.1064</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">77.6140</span>, -<span class="number">10.6400</span>])</span><br><span class="line">Epoch <span class="number">2</span>, Loss <span class="number">37.574917</span></span><br><span class="line">    Params: tensor([<span class="number">2.0848</span>, <span class="number">0.1303</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">30.8623</span>,  -<span class="number">2.3864</span>])</span><br><span class="line">Epoch <span class="number">3</span>, Loss <span class="number">30.871077</span></span><br><span class="line">    Params: tensor([<span class="number">2.2094</span>, <span class="number">0.1217</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">12.4631</span>,   <span class="number">0.8587</span>])</span><br><span class="line">...</span><br><span class="line">Epoch <span class="number">99</span>, Loss <span class="number">22.214186</span></span><br><span class="line">    Params: tensor([ <span class="number">2.7508</span>, -<span class="number">2.4910</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">0.4453</span>,  <span class="number">2.5208</span>])</span><br><span class="line">Epoch <span class="number">100</span>, Loss <span class="number">22.148710</span></span><br><span class="line">    Params: tensor([ <span class="number">2.7553</span>, -<span class="number">2.5162</span>])</span><br><span class="line">    Grad:   tensor([-<span class="number">0.4446</span>,  <span class="number">2.5165</span>])</span><br></pre></td></tr></table></figure>
<p>Even though we set our learning rate back to 1e-2, parameters don’t blow up during iterative updates. We could probably do a better job of normalization than a simple rescaling by a factor of 10, but since doing so is good enough for our needs, we’re going to stick with that for now.</p>
<p>Let’s run the loop for enough iterations to see the changes in params get small. We’ll change n_epochs to 5,000:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">params = training_loop(</span><br><span class="line">    n_epochs = <span class="number">5000</span>,</span><br><span class="line">    learning_rate = <span class="number">1e-2</span>,</span><br><span class="line">    params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>]),</span><br><span class="line">    t_u = t_un,</span><br><span class="line">    t_c = t_c,</span><br><span class="line">    print_params = <span class="literal">False</span>)</span><br><span class="line">params</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[21]:</span></span><br><span class="line">Epoch <span class="number">1</span>, Loss <span class="number">80.364342</span></span><br><span class="line">Epoch <span class="number">2</span>, Loss <span class="number">37.574917</span></span><br><span class="line">Epoch <span class="number">3</span>, Loss <span class="number">30.871077</span></span><br><span class="line">...</span><br><span class="line">Epoch <span class="number">10</span>, Loss <span class="number">29.030487</span></span><br><span class="line">Epoch <span class="number">11</span>, Loss <span class="number">28.941875</span></span><br><span class="line">...</span><br><span class="line">Epoch <span class="number">4000</span>, Loss <span class="number">2.927680</span></span><br><span class="line">Epoch <span class="number">5000</span>, Loss <span class="number">2.927648</span></span><br><span class="line">tensor([  <span class="number">5.3671</span>, -<span class="number">17.3012</span>])</span><br></pre></td></tr></table></figure>
<h4 id="visualizing-again">5.4.5 <em>Visualizing (again)</em></h4>
<p><img src="1.gif" alt="1" style="zoom:70%;" /></p>
<h3 id="pytorchs-autograd-backpropagating-all-things">5.5 <em>PyTorch’s autograd: Backpropagating all things</em></h3>
<p>we computed the gradient of a composition of functions—the model and the loss—with respect to their innermost parameters (w and b) by propagating derivatives backward using the <strong><em>chain rule</em>.</strong></p>
<p>Granted, writing the analytical expression for the derivatives of a very deep composition of linear and nonlinear functions is not a lot of fun</p>
<h4 id="computing-the-gradient-automatically">5.5.1 <em>Computing the gradient automatically</em></h4>
<p>This is when PyTorch tensors come to the rescue, with a PyTorch component called <strong><em>autograd</em></strong>. PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs.</p>
<h5 id="applying-autograd">APPLYING AUTOGRAD</h5>
<p>At this point, the best way to proceed is to rewrite our thermometer calibration code, this time using autograd, and see what happens. First, we recall our model and loss function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">t_u, w, b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w * t_u + b</span><br><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span>(<span class="params">t_p, t_c</span>):</span></span><br><span class="line">    squared_diffs = (t_p - t_c)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> squared_diffs.mean()</span><br><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h5 id="using-the-grad-attribute">USING THE GRAD ATTRIBUTE</h5>
<p>Notice the <code>requires_grad=True</code> argument to the tensor constructor? That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that will have params as an ancestor will have access to the chain of functions that were called to get from params to that tensor. the value of the derivative will be automatically populated as a <code>grad</code> attribute of the params tensor.</p>
<p>All we have to do to populate it is to start with a tensor with requires_grad set to True, then call the model and compute the loss, and then call backward on the loss tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = loss_fn(model(t_u, *params), t_c)</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">params.grad</span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">tensor([<span class="number">4517.2969</span>,   <span class="number">82.6000</span>])</span><br></pre></td></tr></table></figure>
<p>At this point, the grad attribute of params contains the derivatives of the loss with respect to each element of params.</p>
<p><strong>PyTorch creates the autograd graph with the operations (in black circles) as nodes</strong>, as shown in the top row of figure 5.10. When we call <code>loss.backward()</code>, PyTorch traverses this graph in the reverse direction to compute the gradients, as shown by the arrows in the bottom row of the figure.</p>
<p><img src="12.png" alt="12" style="zoom:50%;" /></p>
<h5 id="accumulating-grad-functions">ACCUMULATING GRAD FUNCTIONS</h5>
<p>In this case, PyTorch would compute the derivatives of the loss throughout the chain of functions (the computation graph) and <strong>accumulate their values in the grad attribute</strong> of those tensors (the leaf nodes of the graph).</p>
<p>Alert! <em>Big gotcha ahead</em>. This is something PyTorch newcomers—and a lot of more experienced folks, too—trip up on regularly. <strong>We just wrote <em>accumulate</em>, not <em>store</em>.</strong></p>
<blockquote>
<p><strong>WARNING Calling backward will lead derivatives to <em>accumulate</em> at leaf nodes. We need to <em>zero the gradient explicitly</em> after using it for parameter updates.</strong></p>
</blockquote>
<p><strong>So if backward was called earlier, the loss is evaluated again, backward is called again (as in any training loop), and the gradient at each leaf is accumulated (that is, summed) on top of the one computed at the previous iteration, which leads to an incorrect value for the gradient.</strong></p>
<p>In order to prevent this from occurring, we need to <em>zero the gradient explicitly</em> at each iteration. We can do this easily using the in-place <code>zero_ method</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>Having this reminder drilled into our heads, let’s see what our autograd-enabled training code looks like, start to finish:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimizer</span>(<span class="params">param</span>):</span></span><br><span class="line">    gradient = param.grad</span><br><span class="line">    param -= learning_rate * gradient</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">n_epochs, learning_rate, params, t_u, t_c</span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> parameters.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            parameters.grad.zero_()</span><br><span class="line">        t_p = model(t_u, *parameters)</span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            optimizer(parameters)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;n_epochs&#125;</span>  ------------------------------\n&#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;Loss <span class="subst">&#123;loss&#125;</span>       param <span class="subst">&#123;parameters.data&#125;</span>&#x27;</span>)</span><br><span class="line">    draw()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>First, we are encapsulating the update in a <code>no_grad</code> context using the Python with statement. This means within the with block, the PyTorch autograd mechanism should <em>look away</em>: that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that PyTorch records is consumed when we call backward, leaving us with the params leaf node.</li>
<li>Second, we update params in place. This means we keep the same params tensor around but subtract our update from it. When using autograd, we usually avoid inplace updates because <strong>PyTorch’s autograd engine might need the values we would be modifying for the backward pass.</strong></li>
</ul>
<p>The result is the same as we got previously. Good for us! It means that while we are <em>capable</em> of computing derivatives by hand, we no longer need to.</p>
<h4 id="optimizers-a-la-carte">5.5.2 <em>Optimizers a la carte</em></h4>
<p>In the example code, we used <strong><em>vanilla</em> gradient descent</strong> for optimization, which worked fine for our simple case. there are several optimization strategies and tricks that can assist convergence, especially when models get complicated.</p>
<p>The <code>torch</code> module has an <code>optim</code> submodule where we can find classes implementing different optimization algorithms.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span>(torch.optim)</span><br><span class="line">[<span class="string">&#x27;ASGD&#x27;</span>, <span class="string">&#x27;Adadelta&#x27;</span>, <span class="string">&#x27;Adagrad&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>, <span class="string">&#x27;AdamW&#x27;</span>, <span class="string">&#x27;Adamax&#x27;</span>, <span class="string">&#x27;LBFGS&#x27;</span>, <span class="string">&#x27;Optimizer&#x27;</span>, <span class="string">&#x27;RMSprop&#x27;</span>, <span class="string">&#x27;Rprop&#x27;</span>, <span class="string">&#x27;SGD&#x27;</span>, <span class="string">&#x27;SparseAdam&#x27;</span>,...]</span><br></pre></td></tr></table></figure>
<p>Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically with requires_grad set to True) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their val- ues and access their grad attribute, as represented in figure 5.11.</p>
<p><img src="13.png" alt="13" style="zoom:50%;" /></p>
<center>
Figure 5.11 (A) Conceptual representation of how an optimizer holds a reference to parameters. (B) After a loss is computed from inputs, (C) a call to .backward leads to .grad being populated on parameters. (D) At that point, the optimizer can access .grad and compute the parameter updates.
</center>
<p>Each optimizer exposes two methods: <code>zero_grad and step</code>. <code>zero_grad</code> zeroes the grad attribute of all the parameters passed to the optimizer upon construction. <code>step</code> updates the value of those parameters according to the optimization strategy implemented by the specific optimizer.</p>
<h5 id="using-a-gradient-descent-optimizer">USING A GRADIENT DESCENT OPTIMIZER</h5>
<p>Let’s create params and instantiate a gradient descent optimizer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-5</span></span><br><span class="line">optimizer = optim.SGD([params], lr=learning_rate)</span><br></pre></td></tr></table></figure>
<p>Here SGD stands for <strong><em>stochastic gradient descent</em>.</strong> Actually, the optimizer itself is exactly a vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the default). The term <em>stochastic</em> comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a <strong><em>minibatch</em></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">params = torch.tensor([<span class="number">1.0</span>, <span class="number">0.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = optim.SGD([params], lr=learning_rate)</span><br><span class="line"></span><br><span class="line">t_p = model(t_un, *params)</span><br><span class="line">loss = loss_fn(t_p, t_c)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>All we have to do is provide a list of params to it (that list can be extremely long, as is needed for very deep neural network models), and we can forget about the details.</p>
<h5 id="testing-other-optimizers">TESTING OTHER OPTIMIZERS</h5>
<p>In order to test more optimizers, all we have to do is instantiate a different optimizer, say Adam, instead of SGD. The rest of the code stays as it is. Pretty handy stuff.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam([params], lr=learning_rate)</span><br></pre></td></tr></table></figure>
<h4 id="training-validation-and-overfitting">5.5.3 <em>Training, validation, and overfitting</em></h4>
<p>If we had independent data points that we didn’t use to evaluate our loss or descend along its nega- tive gradient, we would soon find out that evaluating the loss at those independent data points would yield higher-than-expected loss. We have already mentioned this phenomenon, called <em>overfitting</em>.</p>
<p>The first action we can take to combat overfitting is recognizing that it might happen. we must take a few data points out of our dataset (the <em>validation set</em>) and only fit our model on the remaining data points (the <em>training set</em>), as shown in figure 5.12. Then, while we’re fitting the model, we can evaluate the loss once on the training set and once on the validation set.</p>
<p><img src="14.png" alt="14" style="zoom:50%;" /></p>
<h5 id="evaluating-the-training-loss">EVALUATING THE TRAINING LOSS</h5>
<p>The training loss will tell us if our model can fit the training set at all—in other words, if our model has enough <em>capacity</em> to process the relevant information in the data. A deep neural network can potentially approximate complicated functions, pro- vided that the number of neurons, and therefore parameters, is high enough. The fewer the number of parameters, the simpler the shape of the function our network will be able to approximate.</p>
<p>rule 1: if the training loss is not decreasing, chances are the model is too simple for the data. The other possibility is that our data just doesn’t contain meaningful information that lets it explain the output</p>
<h5 id="generalizing-to-the-validation-set">GENERALIZING TO THE VALIDATION SET</h5>
<p>if the loss evaluated in the validation set doesn’t decrease along with the training set, it means our model is improving its fit of the samples it is seeing during training, but it is not <em>generalizing</em> to samples outside this precise set.</p>
<p>rule 2: if the training loss and the validation loss diverge, we’re overfitting.</p>
<p>We could have decided to fit the data with a more complicated function, like a piecewise polynomial or a really large neural network. It could generate a model meandering its way through the data points, as in figure 5.13.</p>
<p><img src="15.png" alt="15" style="zoom:50%;" /></p>
<p>What’s the cure, though?</p>
<ul>
<li>make sure we get enough data for the process</li>
<li>make sure the model that is capable of fitting the training data is as regular as possible in between them
<ul>
<li>make our model simpler</li>
<li>adding <strong><em>penalization terms</em></strong> to the loss function, to make it cheaper for the model to behave more smoothly and change more slowly (up to a point).</li>
<li>add noise to the input samples, to artificially create new data points in between training data samples and force the model to try to fit those, too.</li>
</ul></li>
</ul>
<h5 id="splitting-a-dataset">SPLITTING A DATASET</h5>
<p>Shuffling the elements of a tensor amounts to finding a permutation of its indices.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_samples = t_u.shape[<span class="number">0</span>]</span><br><span class="line">n_val = <span class="built_in">int</span>(<span class="number">0.2</span> * n_samples)</span><br><span class="line">shuffled_indices = torch.randperm(n_samples)</span><br><span class="line">train_indices = shuffled_indices[:-n_val]</span><br><span class="line">val_indices = shuffled_indices[-n_val:]</span><br><span class="line">train_indices, val_indices</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[12]:</span></span><br><span class="line"> (tensor([<span class="number">9</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>]), tensor([ <span class="number">2</span>, <span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>We just got index tensors that we can use to build training and validation sets starting from the data tensors:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">train_t_u = t_u[train_indices]</span><br><span class="line">train_t_c = t_c[train_indices]</span><br><span class="line">val_t_u = t_u[val_indices]</span><br><span class="line">val_t_c = t_c[val_indices]</span><br><span class="line">train_t_un = <span class="number">0.1</span> * train_t_u</span><br><span class="line">val_t_un = <span class="number">0.1</span> * val_t_u</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_loop</span>(<span class="params">model, n_epochs, train_t_u, val_t_u, train_t_c, val_t_c</span>):</span></span><br><span class="line">    optimizer = GradientDescend(model.parameters(), learning_rate)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">        train_t_p = model(train_t_u)</span><br><span class="line">        train_loss = loss_fn(train_t_p, train_t_c)</span><br><span class="line"></span><br><span class="line">        val_t_p = model(val_t_u)</span><br><span class="line">        val_loss = loss_fn(val_t_p, val_t_c)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        train_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>  ------------------------------\n&#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;train_loss <span class="subst">&#123;train_loss&#125;</span>   val loss <span class="subst">&#123;val_loss&#125;</span>\n&#x27;</span></span><br><span class="line">                  <span class="string">f&#x27;param <span class="subst">&#123;model.parameters().data&#125;</span>   param grad <span class="subst">&#123;model.parameters().grad&#125;</span>\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">20000</span> == <span class="number">0</span>:</span><br><span class="line">            draw(model.parameters())</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = Model()</span><br><span class="line">    train_loop(model, <span class="number">100000</span>, train_t_u,val_t_u, train_t_c, val_t_c)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line">Epoch <span class="number">10000</span>  ------------------------------</span><br><span class="line">train_loss <span class="number">23.670759201049805</span>   val loss <span class="number">14.046558380126953</span></span><br><span class="line">param tensor([ <span class="number">0.2920</span>, -<span class="number">3.0043</span>])   param grad tensor([-<span class="number">0.0472</span>,  <span class="number">2.7353</span>])</span><br><span class="line">...</span><br><span class="line">Epoch <span class="number">90000</span>  ------------------------------</span><br><span class="line">train_loss <span class="number">3.7082951068878174</span>   val loss <span class="number">3.7602975368499756</span></span><br><span class="line">param tensor([  <span class="number">0.4944</span>, -<span class="number">14.7332</span>])   param grad tensor([-<span class="number">0.0115</span>,  <span class="number">0.6676</span>])</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">100000</span>  ------------------------------</span><br><span class="line">train_loss <span class="number">3.332547426223755</span>   val loss <span class="number">3.565438985824585</span></span><br><span class="line">param tensor([  <span class="number">0.5050</span>, -<span class="number">15.3453</span>])   param grad tensor([-<span class="number">0.0099</span>,  <span class="number">0.5597</span>])</span><br></pre></td></tr></table></figure>
<p>Our main goal is to also see both the training loss <em>and</em> the validation loss decreasing. While ideally both losses would be roughly the same value, as long as the validation loss stays reasonably close to the training loss, we know that our model is continuing to learn generalized things about our data.</p>
<p><img src="16.png" alt="16" style="zoom:40%;" /></p>
<center>
Figure 5.14 Overfitting scenarios when looking at the training (solid line) and validation (dotted line) losses. (A) Training and validation losses do not decrease; the model is not learning due to no information in the data or insufficient capacity of the model. (B) Training loss decreases while validation loss increases: overfitting. (C) Training and validation losses decrease exactly in tandem. Performance may be improved further as the model is not at the limit of overfitting. (D) Training and validation losses have different absolute values but similar trends: overfitting is under control.
</center>
<h4 id="autograd-nits-and-switching-it-off">5.5.4 <em>Autograd nits and switching it off</em></h4>
<p>The model is evaluated twice—once on train_t_u and once on val_t_u—and then backward is called. Won’t this confuse autograd?</p>
<p>Then train_loss is evaluated from train_t_p. This creates a computation graph that links train_t_u to train_t_p to train_loss. When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this case, a separate computation graph will be created that links val_t_u to val_t_p to val_loss. Separate tensors have been run through the same functions, model and loss_fn, generating separate computation graphs, as shown in figure 5.15.</p>
<p><img src="17.png" alt="17" style="zoom:50%;" /></p>
<center>
Figure 5.15 Diagram showing how gradients propagate through a graph with two losses when .backward is called on one of them
</center>
<p>The only tensors these two graphs have in common are the parameters. When we call backward on train_loss, we run backward on the first graph. calling backward on val_loss would lead to gradients accu- mulating in the params tensor, on top of those generated during the <code>train_loss.backward()</code> call. In this case, we would effectively train our model on the whole dataset (both training and validation)</p>
<p>However optimized, building the autograd graph comes with additional costs that we could totally forgo during the validation pass, especially when the model has millions of parameters. In order to address this, PyTorch allows us to switch off autograd when we don’t need it, using the <code>torch.no_grad</code> context manager.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_t_p = model(val_t_u)</span><br><span class="line">    val_loss = loss_fn(val_t_p, val_t_c)</span><br><span class="line">    <span class="keyword">assert</span> val_loss.required_grad <span class="keyword">is</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>Using the related set_grad_enabled context, we can also condition the code to run with autograd enabled or disabled. We could, for instance, define a <code>calc_forward</code> function that takes data as input and runs model and loss_fn with or without autograd according to a Boolean train_is argument:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_forward</span>(<span class="params">t_u, t_c, is_train</span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.set_grad_enabled(is_train):</span><br><span class="line">        t_p = model(t_u)</span><br><span class="line">        loss = loss_fn(t_p, t_c)</span><br><span class="line">	<span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h3 id="conclusion">5.6 Conclusion</h3>
<p>We started this chapter with a big question: how is it that a machine can learn from examples? We spent the rest of the chapter describing the mechanism with which a model can be optimized to fit data. We chose to stick with a simple model in order to see all the moving parts without unneeded complications.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/25/ML8-2/" rel="prev" title="机器学习 by 李宏毅(8-2)">
                  <i class="fa fa-chevron-left"></i> 机器学习 by 李宏毅(8-2)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/31/ML9/" rel="next" title="机器学习 by 李宏毅(9)">
                  机器学习 by 李宏毅(9) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuYang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">943k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">14:17</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.1.0/dist/quicklink.umd.js"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{&quot;enable&quot;:true,&quot;home&quot;:true,&quot;archive&quot;:true,&quot;delay&quot;:true,&quot;timeout&quot;:3000,&quot;priority&quot;:true,&quot;ignores&quot;:null,&quot;url&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;29&#x2F;core-pytorch2&#x2F;&quot;}</script>
  <script src="/js/third-party/quicklink.js"></script>



  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script async src="/js/cursor/explosion.js"></script>

</body>
</html>
