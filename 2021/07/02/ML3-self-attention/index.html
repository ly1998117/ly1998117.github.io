<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ly1998117.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:true,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="Self-Attention 常见的Network 架构 - Self-attention">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习 by 李宏毅(4)">
<meta property="og:url" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/index.html">
<meta property="og:site_name" content="LiuYang&#39;s Blog">
<meta property="og:description" content="Self-Attention 常见的Network 架构 - Self-attention">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/1.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/2.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/3.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/4.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/5.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/6.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/7.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/8.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/9.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/10.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/11.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/12.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/13.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/14.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/15.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/16.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/17.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/18.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/19.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/20.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/21.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/22.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/23.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/25.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/26.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/27.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/28.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/29.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/30.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/31.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/32.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/33.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/34.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/35.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/36.png">
<meta property="article:published_time" content="2021-07-02T07:02:19.000Z">
<meta property="article:modified_time" content="2021-07-04T14:07:44.208Z">
<meta property="article:author" content="LiuYang">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ly1998117.github.io/2021/07/02/ML3-self-attention/1.png">


<link rel="canonical" href="https://ly1998117.github.io/2021/07/02/ML3-self-attention/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;02&#x2F;ML3-self-attention&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;07&#x2F;02&#x2F;ML3-self-attention&#x2F;&quot;,&quot;title&quot;:&quot;机器学习 by 李宏毅(4)&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>机器学习 by 李宏毅(4) | LiuYang's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiuYang's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">26</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#self-attention"><span class="nav-number">1.</span> <span class="nav-text">Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sophisticated-input"><span class="nav-number">1.1.</span> <span class="nav-text">Sophisticated Input</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#vector-set-as-input"><span class="nav-number">1.1.1.</span> <span class="nav-text">Vector Set as Input</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#output"><span class="nav-number">1.2.</span> <span class="nav-text">output</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sequence-labeling"><span class="nav-number">1.2.1.</span> <span class="nav-text">Sequence Labeling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multi-head-self-attention"><span class="nav-number">1.3.</span> <span class="nav-text">Multi-head Self-attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positional-encoding"><span class="nav-number">1.4.</span> <span class="nav-text">Positional Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#each-positon-has-a-unique-positional-vector"><span class="nav-number">1.4.1.</span> <span class="nav-text">Each positon has a unique positional vector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-for-speech"><span class="nav-number">1.4.2.</span> <span class="nav-text">Self-attention for Speech</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-for-image"><span class="nav-number">1.4.3.</span> <span class="nav-text">Self-attention for Image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-v.s.-cnn"><span class="nav-number">1.4.4.</span> <span class="nav-text">Self-attention v.s. CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-v.s.-rnn"><span class="nav-number">1.4.5.</span> <span class="nav-text">Self-attention v.s. RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-for-graph"><span class="nav-number">1.4.6.</span> <span class="nav-text">Self-attention for Graph</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiuYang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LiuYang</p>
  <div class="site-description" itemprop="description">人与人的悲欢并不相通</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.macwk.com/" title="https:www.macwk.com&#x2F;" rel="noopener" target="_blank">Macwk</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ly1998117.github.io/2021/07/02/ML3-self-attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="LiuYang">
      <meta itemprop="description" content="人与人的悲欢并不相通">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuYang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习 by 李宏毅(4)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-02 15:02:19" itemprop="dateCreated datePublished" datetime="2021-07-02T15:02:19+08:00">2021-07-02</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-07-04 22:07:44" itemprop="dateModified" datetime="2021-07-04T22:07:44+08:00">2021-07-04</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="self-attention">Self-Attention</h1>
<p>常见的Network 架构 - <strong>Self-attention</strong></p>
<span id="more"></span>
<h2 id="sophisticated-input">Sophisticated Input</h2>
<p>输入是多个向量，向量的数目会发生改变</p>
<p><img src="1.png" alt="1" style="zoom:80%;" /></p>
<h3 id="vector-set-as-input">Vector Set as Input</h3>
<ol type="1">
<li>若Network输入是一个句子，每一个句子的长度都不一样,每个句子裡面词汇的数目都不一样。如果把一个句子裡面的每一个词汇, 都描述成一个向量, 那<strong>Model的输入是一个Vector Set</strong>, 而且每次句子的长度不一样, Vector Set的大小就不一样</li>
</ol>
<ul>
<li><strong>One-Hot Encoding 把词汇表示为向量</strong>，但这种表示方法有一个非常严重的问题,它假设所有的**词汇彼此之间都是没有关系的,从这个向量裡面你看不到：Cat跟Dog都是动物所以他们比较接近。这个向量中,没有任何语义的信息</li>
</ul>
<p><img src="2.png" alt="1" style="zoom:80%;" /></p>
<ul>
<li>Word Embedding：给每一个词汇一个向量,而这个<strong>向量具有语义信息</strong>，相似的类别聚集在一起</li>
</ul>
<p><img src="3.png" alt="1" style="zoom:100%;" /></p>
<ol start="2" type="1">
<li>声音信号，一段声音信号其实是一排向量, 把一段声音信号取一个范围, 这个范围叫做一个<strong>Window</strong></li>
</ol>
<p><img src="4.png" alt="1" style="zoom:50%;" /></p>
<p>Window 里的信号描述为一个向量，称为 <strong>Frame</strong>，通常Window 的长度是25ms，一整段的信号需要 Window 向右滑动一般10ms，一秒的声音信号有100个 Frame，一分钟则有6000个Frame</p>
<ol start="3" type="1">
<li>Graph，is a set of vectors。在Social Network上每一个节点就是一个人<strong>,然后</strong>节点跟节点之间的edge就是他们两个的关系连接，每个节点可以看做一个向量（性别，年龄，工作等等）</li>
</ol>
<p><img src="5.png" alt="1" style="zoom:30%;" /></p>
<ol start="4" type="1">
<li>分子信息，可以看做一个Graph。<strong>一个分子可以看作是一个Graph</strong>,分子上面的每一个原子，可以表述成一个向量<strong>，一个</strong>原子可以用One-Hot Vector</li>
</ol>
<p><img src="6.png" alt="1" style="zoom:60%;" /></p>
<h2 id="output">output</h2>
<ul>
<li>Each vector has a label，输入多少向量，输出多少对应label</li>
</ul>
<p><img src="7.png" alt="1" style="zoom:40%;" /></p>
<p>Application：POS tagging（词性标注）、Phonetic tagging（拼音标注）、商品推荐</p>
<p><img src="8.png" alt="1" style="zoom:40%;" /></p>
<ul>
<li>The whole sequence has a label</li>
</ul>
<p>Application：Sentiment analysis（情绪分析），语者辨认、分子性质分析</p>
<p><img src="9.png" alt="1" style="zoom:40%;" /></p>
<ul>
<li>Model decides the number of labels itself（又sequence to sequence）</li>
</ul>
<p><img src="10.png" alt="1" style="zoom:40%;" /></p>
<p>Application：机器翻译、语音识别</p>
<h3 id="sequence-labeling">Sequence Labeling</h3>
<p>输入跟输出数目一样多的情况又叫做 <strong>Sequence Labeling</strong>，模型如何设计？</p>
<ol type="1">
<li>Fully-Connected Network</li>
</ol>
<p><img src="11.png" alt="1" style="zoom:50%;" /></p>
<p>问题：如词性标注，单词和单词之间存在关系，同一个单词不同位置具有不同的词性，需要考虑上下文，把前后几个向量都作为一个neuron的输入</p>
<p><img src="12.png" alt="1" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li>Self-attention</li>
</ol>
<p><img src="13.png" alt="1" style="zoom:60%;" /></p>
<p><strong>Self-Attention会考虑整个Sequence的信息，Input几个Vector 就输出几个Vector，输出的 Vector 都是考虑一整个Sequence以后才得到的</strong></p>
<p><strong>Self-Attention不是只能用一次, 可以叠加很多次</strong></p>
<p><img src="14.png" alt="1" style="zoom:60%;" /></p>
<p><strong>可以把Fully-Connected的Network,跟Self-Attention交替使用</strong></p>
<ul>
<li>Self-Attention处理整个Sequence的信息</li>
<li>Fully-Connected的Network, 专注于处理某一个位置的信息</li>
<li>再用Self-Attention,再把整个 Sequence 信息再处理一次</li>
<li>然后交替使用Self-Attention跟Fully-Connected</li>
</ul>
<h4 id="实现">实现</h4>
<p><strong>Self-Attention的Input, 是一串的Vector, Vector可能是整个Network的 Input, 也可能是某个Hidden Layer的 Output，所以用a来表示它，每一个b都是考虑了所有的a以后才生成出来的</strong></p>
<p><img src="15.png" alt="1" style="zoom:60%;" /></p>
<p><span class="math inline">\(b_1\)</span>产生步骤：</p>
<ul>
<li>根据<span class="math inline">\(a_1\)</span>找到与之想关联的向量，每一个向量跟<span class="math inline">\(a_1\)</span>的关联的程度,用一个数值α来表示</li>
</ul>
<p><img src="16.png" alt="1" style="zoom:60%;" /></p>
<p><strong>计算attention的模组可以自动计算两个向量之间的关联性<span class="math inline">\(\alpha\)</span>: Dot-product or Addictive</strong></p>
<p><img src="17.png" alt="1" style="zoom:60%;" /></p>
<p>Dot-product: <strong>输入的这两个向量分别乘上两个不同的矩阵，再把 q 跟 k 做dot product, 做 element-wise 的相乘, 再全部加起来以后就得到一个 scalar, 这个scalar就是α</strong>，最常用的方法,也<strong>是用在Transformer里面的方法</strong></p>
<p><img src="18.png" alt="1" style="zoom:60%;" /></p>
<p>q有一个名字叫Query，k叫做 Key，<strong>Query q1,跟Key k2, 计算 Inner-Product 就得到<span class="math inline">\(\alpha_{1,2}\)</span>，表示Query是<span class="math inline">\(a_1\)</span>提供，key是<span class="math inline">\(a_2\)</span>提供</strong>，一般实际中也会计算<span class="math inline">\(a_1\)</span>和自己的关联度</p>
<ul>
<li>计算出<span class="math inline">\(a_1\)</span>跟每一个向量的关联性以后, 接入一个Soft-Max(<strong>用别的 Activation Function 替代也没问题</strong>)</li>
</ul>
<p><img src="19.png" alt="1" style="zoom:60%;" /></p>
<ul>
<li>得到这个 <span class="math inline">\(\alpha^{&#39;}\)</span> 以后,我们就要根据 <span class="math inline">\(\alpha^{&#39;}\)</span> 抽取出 Sequence 里重要的信息
<ul>
<li>首先把<span class="math inline">\(a_1\)</span>到<span class="math inline">\(a_4\)</span>这边每一个向量,乘上<span class="math inline">\(W^v\)</span>得到新的向量<span class="math inline">\(v_1\)</span>到<span class="math inline">\(v_4\)</span></li>
<li>接下来把这边的<span class="math inline">\(v_1\)</span>到<span class="math inline">\(v_4\)</span>, 每一个向量都乘上attention的分数<span class="math inline">\(\alpha^{&#39;}\)</span></li>
<li>把4个结果相加得到<span class="math inline">\(b^1\)</span></li>
</ul></li>
</ul>
<p><img src="20.png" alt="1" style="zoom:60%;" /></p>
<p>如果某一个向量它得到的分数越高, 比如如果<span class="math inline">\(a^1\)</span>跟<span class="math inline">\(a^2\)</span>的关联性很强, 得到的<span class="math inline">\(\alpha^{&#39;}_{1,2}\)</span>值很大,那我们今天在做 Weighted Sum 以后,得到的值,就可能会比较接近<span class="math inline">\(v^2\)</span></p>
<p>矩阵角度</p>
<ul>
<li>每个a向量都要计算 q k v，因此可以把a向量拼接为矩阵I，q向量拼接为矩阵Q，同理得到矩阵K，V，其中<span class="math inline">\(W^q,W^k,W^v\)</span>是Network的参数，需要learn</li>
</ul>
<p><img src="21.png" alt="1" style="zoom:80%;" /></p>
<ul>
<li>计算 <span class="math inline">\(attention\ score = inner\ product（k^T, q）\)</span></li>
</ul>
<p><img src="22.png" alt="1" style="zoom:80%;" /></p>
<p>拼接成矩阵后</p>
<img src="23.png" alt="1" style="zoom:80%;" /> $$ A^{'}=softmax(A)=softmax
<span class="math display">\[\begin{pmatrix}
 \alpha_{1,1} &amp;\alpha_{2,1}   &amp;\alpha_{3,1} &amp;\alpha_{4,1} \\
  \alpha_{1,2} &amp; \alpha_{2,2}  &amp;\alpha_{3,2}  &amp;\alpha_{4,2}\\
  \alpha_{1,3} &amp;\alpha_{2,3}   &amp;\alpha_{3,3}  &amp;\alpha_{4,3}\\
\alpha_{1,4} &amp;\alpha_{2,4}   &amp;\alpha_{3,4}  &amp;\alpha_{4,4}\\
\end{pmatrix}\]</span>
= softmax
<span class="math display">\[\begin{pmatrix}
 {k^1}^T\\
 {k^2}^T\\
 {k^3}^T\\
 {k^4}^T\\

\end{pmatrix}
\begin{pmatrix}
 q^1 q^2 q^3 q^4\\
\end{pmatrix}\]</span>
<p>= softmax(K^T Q) $$</p>
<p><span class="math display">\[
attention\ score = softmax(K^TQ)
\]</span></p>
<ul>
<li>计算<span class="math inline">\(b^1,b^2,b^3,b^4\)</span></li>
</ul>
$$ b^1 = <em>{1,1}v^1+</em>{1,2}v<sup>2+<em>{1,3}v^3+</em>{1,4}v</sup>4=(v<sup>1v</sup>2v<sup>3v</sup>4)
<span class="math display">\[\begin{pmatrix}
\alpha_{1,1}\\
\alpha_{1,2}\\
\alpha_{1,2}\\
\alpha_{1,3}
\end{pmatrix}\]</span>
<p>\</p>
O = (b<sup>1b</sup>2b<sup>3b</sup>4)=(v<sup>1v</sup>2v<sup>3v</sup>4)   softmax
<span class="math display">\[\begin{pmatrix}
 \alpha_{1,1} &amp;\alpha_{2,1}   &amp;\alpha_{3,1} &amp;\alpha_{4,1} \\
  \alpha_{1,2} &amp; \alpha_{2,2}  &amp;\alpha_{3,2}  &amp;\alpha_{4,2}\\
  \alpha_{1,3} &amp;\alpha_{2,3}   &amp;\alpha_{3,3}  &amp;\alpha_{4,3}\\
\alpha_{1,4} &amp;\alpha_{2,4}   &amp;\alpha_{3,4}  &amp;\alpha_{4,4}\\
\end{pmatrix}\]</span>
<p>= V A<sup>{'}=Vsoftmax(K</sup>TQ) $$</p>
<p><img src="25.png" alt="1" style="zoom:50%;" /></p>
<ul>
<li>需要learn的参数<span class="math inline">\((W^K,W^q,W^v)\)</span></li>
</ul>
<h2 id="multi-head-self-attention">Multi-head Self-attention</h2>
<p>Self-attention 有一个进阶的版本,叫做 <strong>Multi-head Self-attention</strong></p>
<p><img src="26.png" alt="1" style="zoom:70%;" /></p>
<p>Self-attention 的时候, 用 q 去找相关的 k, 但是<strong>相关这件事情有很多种不同的形式</strong>, 有很多种不同的定义, 所以应该要有多个 q, <strong>不同的 q 负责不同种类的相关性</strong></p>
<p>在Self-attention 的基础上得到的q，再乘另外两个不同的矩阵，得到<span class="math inline">\(q^1 \  q^2\)</span>，同理得到 <span class="math inline">\(k^1,k^2,v^1,v^2\)</span></p>
<p><img src="27.png" alt="1" style="zoom:60%;" /></p>
<p><img src="28.png" alt="1" style="zoom:60%;" /></p>
<p>然后把<span class="math inline">\(b^{i,1}\)</span>跟<span class="math inline">\(b^{i,2}\)</span>接起来,然后再通过一个 transform 得到<span class="math inline">\(b^i\)</span></p>
<p><img src="29.png" alt="1" style="zoom:60%;" /></p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Self-attention 机制产生的<span class="math inline">\(b^1到b^4\)</span>操作完全一样，向量<span class="math inline">\(a^1到a^4\)</span>对于self-attention 而言次序的信息被忽略，而次序信息在特定的情况下十分重要，比如 POS tagging, 语音识别等。</p>
<p>所以在self-attention 中加入位置信息，就是通过 position encoding 技术</p>
<h3 id="each-positon-has-a-unique-positional-vector">Each positon has a unique positional vector</h3>
<p>为每个位置设置 positional vector <span class="math inline">\(e^i\)</span>，i 代表不同的位置，然后加到<span class="math inline">\(a^i\)</span>上，每个位置的e是唯一的</p>
<p><img src="30.png" alt="1" style="zoom:80%;" /></p>
<h3 id="self-attention-for-speech">Self-attention for Speech</h3>
<p>把一段声音讯号,表示成一组向量的话可能会非常地长，vector 的 sequence 它的长度是非常可观会造成 attention score 的计算量十分大，所以需要 <strong>Truncated Self-attention</strong></p>
<p><img src="31.png" alt="1" style="zoom:80%;" /></p>
<p>Truncated Self-attention 做的事情就是, 在 Self-attention 的时候,<strong>不看一整句话, 只看一个小的范围就好</strong>，这个范围是一个超参数</p>
<h3 id="self-attention-for-image">Self-attention for Image</h3>
<p><strong>一张图片,可以换一个观点, 把它看作是一个 vector 的 set，如图所示的图像</strong></p>
<p><img src="32.png" alt="1" style="zoom:80%;" /></p>
<p>把每一个位置的 pixel,看作是一个三维的向量, 那<strong>整张图片,其实就是 5 乘以 10 个向量的set</strong></p>
<p><img src="33.png" alt="1" style="zoom:100%;" /></p>
<h3 id="self-attention-v.s.-cnn">Self-attention v.s. CNN</h3>
<p>用 Self-attention 来处理一张图片, 假设一个 pixel 产生 query, 其他 pixel 產生 key，做 inner product 的时候, 考虑的不是一个小的receptive field的信息, 而是整张图像的信息，而CNN的 filter 只考虑Receptive field 内的信息</p>
<ul>
<li>CNN 相当于简化的 Self-attention</li>
<li>CNN中的 Receptive Field 是一个超参数，self-attention 则是自动学习出来的</li>
<li>CNN是self-attention的特例</li>
</ul>
<p><img src="34.png" alt="1" style="zoom:70%;" /></p>
<p>这个实验结果,来自 An image is worth 16 X16 words, 把图像拆成 16 乘以 16 个 patch, 把每一个 patch想像成是一个 word,</p>
<p><strong>随著数据量越来越多, Self-attention 的结果越来越好, 最终在数据量最多的时候, Self-attention 超过 CNN, 但在数据量少的时候, CNN 它是可以比 Self-attention,得到更好的结果</strong></p>
<h3 id="self-attention-v.s.-rnn">Self-attention v.s. RNN</h3>
<p>RNN（recurrent neural network）</p>
<p><img src="35.png" alt="1" style="zoom:70%;" /></p>
<p>第二个 vector 作为 input 的时候, 也会把前一个时间点的output, 当做下一个时间点的输入,再输入 RNN 裡面,然后再产生新的 vector, 再输入 fully connected network</p>
<p>区别：</p>
<ul>
<li>对 RNN 来说,假设最右边这个黄色的 vector, 要考虑最左边的这个输入, 那它必须要把最左边的输入存在 memory 里面, 一直保存到最右边, 才能够在最后一个时间点被考虑。但对 Self-attention 来说没有这个问题, 它只要输出一个 query, 输出一个 key, 可以从整个 sequence 上非常远的 vector, 轻易地抽取信息</li>
<li>RNN 在处理的时候, input 一组 sequence, output 一组 sequence 的时候, <strong>RNN 没有办法平行化处理</strong>，self-attention 可以并行化处理</li>
<li>在运算速度上,Self-attention 会比 RNN 更有效率。目前很多的应用都往往把 RNN 的架构,逐渐改成 Self-attention 的架构</li>
</ul>
<h3 id="self-attention-for-graph">Self-attention for Graph</h3>
<p><img src="36.png" alt="1" style="zoom:70%;" /></p>
<p>之前做 Self-attention 的时候, 所谓的关联性是 network 自己找出来, 但是现在既然有了 Graph 的 edge 的信息, edge 已经暗示node 跟 node 之间的关联性</p>
<p>所以 Self-attention 用在 Graph 上面的时候, 在做这个 Attention Matrix 计算的时候,你可以<strong>只计算有 edge 相连的 node ，也就是有关系的向量<span class="math inline">\(a\)</span>，无关的node之间的attention score 设为0</strong></p>
<p>Self-attention 最早用在 Transformer 上面, 所以很多人讲 Transformer 的时候, 指的就是 Self-attention, 所以后来各式各样的 Self-attention 的变形都叫xxformer, 比如 Linformer Performer Reformer 等等。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/01/ML3-CNN/" rel="prev" title="机器学习 by 李宏毅(3)">
                  <i class="fa fa-chevron-left"></i> 机器学习 by 李宏毅(3)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/04/RNN/" rel="next" title="Recurrent Neural Network">
                  Recurrent Neural Network <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuYang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">943k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">14:17</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.1.0/dist/quicklink.umd.js"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{&quot;enable&quot;:true,&quot;home&quot;:true,&quot;archive&quot;:true,&quot;delay&quot;:true,&quot;timeout&quot;:3000,&quot;priority&quot;:true,&quot;ignores&quot;:null,&quot;url&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;02&#x2F;ML3-self-attention&#x2F;&quot;}</script>
  <script src="/js/third-party/quicklink.js"></script>



  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script async src="/js/cursor/explosion.js"></script>

</body>
</html>
