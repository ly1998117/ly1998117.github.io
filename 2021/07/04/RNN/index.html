<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ly1998117.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:true,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="Introduction  Rich semantic representations of text, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but i">
<meta property="og:type" content="article">
<meta property="og:title" content="Recurrent Neural Network">
<meta property="og:url" content="https://ly1998117.github.io/2021/07/04/RNN/index.html">
<meta property="og:site_name" content="LiuYang&#39;s Blog">
<meta property="og:description" content="Introduction  Rich semantic representations of text, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but i">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/1.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/RNN-unrolled.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/2.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/3.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/15.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/LSTM3-chain.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/4.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/7.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/5.jpg">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/6.jpg">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/8.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/10.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/9.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/11.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/12.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/13.gif">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/14.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/04/RNN/16.png">
<meta property="article:published_time" content="2021-07-04T11:51:55.000Z">
<meta property="article:modified_time" content="2021-07-05T10:32:57.384Z">
<meta property="article:author" content="LiuYang">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="python">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ly1998117.github.io/2021/07/04/RNN/1.png">


<link rel="canonical" href="https://ly1998117.github.io/2021/07/04/RNN/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;04&#x2F;RNN&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;07&#x2F;04&#x2F;RNN&#x2F;&quot;,&quot;title&quot;:&quot;Recurrent Neural Network&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Recurrent Neural Network | LiuYang's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiuYang's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">24</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">11</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#long-short-term-memory-lstm"><span class="nav-number">2.</span> <span class="nav-text">Long Short Term Memory (LSTM)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#packed-sequences"><span class="nav-number">3.</span> <span class="nav-text">Packed sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bidirectional-and-multilayer-rnns"><span class="nav-number">4.</span> <span class="nav-text">Bidirectional and multilayer RNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#generative-networks"><span class="nav-number">5.</span> <span class="nav-text">Generative networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#building-character-vocabulary"><span class="nav-number">5.1.</span> <span class="nav-text">Building character vocabulary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training-a-generative-rnn"><span class="nav-number">6.</span> <span class="nav-text">Training a generative RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#soft-text-generation-and-temperature"><span class="nav-number">6.1.</span> <span class="nav-text">Soft text generation and temperature</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention-mechanisms-and-transformers"><span class="nav-number">7.</span> <span class="nav-text">Attention mechanisms and transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer-models"><span class="nav-number">7.1.</span> <span class="nav-text">Transformer models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#using-bert-for-text-classification"><span class="nav-number">8.</span> <span class="nav-text">Using BERT for text classification</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiuYang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LiuYang</p>
  <div class="site-description" itemprop="description">人与人的悲欢并不相通</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.macwk.com/" title="https:www.macwk.com&#x2F;" rel="noopener" target="_blank">Macwk</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ly1998117.github.io/2021/07/04/RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="LiuYang">
      <meta itemprop="description" content="人与人的悲欢并不相通">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuYang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Recurrent Neural Network
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-04 19:51:55" itemprop="dateCreated datePublished" datetime="2021-07-04T19:51:55+08:00">2021-07-04</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-07-05 18:32:57" itemprop="dateModified" datetime="2021-07-05T18:32:57+08:00">2021-07-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>27k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="introduction">Introduction</h2>
<blockquote>
<p>Rich semantic representations of text, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it does not take into account the <strong>order</strong> of words, because aggregation operation on top of embeddings removed this information from the original text.</p>
<span id="more"></span>
<p>Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.</p>
<p>To capture the meaning of text sequence, we need to use another neural network architecture, which is called a <strong>recurrent neural network</strong>, or RNN. In RNN, we pass our sentence through the network one symbol at a time, and the network produces some <strong>state</strong>, which we then pass to the network again with the next symbol.</p>
</blockquote>
<p><img src="1.png" alt="1" style="zoom:80%;" /></p>
<center>
Recurrent Neural Networks have loops.
</center>
<blockquote>
<p>Given the input sequence of tokens <span class="math inline">\(X_0,\dots,X_n\)</span>, RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using back propagation.</p>
</blockquote>
<p><img src="RNN-unrolled.png" alt="RNN-unrolled" style="zoom:25%;" /></p>
<center>
An unrolled recurrent neural network.
</center>
<p>A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Each network block takes a pair <strong><span class="math inline">\((X_i,S_i)\)</span> as an input</strong>, and produces <strong><span class="math inline">\(S_{i+1}\)</span> as a result.</strong> <span class="math display">\[
S_{i+1}= f(X_i,S_i)
\]</span> Final state <span class="math inline">\(S_n\)</span> or output <span class="math inline">\(X_n\)</span> goes into a linear classifier to produce the result. All network blocks share the same weights, and are trained end-to-end using one backpropagation pass.</p>
<blockquote>
<p>Because state vectors <span class="math inline">\(S_0,\dots,S_n\)</span> are passed through the network, it is able to learn the sequential dependencies between words. For example, when the word <strong>not</strong> appears somewhere in the sequence, it can learn to negate certain elements within the state vector, resulting in negation.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, hidden_dim, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim)</span><br><span class="line">        self.fc = nn.Linear(in_features=hidden_dim, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x, h = self.rnn(x)</span><br><span class="line">        x = torch.mean(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    load_data(<span class="string">&#x27;embedding&#x27;</span>)</span><br><span class="line">    net = MyRNN(<span class="number">64</span>, <span class="number">32</span>, <span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">    hist = train(net, training_dataloader, valid_dataloader)</span><br><span class="line">    plot_acc_loss(hist)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="2.png" alt="1" style="zoom:50%;" /></p>
<blockquote>
<p>In our case, we will use padded data loader, so each batch will have a number of padded sequences of the same length. RNN layer will take the sequence of embedding tensors, and produce two outputs:</p>
</blockquote>
<ul>
<li><strong>x is a sequence of RNN cell outputs at each step</strong></li>
<li><strong>h is a final hidden state for the last element of the sequence</strong></li>
</ul>
<p>We then apply a fully-connected linear classifier to get the number of class.</p>
<blockquote>
<p><strong>Note:</strong> RNNs are quite difficult to train, because once the RNN cells are unrolled along the sequence length, the resulting number of layers involved in back propagation is quite large. Thus we need to select small learning rate, and train the network on larger dataset to produce good results. It can take quite a long time, so using GPU is preferred.</p>
</blockquote>
<h2 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>
<p>经典 RNN 的主要问题之一是所谓的梯度消失问题。RNNs在一次反向传播过程中进行端到端的训练，它很难将错误传播到网络的第一层，因此网络无法学习遥远的 token 之间的关系。</p>
<p>避免这个问题的方法之一是通过使用所谓的 <strong>gates</strong> 来引入明确的状态管理。有两种最著名的此类架构。长短期记忆（LSTM）和<strong>Gated Relay Unit</strong>（GRU）。</p>
<p><img src="3.png" alt="1" style="zoom:40%;" /></p>
<p><img src="15.png" alt="15" style="zoom:50%;" /></p>
<blockquote>
<p>At each unit, hidden vector <span class="math inline">\(h_i\)</span> is concatenated with input <span class="math inline">\(x_i\)</span>, and they control what happens to the state <span class="math inline">\(c\)</span> via <strong>gates</strong>. Each gate is a <strong>neural network with sigmoid activation</strong> (output in the range <span class="math inline">\([0,1]\)</span>), which can be thought of as bitwise mask when multiplied by the state vector. There are the following gates (from left to right on the picture above):</p>
<p><strong>forget gate</strong> takes hidden vector and determines, which components of the vector <span class="math inline">\(c\)</span> we need to forget, and which to pass through.</p>
<p><strong>input gate</strong> takes some information from the input and hidden vector, and inserts it into state.</p>
<p><strong>output gate</strong> transforms state via some linear layer with <span class="math inline">\(\tanh\)</span> activation, then selects some of its components using hidden vector <span class="math inline">\(h_i\)</span> to produce new state <span class="math inline">\(c_{i+1}\)</span>.</p>
</blockquote>
<p><img src="LSTM3-chain.png" alt="LSTM3-chain" style="zoom:30%;" /></p>
<center>
LSTMs also have this chain like structure, but the repeating module has a different structure
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMClassifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, hidden_dim, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTMClassifier, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) - <span class="number">.5</span></span><br><span class="line">        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(in_features=hidden_dim, out_features=classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x, (h, c) = self.rnn(x)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h[-<span class="number">1</span>])</span><br><span class="line">      </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    load_data(<span class="string">&#x27;embedding&#x27;</span>)</span><br><span class="line">    net = LSTMClassifier(<span class="number">64</span>, <span class="number">32</span>, <span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">    summary(net)</span><br><span class="line">    hist = train(net, training_dataloader, valid_dataloader, print_step=<span class="number">1000</span>)</span><br><span class="line">    plot_acc_loss(hist)</span><br></pre></td></tr></table></figure>
<p><img src="4.png" alt="LSTM3-chain" style="zoom:50%;" /></p>
<h2 id="packed-sequences">Packed sequences</h2>
<blockquote>
<p>In our example, we had to pad all sequences in the minibatch with zero vectors. While it results in some memory waste, with RNNs it is more critical that additional RNN cells are created for the padded input items, which take part in training, yet do not carry any important input information. It would be much better to train RNN only to the actual sequence size.</p>
<p>To do that, a special format of padded sequence storage is introduced in PyTorch. Suppose we have input padded minibatch which looks like this:</p>
</blockquote>
<p><span class="math display">\[
[[1,2,3,4,5], \\
 [6,7,8,0,0],\\
 [9,0,0,0,0]]
\]</span></p>
<blockquote>
<p>Here 0 represents padded values, and the actual length vector of input sequences is <code>[5,3,1]</code>.</p>
<p>In order to effectively train RNN with padded sequence, we want to begin training first group of RNN cells with large minibatch (<code>[1,6,9]</code>), but then end processing of third sequence, and continue training with shorted minibatches (<code>[2,7]</code>, <code>[3,8]</code>), and so on. Thus, packed sequence is represented as one vector - in our case <code>[1,6,9,2,7,3,8,4,5]</code>, and length vector (<code>[5,3,1]</code>), from which we can easily reconstruct the original padded minibatch.</p>
<p>To produce packed sequence, we can use <code>torch.nn.utils.rnn.pack_padded_sequence</code> function. All recurrent layers, including RNN, LSTM and GRU, support packed sequences as input, and produce packed output, which can be decoded using <code>torch.nn.utils.rnn.pad_packed_sequence</code>.</p>
<p>To be able to produce packed sequence, we need to pass length vector to the network, and thus we need a different function to prepare minibatches:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_length</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch and length sequence itself</span></span><br><span class="line">    len_seq = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">len</span>, v))</span><br><span class="line">    l = <span class="built_in">max</span>(len_seq)</span><br><span class="line">    <span class="keyword">return</span> (  <span class="comment"># tuple of three tensors - length sequence,  padded features, labels</span></span><br><span class="line">        torch.tensor(len_seq),</span><br><span class="line">        torch.stack(</span><br><span class="line">            [torch.nn.functional.pad(torch.tensor(t), (<span class="number">0</span>, l - <span class="built_in">len</span>(t)), mode=<span class="string">&#x27;constant&#x27;</span>, value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v]),</span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>] - <span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMClassifierPacked</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, hidden_dim, classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTMClassifierPacked, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) - <span class="number">.5</span></span><br><span class="line">        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(in_features=hidden_dim, out_features=classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, length</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        pad_x = nn.utils.rnn.pack_padded_sequence(x, length, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        pad_x, (h, c) = self.rnn(pad_x)</span><br><span class="line">        x, _ = nn.utils.rnn.pad_packed_sequence(pad_x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h.squeeze(dim=<span class="number">0</span>))</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    load_data(<span class="built_in">type</span>=<span class="string">&#x27;packed_sequence&#x27;</span>)</span><br><span class="line">    net = LSTMClassifierPacked(<span class="number">64</span>, <span class="number">32</span>, <span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">    summary(net)</span><br><span class="line">    hist = train_embed_offset(net, training_dataloader, valid_dataloader, print_step=<span class="number">1000</span>)</span><br><span class="line">    plot_acc_loss(hist)</span><br><span class="line"></span><br><span class="line">=================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                   Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">LSTMClassifierPacked                     --</span><br><span class="line">├─Embedding: <span class="number">1</span>-<span class="number">1</span>                         <span class="number">6</span>,<span class="number">131</span>,<span class="number">840</span></span><br><span class="line">├─LSTM: <span class="number">1</span>-<span class="number">2</span>                              <span class="number">12</span>,<span class="number">544</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">3</span>                            <span class="number">132</span></span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">6</span>,<span class="number">144</span>,<span class="number">516</span></span><br><span class="line">Trainable params: <span class="number">6</span>,<span class="number">144</span>,<span class="number">516</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">=================================================================</span><br></pre></td></tr></table></figure>
<p><img src="7.png" alt="LSTM3-chain" style="zoom:50%;" /></p>
<h2 id="bidirectional-and-multilayer-rnns">Bidirectional and multilayer RNNs</h2>
<blockquote>
<p>在我们的例子中，所有的递归网络都在一个方向上运行，从一个序列的开始到结束。这看起来很自然，因为它类似于我们阅读和聆听语言的方式。然而，由于在许多实际案例中，我们可以随机访问输入序列，因此在两个方向上运行递归计算可能是有意义的。这样的网络被称为双向RNN，它们可以通过向RNN/LSTM/GRU构造函数传递bidirectional=True参数来创建。</p>
<p>在处理双向网络时，我们需要两个隐藏状态向量，每个方向一个。PyTorch将这些向量编码为一个大一倍的向量，这很方便，因为你通常会将产生的隐藏状态传递给全连接的线性层，你只需要在创建层的时候考虑到这个大小的增加。 递归网络，单向或双向，捕捉序列中的某些模式，并可以将它们存储到状态向量或传递到输出。与卷积网络一样，我们可以在第一层的基础上建立另一个递归层，以捕捉更高层次的模式，这些模式是由第一层提取的低层次模式建立的。这使我们想到了多层RNN的概念，它由两个或更多的递归网络组成，上一层的输出被传递到下一层作为输入。</p>
</blockquote>
<p><img src="5.jpg" alt="LSTM3-chain" style="zoom:100%;" /></p>
<h2 id="generative-networks">Generative networks</h2>
<p>递归神经网络（RNN）及其门控单元的变体，如长短期记忆单元（LSTM）和门控递归单元（GRU）为语言建模提供了一种机制，即它们可以学习单词排序并提供对序列中下一个单词的预测。这使我们能够将RNN用于生成任务，如普通文本生成、机器翻译，甚至图像字幕。</p>
<p>在我们在上一单元讨论的RNN架构中，每个RNN单元产生下一个隐藏状态作为输出。然而，我们也可以给每个递归单元添加另一个输出，这将使我们能够输出一个序列（其长度与原始序列相等）。此外，我们可以使用RNN单元，在每一步都不接受输入，只接受一些初始状态向量，然后产生一个输出序列。</p>
<p><img src="6.jpg" alt="LSTM3-chain" style="zoom:100%;" /></p>
<ul>
<li><strong>One-to-one</strong> is a traditional neural network with one input and one output</li>
<li><strong>One-to-many</strong> is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train <strong>image captioning</strong> network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word</li>
<li><strong>Many-to-one</strong> corresponds to RNN architectures we described in the previous unit, such as text classification</li>
<li><strong>Many-to-many</strong>, or <strong>sequence-to-sequence</strong> corresponds to tasks such as <strong>machine translation</strong>, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.</li>
</ul>
<h3 id="building-character-vocabulary">Building character vocabulary</h3>
<p>To build character-level generative network, we need to split text into individual characters instead of words. This can be done by defining a different tokenizer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">char_tokenizer = <span class="built_in">list</span>  <span class="comment"># [word for word in words]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params"><span class="built_in">type</span>, vocab=<span class="literal">None</span>, token=<span class="string">&#x27;bow&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param type: bow, embedding, embedding_offset, packed_sequence</span></span><br><span class="line"><span class="string">    :param vocab: Other vocab</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    os.makedirs(root, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    training_iter, test_iter = torchtext.datasets.AG_NEWS(root=root)</span><br><span class="line">    builtins.training_data, test_data = to_map_style_dataset(training_iter), to_map_style_dataset(test_iter)</span><br><span class="line">    num_train = <span class="built_in">int</span>(<span class="built_in">len</span>(builtins.training_data) * <span class="number">0.8</span>)</span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 获取训练集的 vocab</span></span><br><span class="line">        builtins.vocab = get_vocab(token)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        builtins.vocab = vocab</span><br><span class="line">    builtins.vocab_size = <span class="built_in">len</span>(builtins.vocab)</span><br><span class="line">    <span class="comment"># 训练集细分训练集和验证集</span></span><br><span class="line">    training_data, valid_data = random_split(builtins.training_data,</span><br><span class="line">                                             [num_train, <span class="built_in">len</span>(builtins.training_data) - num_train])</span><br><span class="line"></span><br><span class="line">    builtins.training_data, builtins.valid_data, builtins.test_data = training_data, valid_data, test_data</span><br><span class="line">    <span class="comment"># bowify, padify, offsetify</span></span><br><span class="line">    collate_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;bow&#x27;</span>:</span><br><span class="line">        collate_fn = bowify</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;embedding&#x27;</span>:</span><br><span class="line">        collate_fn = padify</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;embedding_offset&#x27;</span>:</span><br><span class="line">        collate_fn = offsetify</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;packed_sequence&#x27;</span>:</span><br><span class="line">        collate_fn = pad_length</span><br><span class="line"></span><br><span class="line">    builtins.training_dataloader = DataLoader(training_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">    builtins.valid_dataloader = DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># builtins.test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=bowify, shuffle=True)</span></span><br><span class="line"></span><br><span class="line">    builtins.classes = [<span class="string">&#x27;World&#x27;</span>, <span class="string">&#x27;Sports&#x27;</span>, <span class="string">&#x27;Business&#x27;</span>, <span class="string">&#x27;Sci/Tech&#x27;</span>]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span>(<span class="params"><span class="built_in">type</span>=<span class="string">&#x27;bow&#x27;</span></span>):</span></span><br><span class="line">    counter = collections.Counter()</span><br><span class="line">    <span class="keyword">for</span> (label, line) <span class="keyword">in</span> training_data:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;bow&#x27;</span>:</span><br><span class="line">            counter.update(tokenizer(line))</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;ngram&#x27;</span>:</span><br><span class="line">            counter.update(torchtext.data.utils.ngrams_iterator(line, ngrams=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">            counter.update(char_tokenizer(line))</span><br><span class="line">    vocab = torchtext.vocab.vocab(counter, min_freq=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">x, <span class="built_in">type</span>=<span class="string">&#x27;bow&#x27;</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;bow&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> tokenizer(x)]</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [vocab[i] <span class="keyword">for</span> i <span class="keyword">in</span> char_tokenizer(x)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enc_char</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> encode(x, <span class="built_in">type</span>=<span class="string">&#x27;char&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="training-a-generative-rnn">Training a generative RNN</h2>
<p>The way we will train RNN to generate text is the following. On each step, we will take a sequence of characters of length <code>nchars</code>, and ask the network to generate next output character for each input character:</p>
<p><img src="8.png" alt="LSTM3-chain" style="zoom:40%;" /></p>
<blockquote>
<p>Depending on the actual scenario, we may also want to include some special characters, such as <em>end-of-sequence</em> <code>&lt;eos&gt;</code>. In our case, we just want to train the network for endless text generation, thus we will fix the size of each sequence to be equal to <code>nchars</code> tokens. Consequently, each training example will consist of <code>nchars</code> inputs and <code>nchars</code> outputs (which are input sequence shifted one symbol to the left). Minibatch will consist of several such sequences.</p>
<p>The way we will generate minibatches is to take each news text of length <code>l</code>, and generate all possible input-output combinations from it (there will be <code>l-nchars</code> such combinations). They will form one minibatch, and size of minibatches would be different at each training step.</p>
</blockquote>
<p>Because the network takes characters as input, and vocabulary size is pretty small, we do not need embedding layer, one-hot-encoded input can directly go to LSTM cell. However, because we pass character numbers as input, we need to one-hot-encode them before passing to LSTM. This is done by calling <code>one_hot</code> function during <code>forward</code> pass. Output encoder would be a linear layer that will convert hidden state into one-hot-encoded output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMGenerator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rnn = nn.LSTM(vocab_size, hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, s=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = nn.functional.one_hot(x, vocab_size).to(torch.float32)</span><br><span class="line">        x, s = self.rnn(x, s)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x), s</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>During training, we want to be able to sample generated text. To do that, we will define <code>generate</code> function that will produce output string of length <code>size</code>, starting from the initial string <code>start</code>.</p>
<p>The way it works is the following. First, we will pass the whole start string through the network, and take output state <code>s</code> and next predicted character <code>out</code>. Since <code>out</code> is one-hot encoded, we take <code>argmax</code> to get the index of the character <code>nc</code> in the vocabulary, and use <code>itos</code> to figure out the actual character and append it to the resulting list of characters <code>chars</code>. This process of generating one character is repeated <code>size</code> times to generate required number of characters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">net, size=<span class="number">100</span>, start=<span class="string">&#x27;today &#x27;</span></span>):</span></span><br><span class="line">    chars = <span class="built_in">list</span>(start)</span><br><span class="line">    out, s = net(enc_char(chars).view(<span class="number">1</span>, -<span class="number">1</span>).to(device))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">        nc = out.argmax(<span class="number">2</span>)[-<span class="number">1</span>][-<span class="number">1</span>]</span><br><span class="line">        chars.append(vocab.get_itos()[nc])</span><br><span class="line">        out, s = net(enc_char(chars).view(<span class="number">1</span>, -<span class="number">1</span>).to(device), s)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Special attention needs to be paid to the way we compute loss. We need to compute loss given one-hot-encoded output <code>out</code>, and expected text <code>text_out</code>, which is the list of character indices. Luckily, the <code>cross_entropy</code> function expects unnormalized network output as first argument, and class number as the second, which is exactly what we have. It also performs automatic averaging over minibatch size.</p>
<p>We also limit the training by <code>samples_to_train</code> samples, in order not to wait for too long. We encourage you to experiment and try longer training, possibly for several epochs (in which case you would need to create another loop around this code).</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generate</span>(<span class="params">net, training_dataloader, valid_dataloader, nchars, print_step=<span class="number">1000</span>, lr=learning_rate</span>):</span></span><br><span class="line">    loss_fn = torch.nn.CrossEntropyLoss()</span><br><span class="line">    hist = &#123;<span class="string">&#x27;train_loss&#x27;</span>: [], <span class="string">&#x27;train_acc&#x27;</span>: [], <span class="string">&#x27;val_loss&#x27;</span>: [], <span class="string">&#x27;val_acc&#x27;</span>: []&#125;</span><br><span class="line">    optimizer = op.Adam(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epoch + <span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;i&#125;</span>\n------------------------------------------&quot;</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        size, total_size, acc, total_loss, batch = <span class="number">0</span>, <span class="number">1000</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch, (_, x) <span class="keyword">in</span> <span class="built_in">enumerate</span>(training_dataloader):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(x) - nchars &lt; <span class="number">10</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> size &gt; total_size:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            text_in, text_out = get_batch(x)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            out, s = net(text_in)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># out: one-hot encoding</span></span><br><span class="line">            loss = loss_fn(out.flatten(<span class="number">0</span>, <span class="number">1</span>), text_out.flatten())</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            acc += (out.argmax(<span class="number">2</span>) == text_out).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch % print_step == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;train loss: <span class="subst">&#123;loss:&gt;5f&#125;</span>   [<span class="subst">&#123;batch&#125;</span>/<span class="subst">&#123;total_size&#125;</span>]&quot;</span>)</span><br><span class="line">                <span class="built_in">print</span>(generate(net))</span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;train_loss: <span class="subst">&#123;total_loss / batch:&gt;5f&#125;</span>     &quot;</span></span><br><span class="line">              <span class="string">f&quot;train_acc: <span class="subst">&#123;acc / (text_out.shape[<span class="number">0</span>] * nchars * total_size):&gt;5f&#125;</span>     <span class="subst">&#123;total_size&#125;</span>&quot;</span>)</span><br><span class="line">        hist[<span class="string">&#x27;train_loss&#x27;</span>].append(total_loss / batch)</span><br><span class="line">        hist[<span class="string">&#x27;train_acc&#x27;</span>].append(acc / total_size)</span><br><span class="line"></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        total_size, acc, total_loss, count = <span class="number">1000</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> _, x <span class="keyword">in</span> valid_dataloader:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(x) - nchars &lt; <span class="number">10</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> count &gt; total_size:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                size += <span class="number">1</span></span><br><span class="line">                text_in, text_out = get_batch(x)</span><br><span class="line">                out, s = net(text_in)</span><br><span class="line"></span><br><span class="line">                total_loss += loss_fn(out.flatten(<span class="number">0</span>, <span class="number">1</span>), text_out.flatten()).item()</span><br><span class="line">                acc += (out.argmax(dim=<span class="number">2</span>) == text_out).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;val_loss: <span class="subst">&#123;total_loss / count:&gt;5f&#125;</span>     &quot;</span></span><br><span class="line">                  <span class="string">f&quot;val_acc: <span class="subst">&#123;acc / (text_out.shape[<span class="number">0</span>] * nchars * total_size):&gt;5f&#125;</span>     <span class="subst">&#123;total_size&#125;</span>\n&quot;</span>)</span><br><span class="line">            hist[<span class="string">&#x27;val_loss&#x27;</span>].append(total_loss / count)</span><br><span class="line">            hist[<span class="string">&#x27;val_acc&#x27;</span>].append(acc / total_size)</span><br><span class="line">    <span class="keyword">return</span> hist</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    load_data(token=<span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    net = LSTMGenerator(<span class="number">64</span>)</span><br><span class="line">    hist = train_generate(net, training_data, valid_data, n_chars, <span class="number">100</span>)</span><br><span class="line">    plot_acc_loss(hist)</span><br><span class="line">    torch.save(net, <span class="string">&#x27;data/model.pth&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">=================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                   Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">LSTMGenerator                            --</span><br><span class="line">├─LSTM: <span class="number">1</span>-<span class="number">1</span>                              <span class="number">71</span>,<span class="number">168</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">2</span>                            <span class="number">5</span>,<span class="number">330</span></span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">76</span>,<span class="number">498</span></span><br><span class="line">Trainable params: <span class="number">76</span>,<span class="number">498</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">=================================================================</span><br><span class="line">epoch: <span class="number">1</span></span><br><span class="line">------------------------------------------</span><br><span class="line">train loss: <span class="number">4.440310</span>   [<span class="number">0</span>/<span class="number">1000</span>]</span><br><span class="line">today llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll</span><br><span class="line">train loss: <span class="number">2.982985</span>   [<span class="number">100</span>/<span class="number">1000</span>]</span><br><span class="line">today ton te the the the the the the the the the the the the the the the the the the the the the the the t</span><br><span class="line">train loss: <span class="number">2.483560</span>   [<span class="number">200</span>/<span class="number">1000</span>]</span><br><span class="line">today an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an a</span><br><span class="line">train loss: <span class="number">2.378409</span>   [<span class="number">300</span>/<span class="number">1000</span>]</span><br><span class="line">today the the the the the the the the the the the the the the the the the the the the the the the the the </span><br></pre></td></tr></table></figure>
<p>问题：</p>
<ul>
<li><p><strong>Better minibatch generation</strong>。我们准备训练数据的方式是从一个样本中生成一个小批量。这并不理想，因为minibatch都是不同大小的，其中一些甚至无法生成，因为文本比nchars小。此外，小批量加载的GPU不够充分。更明智的做法是从所有样本中获取一大块文本，然后生成所有输入输出对，将它们随机播放，并生成大小相等的小批。</p></li>
<li><p><strong>Multilayer LSTM</strong> 尝试2或3层的LSTM cell 是有意义的。正如我们在上一单元中提到的，LSTM的每一层都从文本中提取某些模式，在字符级发生器的情况下，我们可以期望较低的LSTM层负责提取音节，而较高的层负责提取单词和单词组合。这可以通过向LSTM构造器传递层数参数简单实现。</p></li>
</ul>
<h3 id="soft-text-generation-and-temperature">Soft text generation and temperature</h3>
<p>在上一个generate的定义中，我们总是将概率最高的字符作为生成文本中的下一个字符。这导致了这样一个事实，即文本经常一次又一次地在相同的字符序列之间 “循环”</p>
<p>然而，如果我们看一下下一个字符的概率分布，可能几个最高概率之间的差别并不大，例如，一个字符的概率为0.2，另一个为0.19，等等。例如，当寻找序列 "play "中的下一个字符时，下一个字符同样可以是空格，或e（如单词player）。</p>
<p>选择概率较高的字符并不总是 "公平 "的，因为选择第二高的字符仍然可能使我们得到有意义的文本。从网络输出所给出的概率分布中抽取字符是比较明智的做法。可以使用实现所谓的<code>multinomial</code> function 的 <strong>multinomial distribution</strong>来完成此采样。下面定义了实现此软文本生成的函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_soft</span>(<span class="params">net, size=<span class="number">100</span>, start=<span class="string">&#x27;today &#x27;</span>, temperature=<span class="number">1.0</span></span>):</span></span><br><span class="line">    chars = <span class="built_in">list</span>(start)</span><br><span class="line">    out, s = net(enc_char(chars).view(<span class="number">1</span>, -<span class="number">1</span>).to(device))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">        <span class="comment"># nc = torch.argmax(out[0][-1])</span></span><br><span class="line">        out_dist = out[<span class="number">0</span>][-<span class="number">1</span>].div(temperature).exp()</span><br><span class="line">        nc = torch.multinomial(out_dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        chars.append(vocab.get_itos()[nc])</span><br><span class="line">        out, s = net(nc.view(<span class="number">1</span>, -<span class="number">1</span>), s)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br><span class="line">  </span><br><span class="line">------------------------------------------------------------------------------------------------</span><br><span class="line">train loss: <span class="number">1.953934</span>   [<span class="number">600</span>/<span class="number">1000</span>]</span><br><span class="line">Today the tom pist whel <span class="keyword">and</span> ention threurs miles to ubset <span class="keyword">in</span> morer Drovald, carl ctafpored Mayidaming todaidurs brot stuptuw lives urpires aftee rrip Jeplinvend  of Spaukious oudders on over <span class="keyword">in</span> throunces of bestrows export chamal expent nack seadanned tipt  Sevend Co. to acttulars coupive baversday of the</span><br><span class="line"></span><br><span class="line">train loss: <span class="number">1.950226</span>   [<span class="number">700</span>/<span class="number">1000</span>]</span><br><span class="line">Today perment ride they caid, bet its Aie sales Wet; Friday greame face Wolk Rajing a  third quole plam, <span class="keyword">and</span> fan has Boinn mamaloics cres, brekin kemall entownifg the Gerferder Plater, Inc..mown Surds the <span class="keyword">as</span> nech to Stour detersdes work mote Thursday ratun of the t5 <span class="keyword">in</span> Iracor XN nee stail pohon, cael <span class="keyword">in</span> H</span><br><span class="line"></span><br><span class="line">train loss: <span class="number">1.878042</span>   [<span class="number">800</span>/<span class="number">1000</span>]</span><br><span class="line">Today strowd Dyrtar Wasulys UN6 kiches Carthan tolGsherity gandsing of the larous it hald? companing enfore quo to oud erices to intity quother, osing bekeriins alnpurity the choags onler poutlomed Bassec, romoffencomer. The intand <span class="number">5L</span>...E YMANFS has migeles wath goinforsed sowe sarger off terenomes taiver</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>引入了一个叫做 <strong>temperature</strong> 的参数，用来表示我们应该多努力地坚持最高概率。如果<strong>temperature</strong>是1.0，就做公平的 <strong>fair multinomial sampling</strong> ，当温度达到无穷大时--所有的概率都变得相等，我们就随机选择下一个字符。我们可以观察到，当我们把温度提高得太多时，文本变得毫无意义，而当温度接近0时，它就像 "循环的 "硬生成的文本。</p>
<h2 id="attention-mechanisms-and-transformers">Attention mechanisms and transformers</h2>
<blockquote>
<p><strong>recurrent networks</strong>的一个主要缺点是，一个序列中的所有词对结果的影响都是一样的。这导致标准的LSTM编码器-解码器模型在序列到序列的任务中表现欠佳，如命名实体识别和机器翻译。在现实中，输入序列中的特定词往往比其他词对序列输出有更大的影响。</p>
</blockquote>
<blockquote>
<p>考虑<strong>sequence-to-sequence</strong>的模型，如机器翻译。它由两个递归网络实现，其中一个网络（编码器）将输入序列折叠成<strong>hidden</strong>状态，另一个网络（解码器）将该<strong>hidden</strong>状态展开成翻译结果。这种方法的问题是，网络的最终状态很难记住一个句子的开头，从而导致该模型在长句子上的质量不佳。</p>
</blockquote>
<p><img src="10.png" alt="LSTM3-chain" style="zoom:30%;" /></p>
<center>
The encoder-decoder model, translating the sentence “she is eating a green apple” to Chinese.
</center>
<p><strong>Attention Mechanisms</strong>提供了一种手段，可以对每个输入 <strong>vector</strong> 对RNN的每个输出预测的背景影响进行加权。它的实现方式是在输入RNN的中间状态和输出RNN之间建立捷径。这样，在生成输出符号<span class="math inline">\(y_t\)</span>时，我们将考虑到所有的输入隐藏状态<span class="math inline">\(h_i\)</span>，其权重系数<span class="math inline">\(a_{t,i}\)</span>不同。</p>
<p><img src="9.png" alt="LSTM3-chain" style="zoom:40%;" /></p>
<center>
The encoder-decoder model with additive attention mechanism in Bahdanau et al., 2015.
</center>
<p><img src="11.png" alt="LSTM3-chain" style="zoom:50%;" /></p>
<p>Attention matrix <span class="math inline">\(\{α_{i,j}\}\)</span> would represent the degree which certain input words play in generation of a given word in the output sequence. Below is the example of such a matrix:</p>
<p><img src="12.png" alt="LSTM3-chain" style="zoom:80%;" /></p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">More About Attention</a></p>
<p>在自然语言处理领域，注意力机制是目前技术水平现状的主要原因。然而，增加注意力大大增加了模型参数的数量，导致了RNNs的扩展问题。缩放RNN的一个关键制约因素是模型的递归性质使得批量和并行化训练具有挑战性。在RNN中，一个序列的每个元素都需要按顺序处理，这意味着它不容易被并行化。</p>
<p>注意力机制的采用与这种约束相结合，导致了现在我们所知道和使用的从BERT到OpenGPT3的最先进的<strong>Art Transformer Models</strong> 的产生。</p>
<h3 id="transformer-models">Transformer models</h3>
<blockquote>
<p><strong>Transformer models</strong> 不是将以前的每一个预测的上下文转发到下一个评估步骤中，而是使用<strong>positional encodings</strong>和<strong>attention</strong>来捕捉一个给定的输入的上下文，并在一个给定的文本窗口中。下面的图片显示了位置编码和注意力是如何在一个给定的窗口中捕捉上下文的。</p>
</blockquote>
<p><img src="13.gif" alt="LSTM3-chain" style="zoom:80%;" /></p>
<p>由于每个输入位置被独立地映射到每个输出位置，<strong>Transformer models</strong>可以比RNN更好地并行化，这使得更大和更有表现力的语言模型成为可能。每个 <strong>attention head</strong> 都可以用来学习不同的词之间的关系，从而改善下游的自然语言处理任务。</p>
<p>BERT（Bidirectional Encoder Representations from Transformers）是一个非常大的多层 <strong>transformer</strong> 网络，BERT-base有12层，BERT-large有24层。该模型首先在大型文本数据语料库（WikiPedia+书籍）上使用无监督训练（预测句子中的 masked words）进行预训练。在预训练期间，该模型吸收了大量的语言理解，然后可以通过微调来利用其他数据集。这个过程被称为迁移学习。</p>
<p><img src="14.png" alt="LSTM3-chain" style="zoom:60%;" /></p>
<h2 id="using-bert-for-text-classification">Using BERT for text classification</h2>
<p>Let's see how we can use pre-trained BERT model for solving our traditional task: sequence classification. We will classify our original AG News dataset.</p>
<blockquote>
<p>Because we will be using pre-trained BERT model, we would need to use specific tokenizer. First, we will load a tokenizer associated with pre-trained BERT model.</p>
<p>HuggingFace library contains a repository of pre-trained models, which you can use just by specifying their names as arguments to <code>from_pretrained</code> functions. All required binary files for the model would automatically be downloaded.</p>
<p>However, at certain times you would need to load your own models, in which case you can specify the directory that contains all relevant files, including parameters for tokenizer, <code>config.json</code> file with model parameters, binary weights, etc.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To load the model from Internet repository using model name. </span></span><br><span class="line"><span class="comment"># Use this if you are running from your own copy of the notebooks</span></span><br><span class="line">bert_model = <span class="string">&#x27;bert-base-uncased&#x27;</span> </span><br><span class="line"></span><br><span class="line">tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)</span><br><span class="line"></span><br><span class="line">MAX_SEQ_LEN = <span class="number">128</span></span><br><span class="line">PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)</span><br><span class="line">UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)</span><br></pre></td></tr></table></figure>
<p>The <code>tokenizer</code> object contains the <code>encode</code> function that can be directly used to encode text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&#x27;PyTorch is a great framework for NLP&#x27;</span>)</span><br><span class="line"></span><br><span class="line">[<span class="number">101</span>, <span class="number">1052</span>, <span class="number">22123</span>, <span class="number">2953</span>, <span class="number">2818</span>, <span class="number">2003</span>, <span class="number">1037</span>, <span class="number">2307</span>, <span class="number">7705</span>, <span class="number">2005</span>, <span class="number">17953</span>, <span class="number">2361</span>, <span class="number">102</span>]</span><br></pre></td></tr></table></figure>
<p>let's create iterators which we will use during training to access the data. Because BERT uses it's own encoding function, we would need to define a padding function similar to <code>padify</code> we have defined before:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_bert</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># b is the list of tuples of length batch_size</span></span><br><span class="line">    <span class="comment">#   - first element of a tuple = label,</span></span><br><span class="line">    <span class="comment">#   - second = feature (text sequence)</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [tokenizer.encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch</span></span><br><span class="line">    l = <span class="built_in">max</span>(<span class="built_in">map</span>(<span class="built_in">len</span>, v))</span><br><span class="line">    <span class="keyword">return</span> (  <span class="comment"># tuple of two tensors -  features and labels</span></span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t), (<span class="number">0</span>, l - <span class="built_in">len</span>(t)), mode=<span class="string">&#x27;constant&#x27;</span>, value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v]),</span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>In our case, we will be using pre-trained BERT model called <code>bert-base-uncased</code>. Let's load the model using <code>BertForSequenceClassfication</code> package. This ensures that our model already has a required architecture for classification, including final classifier. You will see warning message stating that weights of the final classifier are not initialized, and model would require pre-training - that is perfectly okay, because it is exactly what we are about to do!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = transformers.BertForSequenceClassification.from_pretrained(bert_model,num_labels=<span class="number">4</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>Because BERT is already pre-trained, we want to start with rather small learning rate in order not to destroy initial weights.</p>
<p>All hard work is done by <code>BertForSequenceClassification</code> model. When we call the model on the training data, it returns both loss and network output for input minibatch. We use loss for parameter optimization (<code>loss.backward()</code> does the backward pass), and <code>out</code> for computing training accuracy by comparing obtained labels <code>labs</code> (computed using <code>argmax</code>) with expected <code>labels</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_bert</span>(<span class="params">net, training_dataloader, valid_dataloader, iterations, print_step=<span class="number">1000</span>, lr=learning_rate</span>):</span></span><br><span class="line">    hist = &#123;<span class="string">&#x27;train_loss&#x27;</span>: [], <span class="string">&#x27;train_acc&#x27;</span>: [], <span class="string">&#x27;val_loss&#x27;</span>: [], <span class="string">&#x27;val_acc&#x27;</span>: []&#125;</span><br><span class="line">    optimizer = op.Adam(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epoch + <span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;i&#125;</span>\n------------------------------------------&quot;</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        iter_times, size, acc, total_loss, batch = iterations, <span class="built_in">len</span>(training_dataloader.dataset), <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(training_dataloader):</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            loss, pred_y = net(x, labels=y)[:<span class="number">2</span>]</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            acc += (pred_y.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch % print_step == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;train loss: <span class="subst">&#123;loss:&gt;5f&#125;</span>   [<span class="subst">&#123;batch * batch_size&#125;</span>/<span class="subst">&#123;size&#125;</span>]&quot;</span>)</span><br><span class="line">            iter_times -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> iter_times:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;train_loss: <span class="subst">&#123;total_loss / batch:&gt;5f&#125;</span>     train_acc: <span class="subst">&#123;acc / size:&gt;5f&#125;</span>     <span class="subst">&#123;size&#125;</span>&quot;</span>)</span><br><span class="line">        hist[<span class="string">&#x27;train_loss&#x27;</span>].append(total_loss / batch)</span><br><span class="line">        hist[<span class="string">&#x27;train_acc&#x27;</span>].append(acc / size)</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        iter_times, size, acc, total_loss, count = iterations / <span class="number">4</span>, <span class="built_in">len</span>(valid_dataloader.dataset), <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dataloader:</span><br><span class="line">                x, y = x.to(device), y.to(device)</span><br><span class="line">                loss, pred_y = net(x, labels=y)[:<span class="number">2</span>]</span><br><span class="line">                total_loss += loss.item()</span><br><span class="line">                acc += (pred_y.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> count &gt; iter_times:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;val_loss: <span class="subst">&#123;total_loss / count:&gt;5f&#125;</span>     val_acc: <span class="subst">&#123;acc / size:&gt;5f&#125;</span>     <span class="subst">&#123;size&#125;</span>\n&quot;</span>)</span><br><span class="line">            hist[<span class="string">&#x27;val_loss&#x27;</span>].append(total_loss / count)</span><br><span class="line">            hist[<span class="string">&#x27;val_acc&#x27;</span>].append(acc / size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> hist</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># batch_size = 32</span></span><br><span class="line">    load_data(vocab=tokenizer.vocab, tokenizer=tokenizer, collate_fn=pad_bert)</span><br><span class="line">    model = transformers.BertForSequenceClassification.from_pretrained(bert_model, num_labels=<span class="number">4</span>)</span><br><span class="line">    hist = train_bert(model, training_dataloader, valid_dataloader, iterations=<span class="number">10</span>, print_step=<span class="number">2</span>, lr=<span class="number">1e-5</span>)</span><br><span class="line">    plot_acc_loss(hist)</span><br><span class="line">    </span><br><span class="line">================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                                  Param <span class="comment">#</span></span><br><span class="line">================================================================================</span><br><span class="line">BertForSequenceClassification                           --</span><br><span class="line">├─BertModel: <span class="number">1</span>-<span class="number">1</span>                                        --</span><br><span class="line">│    └─BertEmbeddings: <span class="number">2</span>-<span class="number">1</span>                              --</span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">1</span>                              <span class="number">23</span>,<span class="number">440</span>,<span class="number">896</span></span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">2</span>                              <span class="number">393</span>,<span class="number">216</span></span><br><span class="line">│    │    └─Embedding: <span class="number">3</span>-<span class="number">3</span>                              <span class="number">1</span>,<span class="number">536</span></span><br><span class="line">│    │    └─LayerNorm: <span class="number">3</span>-<span class="number">4</span>                              <span class="number">1</span>,<span class="number">536</span></span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">5</span>                                --</span><br><span class="line">│    └─BertEncoder: <span class="number">2</span>-<span class="number">2</span>                                 --</span><br><span class="line">│    │    └─ModuleList: <span class="number">3</span>-<span class="number">6</span>                             <span class="number">85</span>,054,<span class="number">464</span></span><br><span class="line">│    └─BertPooler: <span class="number">2</span>-<span class="number">3</span>                                  --</span><br><span class="line">│    │    └─Linear: <span class="number">3</span>-<span class="number">7</span>                                 <span class="number">590</span>,<span class="number">592</span></span><br><span class="line">│    │    └─Tanh: <span class="number">3</span>-<span class="number">8</span>                                   --</span><br><span class="line">├─Dropout: <span class="number">1</span>-<span class="number">2</span>                                          --</span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">3</span>                                           <span class="number">3</span>,076</span><br><span class="line">================================================================================</span><br><span class="line">Total params: <span class="number">109</span>,<span class="number">485</span>,<span class="number">316</span></span><br><span class="line">Trainable params: <span class="number">109</span>,<span class="number">485</span>,<span class="number">316</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">================================================================================</span><br><span class="line">epoch: <span class="number">10</span></span><br><span class="line">------------------------------------------</span><br><span class="line">train loss: <span class="number">0.445337</span>   [<span class="number">64</span>/<span class="number">320</span>]</span><br><span class="line">train loss: <span class="number">0.496814</span>   [<span class="number">128</span>/<span class="number">320</span>]</span><br><span class="line">train loss: <span class="number">0.416397</span>   [<span class="number">192</span>/<span class="number">320</span>]</span><br><span class="line">train loss: <span class="number">0.596100</span>   [<span class="number">256</span>/<span class="number">320</span>]</span><br><span class="line">train loss: <span class="number">0.358263</span>   [<span class="number">320</span>/<span class="number">320</span>]</span><br><span class="line">train_loss: <span class="number">0.405214</span>     train_acc: <span class="number">0.890625</span>   [<span class="number">285.0</span>/<span class="number">320</span>]</span><br><span class="line">val_loss: <span class="number">0.269997</span>     val_acc: <span class="number">0.725000</span>     <span class="number">80</span></span><br></pre></td></tr></table></figure>
<p><img src="16.png" alt="16" style="zoom:50%;" /></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/02/ML3-self-attention/" rel="prev" title="机器学习 by 李宏毅(4)">
                  <i class="fa fa-chevron-left"></i> 机器学习 by 李宏毅(4)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/05/cvpr2021/" rel="next" title="CVPR 2021 论文分享--微软亚洲研究院">
                  CVPR 2021 论文分享--微软亚洲研究院 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuYang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">723k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">10:57</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.1.0/dist/quicklink.umd.js"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{&quot;enable&quot;:true,&quot;home&quot;:true,&quot;archive&quot;:true,&quot;delay&quot;:true,&quot;timeout&quot;:3000,&quot;priority&quot;:true,&quot;ignores&quot;:null,&quot;url&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;04&#x2F;RNN&#x2F;&quot;}</script>
  <script src="/js/third-party/quicklink.js"></script>



  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script async src="/js/cursor/explosion.js"></script>

</body>
</html>
