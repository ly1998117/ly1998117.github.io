<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;ly1998117.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:true,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="Chaper 1 Introducing deep learning and the PyTorch Library  That general class of algorithms we’re talking about falls under the AI subcategory of deep learning, which deals with training mathematical">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning with PyTorch -- Part 1 Core PyTorch (C1-C3)">
<meta property="og:url" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/index.html">
<meta property="og:site_name" content="LiuYang&#39;s Blog">
<meta property="og:description" content="Chaper 1 Introducing deep learning and the PyTorch Library  That general class of algorithms we’re talking about falls under the AI subcategory of deep learning, which deals with training mathematical">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/1.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/2.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/3.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/4.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/5.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/6.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/7.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/8.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/9.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/10.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/11.jpeg">
<meta property="og:image" content="https://ly1998117.github.io/Users/liuyang/blog/source/_posts/Core-PyTorch/12.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/13.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/14.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/15.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/16.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/17.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/18.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/19.png">
<meta property="og:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/20.png">
<meta property="article:published_time" content="2021-07-18T07:02:16.000Z">
<meta property="article:modified_time" content="2021-07-25T13:42:26.273Z">
<meta property="article:author" content="LiuYang">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ly1998117.github.io/2021/07/18/Core-PyTorch/1.png">


<link rel="canonical" href="https://ly1998117.github.io/2021/07/18/Core-PyTorch/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;18&#x2F;Core-PyTorch&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;07&#x2F;18&#x2F;Core-PyTorch&#x2F;&quot;,&quot;title&quot;:&quot;Deep Learning with PyTorch -- Part 1 Core PyTorch (C1-C3)&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Deep Learning with PyTorch -- Part 1 Core PyTorch (C1-C3) | LiuYang's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiuYang's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">26</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">12</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#chaper-1-introducing-deep-learning-and-the-pytorch-library"><span class="nav-number">1.</span> <span class="nav-text">Chaper 1 Introducing deep learning and the PyTorch Library</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-deep-learning-revolution"><span class="nav-number">1.1.</span> <span class="nav-text">The deep learning revolution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pytorch-for-deep-learning"><span class="nav-number">1.2.</span> <span class="nav-text">PyTorch for deep learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-overview-of-how-pytorch-supports-deep-learning-projects"><span class="nav-number">1.3.</span> <span class="nav-text">An overview of how PyTorch supports deep learning projects</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hardware-and-software-requirements"><span class="nav-number">1.4.</span> <span class="nav-text">Hardware and software requirements</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter-2-pretrained-networks"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2 Pretrained Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a-pretrained-network-that-recognizes-the-subject-of-an-image"><span class="nav-number">2.1.</span> <span class="nav-text">A pretrained network that recognizes the subject of an image</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#obtaining-a-pretrained-network-for-image-recognition"><span class="nav-number">2.1.1.</span> <span class="nav-text">Obtaining a pretrained network for image recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#alexnet"><span class="nav-number">2.1.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#resnet"><span class="nav-number">2.1.3.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#run"><span class="nav-number">2.1.4.</span> <span class="nav-text">RUN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a-pretrained-model-that-fakes-it-until-it-makes-it"><span class="nav-number">2.2.</span> <span class="nav-text">A pretrained model that fakes it until it makes it</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-gan-game"><span class="nav-number">2.2.1.</span> <span class="nav-text">the GAN game</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cycle-gan"><span class="nav-number">2.2.2.</span> <span class="nav-text">Cycle GAN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-network-that-turns-horses-into-zebras"><span class="nav-number">2.2.3.</span> <span class="nav-text">A network that turns horses into zebras</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a-pretrained-network-that-describes-scenes"><span class="nav-number">2.3.</span> <span class="nav-text">A pretrained network that describes scenes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#neuraltalk2"><span class="nav-number">2.3.1.</span> <span class="nav-text">NeuralTalk2</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-hub"><span class="nav-number">2.4.</span> <span class="nav-text">Torch Hub</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#chapter3-start-with-a-tensor"><span class="nav-number">3.</span> <span class="nav-text">Chapter3 Start with a tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-world-as-floating-point-numbers"><span class="nav-number">3.1.</span> <span class="nav-text">The world as floating-point numbers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensors-multidimensional-arrays"><span class="nav-number">3.2.</span> <span class="nav-text">Tensors: Multidimensional arrays</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#from-python-lists-to-pytorch-tensors"><span class="nav-number">3.2.1.</span> <span class="nav-text">From Python lists to PyTorch tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#constructing-our-first-tensors"><span class="nav-number">3.2.2.</span> <span class="nav-text">Constructing our first tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-essence-of-tensors"><span class="nav-number">3.2.3.</span> <span class="nav-text">The essence of tensors</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#indexing-tensors"><span class="nav-number">3.3.</span> <span class="nav-text">Indexing tensors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#named-tensors"><span class="nav-number">3.4.</span> <span class="nav-text">Named tensors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor-element-types"><span class="nav-number">3.5.</span> <span class="nav-text">Tensor element types</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#specifying-the-numeric-type-with-dtype"><span class="nav-number">3.5.1.</span> <span class="nav-text">Specifying the numeric type with dtype</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a-dtype-for-every-occasion"><span class="nav-number">3.5.2.</span> <span class="nav-text">A dtype for every occasion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#managing-a-tensors-dtype-attribute"><span class="nav-number">3.5.3.</span> <span class="nav-text">Managing a tensor’s dtype attribute</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-tensor-api"><span class="nav-number">3.6.</span> <span class="nav-text">The tensor API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensors-scenic-views-of-storage"><span class="nav-number">3.7.</span> <span class="nav-text">Tensors: Scenic views of storage</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#indexing-into-storage"><span class="nav-number">3.7.1.</span> <span class="nav-text">Indexing into storage</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#modifying-stored-values-in-place-operations"><span class="nav-number">3.7.2.</span> <span class="nav-text">Modifying stored values: In-place operations</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor-metadata-size-offset-and-stride"><span class="nav-number">3.8.</span> <span class="nav-text">Tensor metadata: Size, offset, and stride</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#views-of-another-tensors-storage"><span class="nav-number">3.8.1.</span> <span class="nav-text">Views of another tensor’s storage</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transposing-without-copying"><span class="nav-number">3.8.2.</span> <span class="nav-text">Transposing without copying</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transposing-in-higher-dimensions"><span class="nav-number">3.8.3.</span> <span class="nav-text">Transposing in higher dimensions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#contiguous-tensors"><span class="nav-number">3.8.4.</span> <span class="nav-text">Contiguous tensors</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#moving-tensors-to-the-gpu"><span class="nav-number">3.9.</span> <span class="nav-text">Moving tensors to the GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#managing-a-tensors-device-attribute"><span class="nav-number">3.9.1.</span> <span class="nav-text">Managing a tensor’s device attribute</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy-interoperability"><span class="nav-number">3.10.</span> <span class="nav-text">NumPy interoperability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#generalized-tensors-are-tensors-too"><span class="nav-number">3.11.</span> <span class="nav-text">Generalized tensors are tensors, too</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#serializing-tensors"><span class="nav-number">3.12.</span> <span class="nav-text">Serializing tensors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#serializing-to-hdf5-with-h5py"><span class="nav-number">3.12.1.</span> <span class="nav-text">Serializing to HDF5 with h5py</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary"><span class="nav-number">3.13.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiuYang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LiuYang</p>
  <div class="site-description" itemprop="description">人与人的悲欢并不相通</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.macwk.com/" title="https:www.macwk.com&#x2F;" rel="noopener" target="_blank">Macwk</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ly1998117.github.io/2021/07/18/Core-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="LiuYang">
      <meta itemprop="description" content="人与人的悲欢并不相通">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuYang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning with PyTorch -- Part 1 Core PyTorch (C1-C3)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-18 15:02:16" itemprop="dateCreated datePublished" datetime="2021-07-18T15:02:16+08:00">2021-07-18</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-07-25 21:42:26" itemprop="dateModified" datetime="2021-07-25T21:42:26+08:00">2021-07-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>58k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>52 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="chaper-1-introducing-deep-learning-and-the-pytorch-library">Chaper 1 <em>Introducing deep learning and the PyTorch Library</em></h2>
<blockquote>
<p>That general class of algorithms we’re talking about falls under the AI subcategory of <em>deep learning</em>, which deals with training mathematical entities named <em>deep neural net- works</em> by presenting instructive examples.</p>
</blockquote>
<span id="more"></span>
<ul>
<li>How deep learning changes our approach to machine learning</li>
<li>Understanding why PyTorch is a good fit for deep learning</li>
<li>Examining a typical deep learning project</li>
<li>The hardware you’ll need to follow along with the examples</li>
</ul>
<h3 id="the-deep-learning-revolution">The deep learning revolution</h3>
<p>Until the last decade, the broader class of systems that fell under the label machine learning relied heavily on feature engineering. Features are transformations on input data that facilitate a downstream algorithm, like a classi- fier, to produce correct outcomes on new data. Feature engineering consists of com- ing up with the right transformations so that the downstream algorithm can solve a task.</p>
<p>The ability of a neural network to ingest data and extract useful representations on the basis of examples is what makes deep learning so powerful. The focus of deep learning practitioners is not so much on handcrafting those repre- sentations, but on operating on a mathematical entity so that it discovers representa- tions from the training data autonomously. Often, these automatically created features are better than those that are handcrafted!</p>
<p><img src="1.png" alt="1" style="zoom:40%;" /></p>
<center>
Figure 1.1 Deep learning exchanges the need to handcraft features for an increase in data and computational requirements.
</center>
<p>On the right side of figure 1.1, we see a practitioner busy defining engineering features and feeding them to a learning algorithm; the results on the task will be as good as the features the practitioner engineers.</p>
<p>On the left, with deep learning, the raw data is fed to an algorithm that extracts hierarchical features automatically, guided by the optimization of its own performance on the task; the results will be as good as the ability of the practitioner to drive the algorithm toward its goal.</p>
<p>we already get a glimpse of what we need to execute successful deep learning:</p>
<ul>
<li>We need a way to ingest whatever data we have at hand.</li>
<li>We somehow need to define the deep learning machine.</li>
<li>We must have an automated way, <em>training</em>, to obtain useful representations and make the machine produce desired outputs.</li>
</ul>
<h3 id="pytorch-for-deep-learning">PyTorch for deep learning</h3>
<p>PyTorch is a library for Python programs that facilitates building deep learning proj- ects. It emphasizes flexibility and allows deep learning models to be expressed in idiomatic Python. PyTorch provides a core data structure, the<code>tensor</code>, which is a multidimensional array that shares many similarities with NumPy arrays. Around that foundation, PyTorch comes with features to perform accelerated mathematical operations on dedicated hardware, which makes it convenient to design neural network architectures and train them on individual machines or parallel computing resources.</p>
<p>PyTorch offers some things that make it particularly relevant for deep learning:</p>
<ul>
<li>it provides accelerated computation using graphical processing units (GPUs), often yielding speedups in the range of 50x over doing the same calculation on a CPU.</li>
<li>PyTorch provides facilities that support numerical optimization on generic mathematical expressions, which deep learning uses for training.</li>
<li>PyTorch has been equipped with a high-performance C++ runtime that can be used to deploy models for inference without relying on Python, and can be used for designing and training models in C++.</li>
</ul>
<p>PyTorch has made huge inroads with the research and teaching communities, thanks to its ease of use, and has picked up momentum since, as researchers and graduates train students and move to industry.</p>
<h3 id="an-overview-of-how-pytorch-supports-deep-learning-projects">An overview of how PyTorch supports deep learning projects</h3>
<p>Actually, for performance reasons, most of PyTorch is written in C++ and CUDA (www.geforce.com/hardware/technology/cuda), a C++-like language from NVIDIA that can be compiled to run with massive parallelism on GPUs. most of the time we’ll interact with PyTorch from Python, building models, training them, and using the trained models to solve actual problems.</p>
<p>PyTorch provides the ability of tensors to keep track of the operations performed on them and to analyti- cally compute derivatives of an output of a computation with respect to any of its inputs. Figure 1.2 shows a standard setup that loads data, trains a model, and then deploys that model to production.</p>
<p><img src="2.png" alt="1" style="zoom:40%;" /></p>
<center>
Figure 1.2 Basic, high-level structure of a PyTorch project, with data loading, training, and deployment to production
</center>
<p>The core PyTorch modules for building neural networks are located in <code>torch.nn</code>, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. This bridge between our custom data (in whatever format it might be) and a standardized PyTorch tensor is the Dataset class PyTorch provides <code>torch.utils.data.</code></p>
<p>PyTorch readily provides all that magic in the DataLoader class. Its instances can spawn child processes to load data from a dataset in the background so that it’s ready and waiting for the training loop as soon as the loop can use it.</p>
<p>It’s increasingly common to use more elaborate hardware like multiple GPUs or multiple machines that contribute their resources to training a large model, as seen in the bottom center of figure 1.2. In those cases, <code>torch.nn.parallel.DistributedDataParallel</code> and the<code>torch.distributed</code> submodule can be employed to use the additional hardware.</p>
<h3 id="hardware-and-software-requirements">Hardware and software requirements</h3>
<p>we anticipate that completing a full training run for the more advanced examples in part 2 will require a CUDA-capable GPU. The default parameters used in part 2 assume a GPU with 8 GB of RAM (we suggest an NVIDIA GTX 1070 or better), but those can be adjusted if your hardware has less RAM available.</p>
<p><a target="_blank" rel="noopener" href="https://dawn.cs.stanford.edu/benchmark/index.html">DAWNBench</a> is an interesting initiative from Stanford University aimed at providing benchmarks on training time and cloud computing costs related to common deep learning tasks on publicly available datasets.</p>
<p>Full working code for all listings from the book can be found at the book’s website (www.manning.com/books/deep-learning-with-pytorch) and in our repository on GitHub (https://github.com/deep-learning-with-pytorch/dlwpt-code).</p>
<h2 id="chapter-2-pretrained-networks">Chapter 2 Pretrained Networks</h2>
<ul>
<li>Running pretrained image-recognition models</li>
<li>An introduction to GANs and CycleGAN</li>
<li>Captioning models that can produce text descriptions of images</li>
<li>Sharing models through Torch Hub</li>
</ul>
<p>We are going to learn how to use the work of the best researchers in the field by downloading and running very interesting models that have already been trained on open, large-scale datasets.</p>
<p>In this chapter, we will explore three popular pretrained models:</p>
<ul>
<li>a model that can label an image according to its content</li>
<li>another that can fabricate a new image from a real image --- GAN</li>
<li>a model that can describe the content of an image using proper English sentences.</li>
</ul>
<p>We will learn how to load and run these pretrained models in PyTorch, and we will introduce PyTorch Hub, a set of tools through which PyTorch models like the pretrained ones we’ll discuss can be easily made available through a uniform interface. Along the way, we’ll discuss data sources, define terminology like <em>label</em>, and attend a zebra rodeo.</p>
<h3 id="a-pretrained-network-that-recognizes-the-subject-of-an-image"><em>A pretrained network that recognizes the subject of an image</em></h3>
<p>we’ll run a state-of-the-art deep neural network that was pretrained on an object-recognition task.</p>
<p>The pretrained network we’ll explore here was trained on a subset of the <strong><a target="_blank" rel="noopener" href="http://imagenet.stanford.edu">ImageNet dataset</a>.</strong> ImageNet is a very large dataset of over 14 mil- lion images maintained by Stanford University. All of the images are labeled with a hier- archy of nouns that come from the <strong><a target="_blank" rel="noopener" href="http://wordnet.princeton.edu">WordNet dataset</a>,</strong> which is in turn a large lexical database of the English language. the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has gained popularity since its inception in 2010. This particular competition is based on a few tasks, which can vary each year, such as image classification (telling what object categories the image contains), object localization (identifying objects’ position in images), object detection (identifying and labeling objects in images), scene classifica- tion (classifying a situation in an image), and scene parsing (segmenting an image into regions associated with semantic categories, such as cow, house, cheese, hat).</p>
<p><img src="3.png" alt="3" style="zoom:40%;" /></p>
<center>
Figure 2.1 A small sample of ImageNet images
</center>
<p><img src="4.png" alt="4" style="zoom:50%;" /></p>
<center>
Figure 2.2 The inference process
</center>
<p>We are going to end up being able to take our own images and feed them into our pretrained model, as pictured in figure 2.2. This will result in a list of predicted labels for that image, which we can then examine to see what the model thinks our image is. Some images will have predictions that are accurate, and others will not!</p>
<h4 id="obtaining-a-pretrained-network-for-image-recognition"><em>Obtaining a pretrained network for image recognition</em></h4>
<p><a target="_blank" rel="noopener" href="https://github.com/pytorch/vision">TorchVision project</a>, which contains a few of the best-performing neural network architectures for com- puter vision, such as <a target="_blank" rel="noopener" href="http://mng.bz/lo6z">AlexNet</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/%201512.03385.pdf">ResNet</a>, and <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.00567.pdf">Inception v3</a>.</p>
<p>For now, let’s load up and run two networks: first AlexNet, one of the early breakthrough networks for image recognition; and then a residual network, ResNet for short, which won the ImageNet classification, detection, and localization competitions, among others, in 2015. The predefined models can be found in <code>torchvision.models</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[2]:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dir</span>(models)</span><br><span class="line"><span class="comment"># Out[2]:</span></span><br><span class="line">[<span class="string">&#x27;AlexNet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;DenseNet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Inception3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ResNet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;SqueezeNet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;VGG&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line"> <span class="string">&#x27;alexnet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;densenet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;densenet121&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line"> <span class="string">&#x27;resnet&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;resnet101&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;resnet152&#x27;</span>,</span><br><span class="line"> ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>The capitalized names refer to Python classes that implement a number of popular models. The lowercase names are convenience functions that return models instantiated from those classes, sometimes with different parameter sets. For instance, resnet101 returns an instance of ResNet with 101 layers, resnet18 has 18 layers, and so on. We’ll now turn our attention to AlexNet.</p>
<h4 id="alexnet"><em>AlexNet</em></h4>
<p>The AlexNet architecture won the 2012 ILSVRC by a large margin, with a top-5 test error rate (that is, the correct label must be in the top 5 predictions) of 15.4%. This was a defining moment in the history of computer vision: the moment when the community started to realize the potential of deep learning for vision tasks. That leap was followed by constant improvement, with more modern architectures and training methods getting top-5 error rates as low as 3%.</p>
<p><img src="5.png" alt="4" style="zoom:40%;" /></p>
<center>
Figure 2.3 The AlexNet architecture
</center>
<p>In figure 2.3, input images come in from the left and go through five stacks of filters, each producing a number of output images. After each filter, the images are reduced in size, as annotated. The images produced by the last stack of filters are laid out as a 4,096-element 1D vector and classified to produce 1,000 output probabilities, one for each output class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line">alexnet = models.AlexNet()</span><br></pre></td></tr></table></figure>
<p>Practically speaking, assuming we have an input object of the right type, we can run the forward pass with <code>output = alexnet(input)</code></p>
<h4 id="resnet">ResNet</h4>
<p>Using the resnet101 function, we’ll now instantiate a 101-layer convolutional neural network. Just to put things in perspective, before the advent of residual networks in 2015, achieving stable training at such depths was considered extremely hard. Residual networks pulled a trick that made it possible, and by doing so, beat several benchmarks in one sweep that year.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line">resnet = models.resnet101(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line">resnet</span><br><span class="line"><span class="comment"># Out[5]:</span></span><br><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                  bias=<span class="literal">False</span>)</span><br><span class="line">  (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>,</span><br><span class="line">                     track_running_stats=<span class="literal">True</span>)</span><br><span class="line">  (relu): ReLU(inplace)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>,</span><br><span class="line">ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      			(conv1): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">            (bn1): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">            (conv2): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">            (bn2): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">            (conv3): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">            (bn3): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">            (relu): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    ...	</span><br><span class="line">    )</span><br><span class="line">  (avgpool): AvgPool2d(kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">  (fc): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>If we scroll down, we’ll see a lot of Bottleneck modules repeating one after the other (101 of them!), containing convolutions and other modules. That’s the anatomy of a <strong>typical deep neural network for computer vision</strong>: a more or less sequential cascade of filters and nonlinear functions, ending with a layer (fc) producing scores for each of the 1,000 output classes (out_features).</p>
<p>the torchvision module provides transforms, which allow us to quickly define pipelines of basic preprocessing functions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">				transforms.Resize(<span class="number">256</span>),</span><br><span class="line">				transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">				transforms.ToTensor().</span><br><span class="line">				transforms.Normalize(</span><br><span class="line">          mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">          std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">				)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Scale the input image to 256 × 256</p></li>
<li><p>crop the image to 224 × 224 around the center</p></li>
<li><p>transform it to a tensor (a PyTorch multidimensional array: in this case, a 3D array with color, height, and CHAPTER 2 <strong>Pretrained networks</strong>width)</p></li>
<li><p>normalize its RGB (red, green, blue) components so that they have defined means and standard deviations.</p></li>
</ul>
<p>We can now grab a picture of our favorite dog (say, bobby.jpg from the GitHub repo), preprocess it, and then see what ResNet thinks of it. We can start by loading an image from the local filesystem using <a target="_blank" rel="noopener" href="https://pillow.readthedocs.io/en/stable">Pillow</a>, an image-manipulation module for Python:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[7]:</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;../data/p1ch2/bobby.jpg&quot;</span>).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">img_t = proeprocess(img)</span><br><span class="line">batch_t = torch.unsqueeze(img_t, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="6.png" alt="6" style="zoom:50%;" /></p>
<center>
Figure 2.4 Bobby, our very special input image
</center>
<h4 id="run">RUN</h4>
<p><strong>The process of running a trained model on new data is called <em>inference</em> in deep learning circles.</strong> In order to do inference, we need to put the network in eval mode:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = resnet(batch_t)</span><br><span class="line"></span><br><span class="line">tensor([[-<span class="number">9.0338e-01</span>, -<span class="number">5.5345e-01</span>, -<span class="number">5.0050e-01</span>, -<span class="number">9.7310e-01</span>, -<span class="number">7.5931e-01</span>,</span><br><span class="line">         -<span class="number">1.7018e-01</span>, -<span class="number">6.1143e-01</span>,  <span class="number">4.8775e-01</span>,  <span class="number">3.4872e-01</span>, -<span class="number">5.7588e-01</span>,</span><br><span class="line">         -<span class="number">8.7200e-01</span>, -<span class="number">6.0810e-01</span>, -<span class="number">4.3910e-01</span>, -<span class="number">6.9709e-01</span>, -<span class="number">1.0020e+00</span>,</span><br><span class="line">         ...</span><br><span class="line">         -<span class="number">5.5135e-01</span>, -<span class="number">7.9814e-01</span>, -<span class="number">5.0634e-02</span>, -<span class="number">1.6339e-01</span>, -<span class="number">2.8250e-01</span>,</span><br><span class="line">        ]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index</span><br><span class="line">tensor([<span class="number">207</span>])</span><br></pre></td></tr></table></figure>
<p>We can now use the index to access the label. Here, index is not a plain Python number, but a one-element, one-dimensional tensor. so we need to get the actual numerical value to use as an index into our labels list using index[0].</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(label_path, mode=<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    labels = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines()]</span><br><span class="line"></span><br><span class="line">res_net = resnet101(pretrained=<span class="literal">True</span>)</span><br><span class="line">res_net.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    out = res_net(batch_t)</span><br><span class="line">    index = out.argmax(dim=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># use torch.nn.functional.softmax (http://mng.bz/BYnq) to nor- malize our outputs to the range [0, 1], </span></span><br><span class="line">    <span class="comment"># and divide by the sum. That gives us something roughly akin to the confidence that the model has in its prediction.</span></span><br><span class="line">    percentage = torch.nn.functional.softmax(out, dim=<span class="number">1</span>)[<span class="number">0</span>] * <span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;predict: <span class="subst">&#123;labels[index]&#125;</span>&#x27;</span>, <span class="string">f&#x27;          probability: <span class="subst">&#123;percentage[index]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------------------</span><br><span class="line">predict: golden retriever           probability: <span class="number">97.44773864746094</span></span><br></pre></td></tr></table></figure>
<p>We’ve just run a network that won an image-classification competition in 2015. It learned to recognize our dog from examples of dogs, together with a ton of other real-world subjects. We’ll now see how different architectures can achieve other kinds of tasks, starting with <strong>image generation</strong>.</p>
<h3 id="a-pretrained-model-that-fakes-it-until-it-makes-it"><em>A pretrained model that fakes it until it makes it</em></h3>
<blockquote>
<p>Let’s suppose, for a moment, that we’re career criminals who want to move into sell- ing forgeries of “lost” paintings by famous artists. We’re criminals, not painters, so as we paint our fake Rembrandts and Picassos, it quickly becomes apparent that they’re amateur imitations rather than the real deal. We’d have to randomly try a bunch of things, gauge which ones took slightly longer to recognize as forgeries, and emphasize those traits on our future attempts, which would take far too long. Instead, we need to find an art historian of questionable moral standing to inspect our work and tell us exactly what it was that tipped them off that the painting wasn’t legit. With that feedback, we can improve our output in clear, directed ways, until our sketchy scholar can no longer tell our paintings from the real thing. While this scenario is a bit farcical, the underlying technology is sound and will likely have a profound impact on the perceived veracity of digital data in the years to come. The entire concept of “photographic evidence” is likely to become entirely suspect, given how easy it will be to automate the production of convincing, yet fake, images and video. The only key ingredient is data. Let’s see how this process works.</p>
</blockquote>
<h4 id="the-gan-game">the GAN game</h4>
<p>In the context of deep learning, what we’ve just described is known as <em>the GAN game</em>, where two networks, one acting as the painter and the other as the art historian, compete to outsmart each other at creating and detecting forgeries. GAN stands for <strong><em>generative adversarial network</em></strong>, where <em>generative</em> means something is being created (in this case, fake masterpieces), <em>adversarial</em> means the two networks are competing to outsmart the other, and well, <em>network</em> is pretty obvious.</p>
<p>The <strong><em>generator</em></strong> network takes the role of the painter in our scenario, tasked with pro- ducing realistic-looking images, starting from an arbitrary input.</p>
<p>The <strong><em>discriminator</em></strong> network is the amoral art inspector, needing to tell whether a given image was fabricated by the generator or belongs in a set of real images. This two-network design is atypical for most deep learning architectures but, when used to implement a GAN game, can lead to incredible results.</p>
<p><img src="7.png" alt="7" style="zoom:50%;" /></p>
<center>
Figure 2.5 Concept of a GAN game
</center>
<p>Figure 2.5 shows a rough picture of what’s going on. The end goal for the generator is to fool the discriminator into mixing up real and fake images. The end goal for the discriminator is to find out when it’s being tricked, but it also helps inform the generator about the identifiable mistakes in the generated images.</p>
<p>Note that “Discriminator wins” or “Generator wins” shouldn’t be taken literally— there’s no explicit tournament between the two. However, both networks are trained based on the outcome of the other network, which drives the optimization of the parameters of each network.</p>
<h4 id="cycle-gan">Cycle GAN</h4>
<p>An interesting evolution of this concept is the CycleGAN. A CycleGAN can turn images of one domain into images of another domain (and back), without the need for us to explicitly provide matching pairs in the training set.</p>
<p><img src="8.png" alt="8" style="zoom:50%;" /></p>
<center>
Figure 2.6 A CycleGAN trained to the point that it can fool both discriminator networks
</center>
<p>In figure 2.6, we have a CycleGAN workflow for the task of turning a photo of a horse into a zebra, and vice versa. Note that there are two separate generator networks, as well as two distinct discriminators.</p>
<ul>
<li>the first generator learns to produce an image conforming to a target distribution (zebras, in this case) starting from an image belonging to a different distribution (horses), so that the discriminator can’t tell if the image produced from a horse photo is actually a genuine picture of a zebra or not.</li>
<li>At the same time—and <strong>here’s where the <em>Cycle</em> prefix in the acronym comes in</strong>—the resulting fake zebra is sent through a different generator going the other way (zebra to horse, in our case), to be judged by another discriminator on the other side.</li>
</ul>
<p>Creating such a cycle stabilizes the training process considerably, which addresses one of the original issues with GANs.</p>
<p><strong>At this point, we don’t need matched horse/zebra pairs as ground truths (good luck getting them to match poses!).</strong> the generator learns how to selectively change the appearance of objects in the scene without supervision about what’s what. There’s no signal indicating that manes are manes and legs are legs, but they get translated to something that lines up with the anatomy of the other animal.</p>
<h4 id="a-network-that-turns-horses-into-zebras"><em>A network that turns horses into zebras</em></h4>
<p>The CycleGAN network has been trained on a dataset of (unrelated) horse images and zebra images extracted from the ImageNet dataset. The network learns to take an image of one or more horses and turn them all into zebras, leaving the rest of the image as unmodified as possible.</p>
<p>Playing with a pretrained CycleGAN will give us the opportunity to take a step closer and look at how a network—a generator, in this case—is implemented. We’ll use our old friend ResNet. We’ll define a <strong>ResNetGenerator</strong> class offscreen.</p>
<ul>
<li>Residual block of ResNet</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差 block</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNetBlock, self).__init__()</span><br><span class="line">        self.conv_block = self.build_conv_block(dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_conv_block</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        conv_block = []</span><br><span class="line">        <span class="comment"># 镜像填充</span></span><br><span class="line">        conv_block.append(nn.ReflectionPad2d(<span class="number">1</span>))</span><br><span class="line">        conv_block.extend([</span><br><span class="line">            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 归一化</span></span><br><span class="line">            nn.InstanceNorm2d(dim),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        ])</span><br><span class="line">        conv_block.append(nn.ReflectionPad2d(<span class="number">1</span>))</span><br><span class="line">        conv_block.extend([</span><br><span class="line">            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 归一化</span></span><br><span class="line">            nn.InstanceNorm2d(dim)</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*conv_block)</span><br></pre></td></tr></table></figure>
<ul>
<li>Framework of ResNetGenerator</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNetGenerator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_nc=<span class="number">3</span>, output_nc=<span class="number">3</span>, ngf=<span class="number">64</span>, n_blocks=<span class="number">9</span></span>):</span></span><br><span class="line">        <span class="comment"># 报错</span></span><br><span class="line">        <span class="keyword">assert</span> (n_blocks &gt;= <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">super</span>(ResNetGenerator, self).__init__()</span><br><span class="line">        self.model = self.build_model(input_nc, output_nc, ngf, n_blocks)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">self, input_nc, output_nc, ngf, n_blocks, n_downsampling=<span class="number">2</span></span>):</span></span><br><span class="line">        model = [</span><br><span class="line">            nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(in_channels=input_nc, out_channels=ngf, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.InstanceNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_downsampling):</span><br><span class="line">            multi = <span class="number">2</span> ** i</span><br><span class="line">            model.extend([</span><br><span class="line">                nn.Conv2d(in_channels=ngf * multi, out_channels=ngf * multi * <span class="number">2</span>,</span><br><span class="line">                          kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">                nn.InstanceNorm2d(ngf * multi * <span class="number">2</span>),</span><br><span class="line">                nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">        multi = <span class="number">2</span> ** n_downsampling</span><br><span class="line">        <span class="comment"># Residual blocks</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_blocks):</span><br><span class="line">            model.append(ResNetBlock(ngf * multi))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_downsampling):</span><br><span class="line">            multi = <span class="number">2</span> ** (n_downsampling - i)</span><br><span class="line">            model.extend([</span><br><span class="line">                <span class="comment"># 进行逆卷积操作（fractionally-strided convolutions），对 feature 进行插值</span></span><br><span class="line">                nn.ConvTranspose2d(in_channels=ngf * multi, out_channels=<span class="built_in">int</span>(ngf * multi / <span class="number">2</span>),</span><br><span class="line">                                  kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">                nn.InstanceNorm2d(<span class="built_in">int</span>(ngf * multi / <span class="number">2</span>)),</span><br><span class="line">                nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">        model.extend([</span><br><span class="line">            nn.ReflectionPad2d(<span class="number">3</span>),</span><br><span class="line">            nn.Conv2d(in_channels=ngf, out_channels=output_nc, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.model(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>print the Generator</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">ResNetGenerator(</span><br><span class="line">  (model): Sequential(</span><br><span class="line">    (<span class="number">0</span>): ReflectionPad2d((<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">    (<span class="number">1</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">2</span>): InstanceNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">5</span>): InstanceNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">9</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">10</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">11</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">12</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">13</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">14</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">15</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">16</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">17</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">18</span>): ResNetBlock(</span><br><span class="line">      (conv_block): Sequential(</span><br><span class="line">        (<span class="number">0</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">1</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">2</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">4</span>): ReflectionPad2d((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">5</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        (<span class="number">6</span>): InstanceNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">19</span>): ConvTranspose2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), output_padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): InstanceNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">21</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">22</span>): ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), output_padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">23</span>): InstanceNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">False</span>, track_running_stats=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">25</span>): ReflectionPad2d((<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">64</span>, <span class="number">3</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): Tanh()</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>load a random image of a horse and see what our generator produces.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">&#x27;../DeepLearningWithPyTorch/dlwpt-code/data/p1ch2/horse.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">800</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">img_t = preprocess(img)</span><br><span class="line">batch_t = torch.unsqueeze(img_t, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Sent the batch_t to our model</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_weights_path = <span class="string">&#x27;../DeepLearningWithPyTorch/dlwpt-code/data/p1ch2/horse2zebra_0.4.0.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">resGenerator = ResNetGenerator()</span><br><span class="line">resGenerator.load_state_dict(torch.load(model_weights_path))</span><br><span class="line">resGenerator.<span class="built_in">eval</span>()</span><br><span class="line">output = (resGenerator(batch_t).data.squeeze(<span class="number">0</span>) + <span class="number">1.0</span>) / <span class="number">2.0</span></span><br><span class="line">out_img = transforms.ToPILImage()(output)</span><br><span class="line">out_img.show()</span><br></pre></td></tr></table></figure>
<p>The resulting image (figure 2.8) is not perfect, but consider that it is a bit unusual for the network to find someone (sort of) riding on top of a horse. It bears repeating that the learning process has not passed through direct supervision, where humans have delineated tens of thousands of horses or man- ually Photoshopped thousands of zebra stripes.</p>
<p><img src="9.png" alt="9" style="zoom:35%;" /></p>
<center>
Figure 2.8 The left image is a man riding a horse. The horse is not having it.
</center>
<center>
The right image is a man riding a zebra. The zebra is not having it.
</center>
<p>On a serious note, it’s hard to overstate the implications of this kind of work. Tools like the one we just downloaded are only going to become higher quality and more ubiquitous. Face-swapping technology, in particular, has gotten considerable media attention.</p>
<h3 id="a-pretrained-network-that-describes-scenes"><em>A pretrained network that describes scenes</em></h3>
<p>In order to get firsthand experience with a model involving natural language, we will use a pretrained image-captioning model, generously provided by Ruotian Luo. It is an implementation of the NeuralTalk2 model by Andrej Karpathy. When presented with a natural image, this kind of model generates a caption in English that describes the scene, as shown in figure 2.9. a paired sentence description: for example, “A Tabby cat is leaning on a wooden table, with one paw on a laser mouse and the other on a black laptop.”3</p>
<p><img src="10.png" alt="10" style="zoom:50%;" /></p>
<center>
Figure 2.9 Concept of a captioning model
</center>
<p>This captioning model has two connected halves. The first half of the model is a network that learns to generate “descriptive” numerical representations of the scene (Tabby cat, laser mouse, paw), which are then taken as input to the second half. That second half is a <em>recurrent neural network</em> that generates a coherent sentence by putting those numerical descriptions together. The two halves of the model are trained together on image-caption pairs.</p>
<h4 id="neuraltalk2">NeuralTalk2</h4>
<p>The NeuralTalk2 model can be found at <a target="_blank" rel="noopener" href="https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch">ImageCaptioning.pytorch</a>. We can place a set of images in the data directory and run the following script:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python <span class="built_in">eval</span>.py --model ./data/FC/fc-model.pth --infos_path ./data/FC/fc-infos.pkl --image_folder ./data</span><br></pre></td></tr></table></figure>
<p><img src="11.jpeg" alt="11" style="zoom:33%;" /></p>
<blockquote>
<p>Output:</p>
<p>image 1: a brown horse standing on top of a beach</p>
</blockquote>
<h3 id="torch-hub">Torch Hub</h3>
<p>Pretrained models have been published since the early days of deep learning, but until PyTorch 1.0, there was no way to ensure that users would have a uniform inter- face to get them. PyTorch 1.0 saw the introduction of Torch Hub, which is a mechanism through which authors can publish a model on GitHub, with or without pretrained weights, and expose it through an interface that PyTorch understands.</p>
<p>All it takes for an author to publish a model through the Torch Hub mechanism is to place a file named <code>hubconf.py</code> in the root directory of the GitHub repository. The file has a very simple structure:</p>
<p><img src="/Users/liuyang/blog/source/_posts/Core-PyTorch/12.png" alt="12" style="zoom:50%;" /></p>
<p>In our quest for interesting pretrained models, we can now search for GitHub repositories that include <code>hubconf.py</code>, and we’ll know right away that we can load them using the<code>torch.hub</code> module. To do that, we’ll go back to <code>TorchVision</code>, because it provides a clean example of how to interact with Torch Hub.</p>
<p>Let’s visit https://github.com/pytorch/vision and notice that it contains a hubconf.py file. The first thing to do is to look in that file to see the entry points for the repo—we’ll need to specify them later. In the case of TorchVision, there are two: resnet18 and resnet50.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optional list of dependencies required by the package</span></span><br><span class="line">dependencies = [<span class="string">&#x27;torch&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># classification</span></span><br><span class="line"><span class="keyword">from</span> torchvision.models.alexnet <span class="keyword">import</span> alexnet</span><br><span class="line"><span class="keyword">from</span> torchvision.models.densenet <span class="keyword">import</span> densenet121, densenet169, densenet201, densenet161</span><br><span class="line"><span class="keyword">from</span> torchvision.models.inception <span class="keyword">import</span> inception_v3</span><br><span class="line"><span class="keyword">from</span> torchvision.models.resnet <span class="keyword">import</span> resnet18, resnet34, resnet50, resnet101, resnet152,\</span><br><span class="line">    resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2</span><br><span class="line"><span class="keyword">from</span> torchvision.models.squeezenet <span class="keyword">import</span> squeezenet1_0, squeezenet1_1</span><br><span class="line"><span class="keyword">from</span> torchvision.models.vgg <span class="keyword">import</span> vgg11, vgg13, vgg16, vgg19, vgg11_bn, vgg13_bn, vgg16_bn, vgg19_bn</span><br><span class="line"><span class="keyword">from</span> torchvision.models.googlenet <span class="keyword">import</span> googlenet</span><br><span class="line"><span class="keyword">from</span> torchvision.models.shufflenetv2 <span class="keyword">import</span> shufflenet_v2_x0_5, shufflenet_v2_x1_0</span><br><span class="line"><span class="keyword">from</span> torchvision.models.mobilenetv2 <span class="keyword">import</span> mobilenet_v2</span><br><span class="line"><span class="keyword">from</span> torchvision.models.mobilenetv3 <span class="keyword">import</span> mobilenet_v3_large, mobilenet_v3_small</span><br><span class="line"><span class="keyword">from</span> torchvision.models.mnasnet <span class="keyword">import</span> mnasnet0_5, mnasnet0_75, mnasnet1_0, \</span><br><span class="line">    mnasnet1_3</span><br><span class="line"></span><br><span class="line"><span class="comment"># segmentation</span></span><br><span class="line"><span class="keyword">from</span> torchvision.models.segmentation <span class="keyword">import</span> fcn_resnet50, fcn_resnet101, \</span><br><span class="line">    deeplabv3_resnet50, deeplabv3_resnet101, deeplabv3_mobilenet_v3_large, lraspp_mobilenet_v3_large</span><br></pre></td></tr></table></figure>
<p>Now we know the repo, the entry points, and one interesting keyword argument. That’s about all we need to load the model using torch.hub, without even cloning the repo. That’s right, PyTorch will handle that for us:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> hub</span><br><span class="line">Name <span class="keyword">and</span> branch of the GitHub repo</span><br><span class="line">resnet18_model = hub.load(<span class="string">&#x27;pytorch/vision:master&#x27;</span>, <span class="string">&#x27;resnet18&#x27;</span>, pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>This manages to download a snapshot of the master branch of the pytorch/vision repo, along with the weights, to a local directory (defaults to .torch/hub in our home directory) and run the resnet18 entry-point function, which returns the instantiated model.</p>
<p>At this point, we can invoke the returned model with proper arguments to run a forward pass on it, the same way we did earlier. The nice part is that now every model published through this mechanism will be accessible to us using the same modalities, well beyond vision.</p>
<p>Torch Hub is quite new at the time of writing, and there are only a few models pub- lished this way. We can get at them by Googling <strong>“github.com hubconf.py.”</strong> Hopefully the list will grow in the future, as more authors share their models through this channel.</p>
<h2 id="chapter3-start-with-a-tensor">Chapter3 Start with a tensor</h2>
<ul>
<li>Understanding tensors, the basic data structure in PyTorch</li>
<li>Indexing and operating on tensors</li>
<li>Interoperating with NumPy multidimensional arrays</li>
<li>Moving computations to the GPU for speed</li>
</ul>
<p>In this chapter, we learn how to deal with all the floating-point numbers in PyTorch by using tensors.</p>
<h3 id="the-world-as-floating-point-numbers"><em>The world as floating-point numbers</em></h3>
<p>Since floating-point numbers are the way a network deals with information, we need a way to encode real-world data of the kind we want to process into something digestible by a network and then decode the output back to something we can understand and use for our purpose.</p>
<p><img src="13.png" alt="13" style="zoom:45%;" /></p>
<center>
Figure 3.1 A deep neural network learns how to transform an input representation to an output representation. (Note: The numbers of neurons and outputs are not to scale.)
</center>
<p>A deep neural network typically learns the transformation from one form of data to another in stages, which means the partially transformed data between each stage can be thought of as a sequence of intermediate representations.For image recognition, early representations can be things such as edge detection or certain textures like fur. Deeper representations can capture more complex structures like ears, noses, or eyes.</p>
<p>In general, such intermediate representations are collections of floating-point numbers that characterize the input and capture the data’s structure in a way that is instrumental for describing how inputs are mapped to the outputs of the neural network.</p>
<p>In the context of deep learning, tensors refer to the generalization of vectors and matrices to an arbi- trary number of dimensions, as we can see in figure 3.2. Another name for the same concept is <em>multidimensional array</em>. The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor.</p>
<p><img src="14.png" alt="13" style="zoom:45%;" /></p>
<center>
Figure 3.2 Tensors are the building blocks for representing data in PyTorch.
</center>
<h3 id="tensors-multidimensional-arrays"><em>Tensors: Multidimensional arrays</em></h3>
<p>A tensor is an array: that is, a data structure that stores a collection of numbers that are accessible individually using an index, and that can be indexed with multiple indices.</p>
<h4 id="from-python-lists-to-pytorch-tensors"><em>From Python lists to PyTorch tensors</em></h4>
<p>It is not unusual for simple Python programs dealing with vectors of numbers, such as the coordinates of a 2D line, to use Python lists to store the vectors. As we will see in the following chapter, using the more efficient tensor data structure, many types of data—from images to time series, and even sentences—can be represented.</p>
<h4 id="constructing-our-first-tensors"><em>Constructing our first tensors</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="the-essence-of-tensors"><em>The essence of tensors</em></h4>
<p>Python lists or tuples of numbers are collections of Python objects that are individually allocated in memory, as shown on the left in figure 3.3. <strong>PyTorch tensors or NumPy arrays, on the other hand, are views over (typically) contiguous memory blocks containing <em>unboxed</em> C numeric types rather than Python objects.</strong> Each element is a 32-bit (4-byte) float in this case, as we can see on the right side of figure 3.3. This means storing a 1D tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes, plus a small overhead for the metadata (such as dimensions and numeric type).</p>
<p><img src="15.png" alt="13" style="zoom:50%;" /></p>
<center>
Figure 3.3 Python object (boxed) numeric values versus tensor (unboxed array) numeric values
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">points = torch.zeros(<span class="number">6</span>)</span><br><span class="line">points[<span class="number">0</span>] = <span class="number">4.0</span></span><br><span class="line">points[<span class="number">1</span>] = <span class="number">1.0</span></span><br><span class="line">points[<span class="number">2</span>] = <span class="number">5.0</span></span><br><span class="line">points[<span class="number">3</span>] = <span class="number">3.0</span></span><br><span class="line">points[<span class="number">4</span>] = <span class="number">2.0</span></span><br><span class="line">points[<span class="number">5</span>] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We can also pass a Python list to the constructor, to the same effect:</span></span><br><span class="line">points = torch.tensor([<span class="number">4.0</span>, <span class="number">1.0</span>, <span class="number">5.0</span>, <span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># To get the coordinates of the first point, we do the following:</span></span><br><span class="line"><span class="built_in">float</span>(points[<span class="number">0</span>]), <span class="built_in">float</span>(points[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2D tensor:</span></span><br><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">points</span><br><span class="line"><span class="comment"># Out[11]:</span></span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">1.</span>],</span><br><span class="line">[<span class="number">5.</span>, <span class="number">3.</span>],</span><br><span class="line">[<span class="number">2.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># the tensor about its shape:</span></span><br><span class="line"><span class="comment"># In[12]:</span></span><br><span class="line">points.shape</span><br><span class="line"><span class="comment"># Out[12]:</span></span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># We could also use zeros or ones to initialize the tensor, providing the size as a tuple:</span></span><br><span class="line"><span class="comment"># In[13]:</span></span><br><span class="line">points = torch.zeros(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># we can access an individual element in the tensor using two indices:</span></span><br><span class="line">points[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="indexing-tensors"><em>Indexing tensors</em></h3>
<p>What if we need to obtain a tensor containing all points but the first?</p>
<ul>
<li>All elements in list</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">some_list = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># From element 1 inclusive to element 4 exclusive</span></span><br><span class="line">some_list[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># From element 1 inclusive to the end of the list</span></span><br><span class="line">some_list[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># From the start of the list to element 4 exclusive</span></span><br><span class="line">some_list[:<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># From the start of the list to one before the last element</span></span><br><span class="line">some_list[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># From element 1 inclusive to element 4 exclusive, in steps of 2</span></span><br><span class="line">some_list[<span class="number">1</span>:<span class="number">4</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>we can use the same notation for PyTorch tensors, with the added benefit that, just as in NumPy and other Python scientific libraries, we can use range indexing for each of the tensor’s dimensions:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># All rows after the first; all columns</span></span><br><span class="line">points[<span class="number">1</span>:, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># All rows after the first; first column</span></span><br><span class="line">points[<span class="number">1</span>:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adds a dimension of size 1, just like unsqueeze</span></span><br><span class="line">points[<span class="literal">None</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In addition to using ranges, PyTorch features a powerful form of indexing, called <em>advanced indexing</em></p>
<h3 id="named-tensors"><em>Named tensors</em></h3>
<p>As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone. To make things concrete, imagine that we have a 3D tensor like img_t，and we want to convert it to gray- scale. We looked up typical weights for the colors to derive a single brightness value</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img_t = torch.randn(<span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>) <span class="comment"># shape [channels, rows, columns]</span></span><br><span class="line">weights = torch.tensor([<span class="number">0.2126</span>, <span class="number">0.7152</span>, <span class="number">0.0722</span>])</span><br></pre></td></tr></table></figure>
<p>We also often want our code to generalize—for example, from grayscale images repre- sented as 2D tensors with height and width dimensions to color images adding a third channel dimension (as in RGB), or from a single image to a batch of images. here we pretend to have a batch of 2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_t = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>) <span class="comment"># shape [batch, channels, rows, columns]</span></span><br></pre></td></tr></table></figure>
<p>So sometimes the RGB channels are in dimension 0, and sometimes they are in dimen- sion 1. But we can generalize by counting from the end: they are always in dimension –3, the third from the end.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img_gray_naive = img_t.mean(-<span class="number">3</span>)</span><br><span class="line">batch_gray_naive = batch_t.mean(-<span class="number">3</span>)</span><br><span class="line">img_gray_naive.shape, batch_gray_naive.shape</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">5</span>]), torch.Size([<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure>
<p>PyTorch will allow us to multiply things that are the same shape, as well as shapes where one operand is of size 1 in a given dimension. It also appends leading dimensions of size 1 automatically. This is a feature called <em>broadcasting</em>.</p>
<p>batch_t of shape (2, 3, 5, 5) is multiplied by unsqueezed_weights of shape (3, 1, 1), resulting in a tensor of shape (2, 3, 5, 5), from which we can then sum the third dimension from the end (the three channels):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">unsqueezed_weights = weights.unsqueeze(-<span class="number">1</span>).unsqueeze_(-<span class="number">1</span>)</span><br><span class="line">img_weights = (img_t * unsqueezed_weights)</span><br><span class="line">batch_weights = (batch_t * unsqueezed_weights)</span><br><span class="line">img_gray_weighted = img_weights.<span class="built_in">sum</span>(-<span class="number">3</span>)</span><br><span class="line">batch_gray_weighted = batch_weights.<span class="built_in">sum</span>(-<span class="number">3</span>)</span><br><span class="line">batch_weights.shape, batch_t.shape, unsqueezed_weights.shape</span><br><span class="line"></span><br><span class="line">Out[<span class="number">5</span>]:</span><br><span class="line">  (torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>]), torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>]), torch.Size([<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>For the sake of efficiency—the PyTorch function einsum (adapted from NumPy) specifies an indexing mini-language giving index names to dimensions for sums of such products. As often in Python, broadcasting—a form of summarizing unnamed things—is done using three dots '...'</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_gray_weighted_fancy = torch.einsum(<span class="string">&#x27;...chw,c-&gt;...hw&#x27;</span>, img_t, weights) </span><br><span class="line">batch_gray_weighted_fancy = torch.einsum(<span class="string">&#x27;...chw,c-&gt;...hw&#x27;</span>, batch_t, weights) batch_gray_weighted_fancy.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[6]:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>As we can see, there is quite a lot of bookkeeping involved. This is error-prone, espe- cially when the locations where tensors are created and used are far apart in our code. This has caught the eye of practitioners, and so it has been suggested3 that the dimension be given a name instead.</p>
<p>PyTorch 1.3 added <em>named tensors</em> as an experimental feature (see <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html">named_tensor_tutorial</a> and <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/named_tensor.html">named_tensor</a>). Tensor factory functions such as tensor and rand take a names argument. The names should be a sequence of strings:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> weights_named = torch.tensor([<span class="number">0.2126</span>, <span class="number">0.7152</span>, <span class="number">0.0722</span>], names=[<span class="string">&#x27;channels&#x27;</span>])</span><br><span class="line"> weights_named</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[7]:</span></span><br><span class="line">        tensor([<span class="number">0.2126</span>, <span class="number">0.7152</span>, <span class="number">0.0722</span>], names=(<span class="string">&#x27;channels&#x27;</span>,))</span><br></pre></td></tr></table></figure>
<p>When we already have a tensor and want to add names (but not change existing ones), we can call the method <code>refine_names</code> on it. Similar to indexing, the ellipsis (...) allows you to leave out any number of dimensions. With the rename sibling method, you can also overwrite or drop (by passing in None) existing names:</p>
<blockquote>
<p>Refining a dimension is defined as a "rename" with the following constraints:</p>
<ul>
<li>A <code>None</code> dim can be refined to have any name</li>
<li>A <code>named dim</code> can only be refined to have the <code>same name</code></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img_named =  img_t.refine_names(..., <span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>)</span><br><span class="line">batch_named = batch_t.refine_names(..., <span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;img named:&quot;</span>, img_named.shape, img_named.names)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;batch named:&quot;</span>, batch_named.shape, batch_named.names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[8]:</span></span><br><span class="line">img named: torch.Size([<span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>]) (<span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>)</span><br><span class="line">batch named: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>]) (<span class="literal">None</span>, <span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>The method align_as returns a tensor with missing dimensions added and existing ones permuted to the right order:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">weights_aligned = weights_named.align_as(img_named)</span><br><span class="line">weights_aligned.shape, weights_aligned.names</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[9]:</span></span><br><span class="line">(torch.Size([<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>]), (<span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>Functions accepting dimension arguments, like sum, also take named dimensions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gray_named = (img_named * weights_aligned).<span class="built_in">sum</span>(<span class="string">&#x27;channels&#x27;</span>)</span><br><span class="line">gray_named.shape, gray_named.names</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[10]:</span></span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">5</span>]), (<span class="string">&#x27;rows&#x27;</span>, <span class="string">&#x27;columns&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>If we try to combine dimensions with different names, we get an error:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gray_named = (img_named[..., :<span class="number">3</span>] * weights_named).<span class="built_in">sum</span>(<span class="string">&#x27;channels&#x27;</span>)</span><br><span class="line"></span><br><span class="line">RuntimeError: Error when</span><br><span class="line"> attempting to broadcast dims [<span class="string">&#x27;channels&#x27;</span>, <span class="string">&#x27;rows&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;columns&#x27;</span>] <span class="keyword">and</span> dims [<span class="string">&#x27;channels&#x27;</span>]: dim <span class="string">&#x27;columns&#x27;</span> <span class="keyword">and</span> dim <span class="string">&#x27;channels&#x27;</span></span><br><span class="line">  are at the same position <span class="keyword">from</span> the right but do <span class="keyword">not</span> match.</span><br></pre></td></tr></table></figure>
<p>If we want to use tensors outside functions that operate on named tensors, we need to drop the names by renaming them to None. The following gets us back into the world of unnamed dimensions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gray_plain = gray_named.rename(<span class="literal">None</span>)</span><br><span class="line">gray_plain.shape, gray_plain.names</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[12]:</span></span><br><span class="line">(torch.Size([<span class="number">5</span>, <span class="number">5</span>]), (<span class="literal">None</span>, <span class="literal">None</span>))</span><br></pre></td></tr></table></figure>
<p>Given the experimental nature of this feature at the time of writing, and to avoid mucking around with indexing and alignment, we will stick to unnamed in the remainder of the book.</p>
<h3 id="tensor-element-types"><em>Tensor element types</em></h3>
<p>using the standard Python numeric types can be suboptimal for several reasons:</p>
<ul>
<li><strong><em>Numbers in Python are objects</em></strong> Whereas a floating-point number might require only 32 bits to be represented on a computer, Python will convert it into a full-fledged Python object with reference counting, and so on. This operation, called <em>boxing</em>, is not a problem if we need to store a small number of numbers, but allocating millions gets very inefficient.</li>
<li><strong><em>Lists in Python are meant for sequential collections of objects.</em></strong> here are no operations defined for efficiently taking the dot product of two vectors, or summing vectors together. Also, Python lists have no way of optimizing the layout of their contents in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers).</li>
<li><strong><em>The Python interpreter is slow compared to optimized, compiled code.</em></strong></li>
</ul>
<p>Performing math- ematical operations on large collections of numerical data can be much faster using optimized code written in a compiled, low-level language like C.</p>
<p>For these reasons, data science libraries rely on NumPy or introduce dedicated data structures like PyTorch tensors, which provide efficient low-level implementations of numerical data structures and related operations on them, wrapped in a convenient high-level API.</p>
<h4 id="specifying-the-numeric-type-with-dtype">Specifying the numeric type with dtype</h4>
<p>The dtype argument to tensor constructors specifies the numerical data (d) type that will be contained in the tensor. Here’s a list of the possible values for the dtype argument:</p>
<ul>
<li><code>torch.float32</code> or <code>torch.float</code>: 32-bit floating-point</li>
<li><code>torch.float64</code> or <code>torch.double</code>: 64-bit, double-precision floating-point</li>
<li><code>torch.float16</code> or <code>torch.half</code>: 16-bit, half-precision floating-point</li>
<li><code>torch.int8</code>: signed 8-bit integers</li>
<li><code>torch.uint8</code>: unsigned 8-bit integers</li>
<li><code>torch.int16</code> or<code>torch.short</code>: signed 16-bit integers</li>
<li><code>torch.int32</code> or <code>torch.int</code>: signed 32-bit integers</li>
<li><code>torch.int64</code> or <code>torch.long</code>: signed 64-bit integers</li>
<li><code>torch.bool</code>: Boolean</li>
</ul>
<p>The default data type for tensors is 32-bit floating-point.</p>
<h4 id="a-dtype-for-every-occasion">A dtype for every occasion</h4>
<p>Computations happening in neural networks are typically executed with 32-bit floating-point precision. Higher precision, like 64-bit, will not buy improvements in the accuracy of a model and will require more memory and computing time. <strong>The 16-bit floating-point, half-precision data type is not present natively in standard CPUs, but it is offered on modern GPUs.</strong> It is possible to switch to half-precision to decrease the footprint of a neural network model if needed, with a minor impact on accuracy.</p>
<p>Creating a tensor with integers as arguments, such as using torch.tensor([2, 2]), will create a 64-bit integer tensor by default. As such, we’ll spend most of our time dealing with float32 and int64.</p>
<h4 id="managing-a-tensors-dtype-attribute">Managing a tensor’s dtype attribute</h4>
<p>In order to allocate a tensor of the right numeric type, we can specify the proper dtype as an argument to the constructor. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">double_points = torch.ones(<span class="number">10</span>, <span class="number">2</span>, dtype=torch.double)</span><br><span class="line">short_points = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=torch.short)</span><br></pre></td></tr></table></figure>
<p>We can find out about the dtype for a tensor by accessing the corresponding attribute:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">short_points.dtype</span><br><span class="line"><span class="comment"># Out[48]:</span></span><br><span class="line">torch.int16</span><br></pre></td></tr></table></figure>
<p>We can also cast the output of a tensor creation function to the right type using the corresponding casting method, such as</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[49]:</span></span><br><span class="line">double_points = torch.zeros(<span class="number">10</span>, <span class="number">2</span>).double()</span><br><span class="line">short_points = torch.ones(<span class="number">10</span>, <span class="number">2</span>).short()</span><br></pre></td></tr></table></figure>
<p>or the more convenient to method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[50]:</span></span><br><span class="line">double_points = torch.zeros(<span class="number">10</span>, <span class="number">2</span>).to(torch.double)</span><br><span class="line">short_points = torch.ones(<span class="number">10</span>, <span class="number">2</span>).to(dtype=torch.short)</span><br></pre></td></tr></table></figure>
<p>When mixing input types in operations, the inputs are converted to the larger type automatically. Thus, if we want 32-bit computation, we need to make sure all our inputs are (at most) 32-bit</p>
<h3 id="the-tensor-api"><em>The tensor API</em></h3>
<p>we’re going to get a gen- eral feel for the API and establish a few directions on where to find things in the online <a target="_blank" rel="noopener" href="http://pytorch.org/docs">documentation</a>, They are exhaustive and well organized, with the tensor operations divided into groups:</p>
<ul>
<li><em>Creation ops</em> —Functions for constructing a tensor, like <code>ones and from_numpy</code></li>
<li><em>Indexing, slicing, joining, mutating ops</em>—Functions for changing the shape, stride, or content of a tensor, like <code>transpose</code></li>
<li><em>Math ops</em>—Functions for manipulating the content of the tensor through computations
<ul>
<li>– <em>Pointwise ops</em>—Functions for obtaining a new tensor by applying a function to each element independently, like <code>abs and cos</code></li>
<li>– <em>Reduction ops</em>—Functions for computing aggregate values by iterating through tensors, like <code>mean, std, and norm</code></li>
<li>– <em>Comparison ops</em>—Functions for evaluating numerical predicates over tensors, like <code>equal and max</code></li>
<li>– <em>Spectral ops</em>—Functions for transforming in and operating in the frequency domain, like <code>stft and hamming_window</code></li>
<li>– <em>Other operations</em> —Special functions operating on vectors, like <code>cross, or matrices, like trace</code></li>
<li>– <em>BLAS and LAPACK operations</em>—Functions following the Basic Linear Algebra Subprograms (BLAS) specification for scalar, vector-vector, matrix-vector, and matrix-matrix operations</li>
</ul></li>
<li><em>Random sampling</em>—Functions for generating values by drawing randomly from probability distributions, like<code>randn and normal</code></li>
<li><em>Serialization</em> —Functions for saving and loading tensors, like<code>load and save</code></li>
<li><em>Parallelism</em>—Functions for controlling the number of threads for parallel CPU execution, like <code>set_num_threads</code></li>
</ul>
<p>the vast majority of operations on and between tensors are available in the torch module and can also be called as methods of a tensor object. For instance, the transpose function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">a_t = torch.transpose(a, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">a.shape, a_t.shape</span><br><span class="line"><span class="comment"># Out[71]:</span></span><br><span class="line">(torch.Size([<span class="number">3</span>, <span class="number">2</span>]), torch.Size([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<p>or as a method of the a tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[72]:</span></span><br><span class="line">a = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">a_t = a.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">a.shape, a_t.shape</span><br><span class="line"><span class="comment"># Out[72]:</span></span><br><span class="line">(torch.Size([<span class="number">3</span>, <span class="number">2</span>]), torch.Size([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="tensors-scenic-views-of-storage"><em>Tensors: Scenic views of storage</em></h3>
<p>Values in tensors are allocated in contiguous chunks of memory managed by torch.Storage instances. <strong>A storage is a one-dimensional array of numerical data: that is, a contiguous block of memory containing numbers of a given type, such as float (32 bits representing a floating-point number) or int64 (64 bits representing an integer).</strong> A PyTorch Tensor instance is a view of such a Storage instance that is capable of indexing into that storage using an offset and per-dimension strides.</p>
<p>Multiple tensors can index the same storage even if they index into the data differently. We can see an example of this in figure 3.4.</p>
<p><img src="16.png" alt="16" style="zoom:50%;" /></p>
<center>
Figure 3.4 Tensors are views of a Storage instance.
</center>
<p><strong>The underlying memory is allocated only once, however, so creating alternate tensor-views of the data can be done quickly regardless of the size of the data managed by the Storage instance.</strong></p>
<h4 id="indexing-into-storage"><em>Indexing into storage</em></h4>
<p>Let’s see how indexing into the storage works in practice with our 2D points. The storage for a given tensor is accessible using the .storage property:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">points.storage()</span><br><span class="line"><span class="comment"># Out[17]:</span></span><br><span class="line"> <span class="number">4.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">5.0</span></span><br><span class="line"> <span class="number">3.0</span></span><br><span class="line"> <span class="number">2.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line">[torch.FloatStorage of size <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<p>Even though the tensor reports itself as having three rows and two columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows how to translate a pair of indices into a location in the storage.</p>
<p>We can also index into a storage manually. For instance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">points_storage = points.storage()</span><br><span class="line">points_storage[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Out[18]:</span></span><br><span class="line"><span class="number">4.0</span></span><br></pre></td></tr></table></figure>
<p>The layout of a storage is always one-dimensional, regardless of the dimensionality of any and all tensors that might refer to it. Changing the value of a storage leads to changing the content of its referring tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">points_storage[<span class="number">0</span>] = <span class="number">2.0</span></span><br><span class="line">points</span><br><span class="line"><span class="comment"># Out[20]:</span></span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="modifying-stored-values-in-place-operations"><em>Modifying stored values: In-place operations</em></h4>
<p>In addition to the operations on tensors introduced in the previous section, a small number of operations exist only as methods of the Tensor object. They are recognizable from a trailing underscore in their name, like zero_, which indicates that <strong>the method operates <em>in place</em> by modifying the input instead of creating a new output tensor and returning it</strong>. <strong>Any method <em>without</em> the trailing underscore leaves the source tensor unchanged and instead returns a new tensor:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[73]:</span></span><br><span class="line">a = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># In[74]:</span></span><br><span class="line">a.zero_()</span><br><span class="line">a</span><br><span class="line"><span class="comment"># Out[74]:</span></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="tensor-metadata-size-offset-and-stride"><em>Tensor metadata: Size, offset, and stride</em></h3>
<p>In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them: size, offset, and stride. How these interact is shown in figure 3.5.</p>
<p><img src="17.png" alt="17" style="zoom:50%;" /></p>
<center>
Figure 3.5 Relationship between a tensor’s offset, size, and stride. Here the tensor is a view of a larger storage, like one that might have been allocated when creating a larger tensor.
</center>
<p>The storage offset is the index in the storage corresponding to the first element in the tensor. The stride is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.</p>
<h4 id="views-of-another-tensors-storage"><em>Views of another tensor’s storage</em></h4>
<p>We can get the second point in the tensor by providing the corresponding index:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[21]:</span></span><br><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">second_point = points[<span class="number">1</span>]</span><br><span class="line">second_point.storage_offset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[21]: </span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[22]:</span></span><br><span class="line">second_point.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[22]:</span></span><br><span class="line">torch.Size([<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>The resulting tensor has offset 2 in the storage (since we need to skip the first point, which has two items), and the size is an instance of the Size class containing one element, since the tensor is one-dimensional.</p>
<p>The stride is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. For instance, our points tensor has a stride of (2, 1):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[24]:</span></span><br><span class="line">points.stride()</span><br><span class="line"><span class="comment"># Out[24]:</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Accessing an element i, j in a 2D tensor results in accessing the <code>storage_offset + stride[0] * i + stride[1] * j</code>element in the storage.</p>
<p>This indirection between Tensor and Storage makes some operations inexpen- sive, like transposing a tensor or extracting a subtensor, because they do not lead to memory reallocations. Instead, they consist of allocating a new Tensor object with a different value for size, storage offset, or stride.</p>
<p>Changing the subtensor will have a side effect on the original tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">second_point = points[<span class="number">1</span>]</span><br><span class="line">second_point[<span class="number">0</span>] = <span class="number">10.0</span></span><br><span class="line">points</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[28]:</span></span><br><span class="line">tensor([[ <span class="number">4.</span>,  <span class="number">1.</span>],</span><br><span class="line">[<span class="number">10.</span>, <span class="number">3.</span>],</span><br><span class="line">[ <span class="number">2.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>This might not always be desirable, so we can eventually clone the subtensor into a new tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">second_point = points[<span class="number">1</span>].clone()</span><br><span class="line">second_point[<span class="number">0</span>] = <span class="number">10.0</span></span><br><span class="line">points</span><br><span class="line"><span class="comment"># Out[29]:</span></span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">1.</span>],</span><br><span class="line">[<span class="number">5.</span>, <span class="number">3.</span>],</span><br><span class="line">[<span class="number">2.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<h4 id="transposing-without-copying"><em>Transposing without copying</em></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[31]:</span></span><br><span class="line">points_t = points.t()</span><br><span class="line">points_t</span><br><span class="line"><span class="comment"># Out[31]:</span></span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>We can easily verify that the two tensors share the same storage</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">id</span>(points.storage()) == <span class="built_in">id</span>(points_t.storage())</span><br><span class="line"><span class="comment"># Out[32]:</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[33]:</span></span><br><span class="line">points.stride()</span><br><span class="line"><span class="comment"># Out[33]:</span></span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># In[34]:</span></span><br><span class="line">points_t.stride()</span><br><span class="line"><span class="comment"># Out[34]:</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>We can transpose points into points_t, as shown in figure 3.6. We change the order of the elements in the stride. After that, increasing the row (the first index of the ten- sor) will skip along the storage by one, just like when we were moving along columns in points. This is the very definition of transposing. No new memory is allocated:</p>
<p><img src="18.png" alt="18" style="zoom:50%;" /></p>
<center>
figure 3.6
</center>
<h4 id="transposing-in-higher-dimensions"><em>Transposing in higher dimensions</em></h4>
<p>Transposing in PyTorch is not limited to matrices. We can transpose a multidimen- sional array by specifying the two dimensions along which transposing (flipping shape and stride) should occur:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">some_t = torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">transpose_t = some_t.transpose(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">transpose_t.shape</span><br><span class="line"><span class="comment"># Out[35]:</span></span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># In[37]:</span></span><br><span class="line">some_t.stride()</span><br><span class="line"><span class="comment"># Out[37]:</span></span><br><span class="line">(<span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># In[38]:</span></span><br><span class="line">transpose_t.stride()</span><br><span class="line"><span class="comment"># Out[38]:</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">5</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<p>A tensor whose values are laid out in the storage starting from the rightmost dimension onward (that is, moving along rows for a 2D tensor) is defined as contiguous. Contiguous tensors are convenient because we can visit them efficiently in order without jumping around in the storage</p>
<h4 id="contiguous-tensors"><em>Contiguous tensors</em></h4>
<p>Some tensor operations in PyTorch only work on contiguous tensors. In that case, PyTorch will throw an informative exception and require us to call contiguous explicitly.</p>
<p>In our case, points is contiguous, while its transpose is not:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[39]:</span></span><br><span class="line">points.is_contiguous()</span><br><span class="line"><span class="comment"># Out[39]:</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="comment"># In[40]:</span></span><br><span class="line">points_t.is_contiguous()</span><br><span class="line"><span class="comment"># Out[40]:</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>We can obtain a new contiguous tensor from a non-contiguous one using the contiguous method.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[41]:</span></span><br><span class="line">points = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">points_t = points.t()</span><br><span class="line">points_t</span><br><span class="line"><span class="comment"># Out[41]:</span></span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># In[42]:</span></span><br><span class="line">points_t.storage()</span><br><span class="line"><span class="comment"># Out[42]:</span></span><br><span class="line"> <span class="number">4.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">5.0</span></span><br><span class="line"> <span class="number">3.0</span></span><br><span class="line"> <span class="number">2.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line">[torch.FloatStorage of size <span class="number">6</span>]</span><br><span class="line"><span class="comment"># In[43]:</span></span><br><span class="line">points_t.stride()</span><br><span class="line"><span class="comment"># Out[43]:</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># In[44]:</span></span><br><span class="line">points_t_cont = points_t.contiguous()</span><br><span class="line">points_t_cont</span><br><span class="line"><span class="comment"># Out[44]:</span></span><br><span class="line">tensor([[<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># In[45]:</span></span><br><span class="line">points_t_cont.stride()</span><br><span class="line"><span class="comment"># Out[45]:</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># In[46]:</span></span><br><span class="line">points_t_cont.storage()</span><br><span class="line"><span class="comment"># Out[46]:</span></span><br><span class="line"> <span class="number">4.0</span></span><br><span class="line"> <span class="number">5.0</span></span><br><span class="line"> <span class="number">2.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">3.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line">[torch.FloatStorage of size <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<p>Notice that the storage has been reshuffled in order for elements to be laid out row- by-row in the new storage.As a refresher, figure 3.7 shows our diagram again.</p>
<p><img src="19.png" alt="19" style="zoom:50%;" /></p>
<center>
Figure 3.7 Relationship between a tensor’s offset, size, and stride. Here the tensor is a view of a larger storage, like one that might have been allocated when creating a larger tensor.
</center>
<h3 id="moving-tensors-to-the-gpu"><em>Moving tensors to the GPU</em></h3>
<p>PyTorch tensors also can be stored on a different kind of processor: a graphics processing unit (GPU). Every PyTorch tensor can be transferred to (one of) the GPU(s) in order to perform massively parallel, fast computations.</p>
<blockquote>
<p>PyTorch support for various GPUs</p>
<p>As of mid-2019, the main PyTorch releases only have acceleration on GPUs that have support for CUDA. PyTorch can run on AMD’s <a target="_blank" rel="noopener" href="https://rocm.github.io">ROCm</a>, and the master repository provides support, but so far, you need to compile it yourself. (Before the regular build process, you need to run tools/amd_build/build_amd.py to translate the GPU code.) Support for Google’s tensor processing units (TPUs) is a work in progress (https://github.com/pytorch/xla), with the current proof of concept available to the public in Google Colab: https://colab.research.google.com. Implementation of data structures and kernels on other GPU technologies, such as OpenCL, are not planned at the time of this writing.</p>
</blockquote>
<h4 id="managing-a-tensors-device-attribute"><em>Managing a tensor’s device attribute</em></h4>
<p>Here is how we can create a tensor on the GPU by specifying the corresponding argument to the constructor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points_gpu = torch.tensor([[<span class="number">4.0</span>, <span class="number">1.0</span>], [<span class="number">5.0</span>, <span class="number">3.0</span>], [<span class="number">2.0</span>, <span class="number">1.0</span>]], device=<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>We could instead copy a tensor created on the CPU onto the GPU using the to method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points_gpu = points.to(device=<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>Doing so returns a new tensor that has the same numerical data, but stored in the RAM of the GPU, rather than in regular system RAM. Now that the data is stored locally on the GPU, we’ll start to see the speedups mentioned earlier when perform- ing mathematical operations on the tensor.</p>
<p>If our machine has more than one GPU, we can also decide on which GPU we allo- cate the tensor by passing a zero-based integer identifying the GPU on the machine, such as</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points_gpu = points.to(device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>At this point, any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[67]:</span></span><br><span class="line"><span class="comment"># Multiplication performed on the CPU</span></span><br><span class="line">points = <span class="number">2</span> * points</span><br><span class="line"><span class="comment"># Multiplication performed on the GPU</span></span><br><span class="line">points_gpu = <span class="number">2</span> * points.to(device=<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that the points_gpu tensor is not brought back to the CPU once the result has been computed. Here’s what happened in this line:</p>
<ol type="1">
<li>The points tensor is copied to the GPU.</li>
<li>A new tensor is allocated on the GPU and used to store the result of the multiplication.</li>
<li>A handle to that GPU tensor is returned.</li>
</ol>
</blockquote>
<p>In order to move the tensor back to the CPU, we need to provide a cpu argument to the to method, such as</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points_cpu = points_gpu.to(device=<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>We can also use the shorthand methods cpu and cuda instead of the to method to achieve the same goal:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[70]:</span></span><br><span class="line"><span class="comment"># Defaults to GPU index 0</span></span><br><span class="line">points_gpu = points.cuda() </span><br><span class="line">points_gpu = points.cuda(<span class="number">0</span>)</span><br><span class="line">points_cpu = points_gpu.cpu()</span><br></pre></td></tr></table></figure>
<h3 id="numpy-interoperability"><em>NumPy interoperability</em></h3>
<p>PyTorch tensors can be converted to NumPy arrays and vice versa very efficiently. By doing so, we can take advantage of the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This zero-copy interoperability with NumPy arrays is due to the storage system working with the <code>Python buffer</code> <a target="_blank" rel="noopener" href="https://docs.python.org/3/c-api/buffer.html">protocol</a>.</p>
<p>To get a NumPy array out of our points tensor, we just call</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[55]:</span></span><br><span class="line">points = torch.ones(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">points_np = points.numpy()</span><br><span class="line">points_np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out[55]:</span></span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32)</span><br></pre></td></tr></table></figure>
<p>Interestingly, the returned array shares the same underlying buffer with the tensor storage. This means the numpy method can be effectively executed at basically no cost, as long as the data sits in CPU RAM.</p>
<blockquote>
<p>NOTE While the default numeric type in PyTorch is 32-bit floating-point, for NumPy it is 64-bit. As discussed in section 3.5.2, we usually want to use 32-bit floating-points, so we need to make sure we have tensors of dtype <code>torch.float</code> after converting.</p>
</blockquote>
<h3 id="generalized-tensors-are-tensors-too"><em>Generalized tensors are tensors, too</em></h3>
<p>PyTorch will cause the right computation functions to be called regardless of whether our tensor is on the CPU or the GPU. This is accomplished through a <em>dispatching</em> mechanism, and that mechanism can cater to other tensor types by hooking up the user-facing API to the right backend functions.</p>
<p>The PyTorch dispatcher on the left in figure 3.8 is designed to be extensible; the subsequent switching done to accommodate the various numeric types of figure 3.8 shown on the right is a fixed aspect of the implementation coded into each backend.</p>
<p><img src="20.png" alt="20" style="zoom:50%;" /></p>
<center>
Figure 3.8 The dispatcher in PyTorch is one of its key infrastructure bits.
</center>
<h3 id="serializing-tensors"><em>Serializing tensors</em></h3>
<p>PyTorch uses pickle under the hood to serialize the tensor object, plus dedicated serialization code for the storage. Here’s how we can save our points tensor to an <code>ourpoints.t</code> file:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(points, <span class="string">&#x27;../data/p1ch3/ourpoints.t&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>As an alternative, we can pass a file descriptor in lieu of the filename:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;../data/p1ch3/ourpoints.t&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">   torch.save(points, f)</span><br></pre></td></tr></table></figure>
<p>Loading our points back is similarly a one-liner</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">points = torch.load(<span class="string">&#x27;../data/p1ch3/ourpoints.t&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>or, equivalently,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[60]:</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;../data/p1ch3/ourpoints.t&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">   points = torch.load(f)</span><br></pre></td></tr></table></figure>
<p>While we can quickly save tensors this way if we only want to load them with PyTorch, the file format itself is not interoperable: we can’t read the tensor with software other than PyTorch</p>
<h4 id="serializing-to-hdf5-with-h5py"><em>Serializing to HDF5 with h5py</em></h4>
<p>HDF5 is a portable, widely supported format for representing serialized multidimensional arrays, organized in a nested keyvalue dictionary. Python supports HDF5 through the h5py library (www.h5py.org), which accepts and returns data in the form of NumPy arrays.</p>
<p>At this point, we can save our points tensor by converting it to a NumPy array (at no cost, as we noted earlier) and passing it to the create_dataset function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[61]:</span></span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line">f = h5py.File(<span class="string">&#x27;../data/p1ch3/ourpoints.hdf5&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">dset = f.create_dataset(<span class="string">&#x27;coords&#x27;</span>, data=points.numpy())</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p>Here 'coords' is a key into the HDF5 file. We can have other keys—even nested ones. One of the interesting things in HDF5 is that we can index the dataset while on disk and access only the elements we’re interested in.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In[62]:</span></span><br><span class="line">f = h5py.File(<span class="string">&#x27;../data/p1ch3/ourpoints.hdf5&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">dset = f[<span class="string">&#x27;coords&#x27;</span>]</span><br><span class="line">last_points = dset[-<span class="number">2</span>:]</span><br><span class="line">last_points = torch.from_numpy(dset[-<span class="number">2</span>:])</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p>The data is not loaded when the file is opened or the dataset is required. Rather, the data stays on disk until we request the second and last rows in the dataset. At that point, h5py accesses those two columns and returns a NumPy array-like object encapsulating that region in that dataset that behaves like a NumPy array and has the same API.</p>
<h3 id="summary">Summary</h3>
<ul>
<li><p>Neural networks transform floating-point representations into other floatingpoint representations. The starting and ending representations are typically human interpretable, but the intermediate representations are less so.</p></li>
<li><p>These floating-point representations are stored in tensors.</p></li>
<li><p>Tensors are multidimensional arrays; they are the basic data structure in PyTorch.</p></li>
<li><p>PyTorch has a comprehensive standard library for tensor creation, manipula- tion, and mathematical operations.</p></li>
<li><p>Tensors can be serialized to disk and loaded back.</p></li>
<li><p>All tensor operations in PyTorch can execute on the CPU as well as on the GPU, with no change in the code.</p></li>
<li><p>PyTorch uses a trailing underscore to indicate that a function operates in place on a tensor (for example, Tensor.sqrt_).</p></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/17/ML7-2/" rel="prev" title="机器学习 by 李宏毅(7-2)">
                  <i class="fa fa-chevron-left"></i> 机器学习 by 李宏毅(7-2)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/19/pmset/" rel="next" title="通过 pmset 工具管理 masOS 睡眠">
                  通过 pmset 工具管理 masOS 睡眠 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2017 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiuYang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">871k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">13:12</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/quicklink@2.1.0/dist/quicklink.umd.js"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{&quot;enable&quot;:true,&quot;home&quot;:true,&quot;archive&quot;:true,&quot;delay&quot;:true,&quot;timeout&quot;:3000,&quot;priority&quot;:true,&quot;ignores&quot;:null,&quot;url&quot;:&quot;https:&#x2F;&#x2F;ly1998117.github.io&#x2F;2021&#x2F;07&#x2F;18&#x2F;Core-PyTorch&#x2F;&quot;}</script>
  <script src="/js/third-party/quicklink.js"></script>



  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script async src="/js/cursor/explosion.js"></script>

</body>
</html>
